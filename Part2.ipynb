{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a967814",
   "metadata": {},
   "source": [
    "# Agent 驱动的金融建模系统 -- 方向A：自动化特征工程\n",
    "\n",
    "## 结果索引\n",
    "| 步骤 | 输出项 | 所在 Cell |\n",
    "|------|--------|-----------|\n",
    "| 1.1 | 数据读取与完整性验证 | Cell 4 |\n",
    "| 1.2 | 数据概况报告 | Cell 6 |\n",
    "| 1.3 | 可视化（收盘价趋势/缺失值热力图/标签分布） | Cell 8 |\n",
    "| 1.4 | 数据泄漏初步检测 | Cell 10 |\n",
    "| 2.1 | 逐特征诊断（缺失/异常/分布/相关性） | Cell 13 |\n",
    "| 2.2 | 结构化特征诊断报告 | Cell 15 |\n",
    "| 2.3 | 诊断可视化（缺失Top20/箱型图/相关性热力图） | Cell 17 |\n",
    "| 3.1 | 千问Agent决策清理方案 | Cell 20 |\n",
    "| 3.2 | 执行特征清理 (clean_features) | Cell 22 |\n",
    "| 3.3 | 清理前后对比报告 | Cell 24 |\n",
    "| 3.4 | 清理后数据验证 | Cell 26 |\n",
    "| 4.1 | 特征有效性评估（Agent选标签+单特征LR） | Cell 29 |\n",
    "| 4.2 | 特征冗余检测（相关系数+VIF） | Cell 31 |\n",
    "| 4.3 | 综合特征评估报告与可视化 | Cell 33 |\n",
    "| 5.1 | 三轮筛选 + Top50特征列表及入选理由 | Cell 36 |\n",
    "| 5.2 | Agent选模型 + Top50模型训练验证 | Cell 38 |\n",
    "| 6.1 | 综合可视化（数据概况+清理前后+特征评估） | Cell 41 |\n",
    "| 6.2 | 模型评估可视化（混淆矩阵/ROC/PR曲线） | Cell 43 |\n",
    "| 6.3 | 数据泄漏检测报告（独立模块, 5项检测） | Cell 45 |\n",
    "| 6.4 | 完整报告汇总 + 产出文件索引 | Cell 47 |\n",
    "\n",
    "---\n",
    "\n",
    "## 步骤1：数据初始化与概况分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "580b8c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 环境初始化完成\n",
      "Python: 3.10.0b4 (tags/v3.10.0b4:2ba4b20, Jul 10 2021, 17:36:48) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas: 2.3.3, NumPy: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import types as _types\n",
    "if not hasattr(_types, 'UnionType'):\n",
    "    _types.UnionType = type('UnionType', (), {})\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # 非交互后端，兼容无GUI环境\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# 设置中文字体和显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "\n",
    "# pandas 显示设置\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# 日志配置\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 常量定义\n",
    "DATA_FILE = 'data.pq'\n",
    "FEATURE_COLS = [f'X{i}' for i in range(1, 301)]   # X1 ~ X300\n",
    "LABEL_COLS = [f'Y{i}' for i in range(1, 13)]       # Y1 ~ Y12\n",
    "BASE_COLS = ['trade_date', 'underlying', 'start_time', 'end_time',\n",
    "             'open', 'high', 'low', 'close', 'volume']\n",
    "IMG_DIR = 'images'\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "print(\"[OK] 环境初始化完成\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Pandas: {pd.__version__}, NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5cc147",
   "metadata": {},
   "source": [
    "### 1.1 数据读取与完整性验证\n",
    "读取 `data.pq` 文件，验证所有必需字段是否齐全（trade_date/underlying/start_time/end_time/open/high/low/close/volume/X1~X300/Y1~Y12）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fb60d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 16:44:43,789] INFO: 文件大小: 173.68 MB\n",
      "[2026-02-28 16:44:44,169] INFO: 数据读取成功: 81046 行 x 321 列\n",
      "[2026-02-28 16:44:44,170] INFO: [OK] 所有必需字段验证通过\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "数据完整性验证报告\n",
      "============================================================\n",
      "  文件路径:     data.pq\n",
      "  文件大小:     173.68 MB\n",
      "  数据规模:     81,046 行 x 321 列\n",
      "  必需字段数:   321\n",
      "  实际字段数:   321\n",
      "  缺失字段数:   0\n",
      "  额外字段数:   0\n",
      "  [OK] 基础字段: 9/9 存在\n",
      "  [OK] 特征字段 X1~X300: 300/300 存在\n",
      "  [OK] 标签字段 Y1~Y12: 12/12 存在\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def load_and_validate_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    读取 parquet 格式的金融时序数据文件，并验证所有必需字段是否齐全。\n",
    "\n",
    "    Description:\n",
    "        1. 检查文件是否存在，获取文件大小。\n",
    "        2. 使用 pyarrow 引擎读取 parquet 文件。\n",
    "        3. 对比实际字段与预期字段（基础字段9个 + 特征字段X1~X300共300个\n",
    "           + 标签字段Y1~Y12共12个 = 321个），输出验证摘要。\n",
    "\n",
    "    Parameters:\n",
    "        file_path : str\n",
    "            parquet 文件的相对或绝对路径，标量字符串。\n",
    "\n",
    "    Returns:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的 DataFrame，其中:\n",
    "            - n_samples: 数据行数（样本数）\n",
    "            - n_columns: 数据列数（至少321列：9基础 + 300特征 + 12标签）\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: 当指定路径的文件不存在时抛出。\n",
    "        RuntimeError: 当 parquet 文件读取失败时抛出。\n",
    "    \"\"\"\n",
    "    # 1. 验证文件存在性\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"[ERROR] 数据文件 '{file_path}' 不存在，请检查路径。\")\n",
    "\n",
    "    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    logger.info(f\"文件大小: {file_size_mb:.2f} MB\")\n",
    "\n",
    "    # 2. 读取 parquet 文件\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        logger.info(f\"数据读取成功: {df.shape[0]} 行 x {df.shape[1]} 列\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"[ERROR] 数据读取失败: {e}\")\n",
    "\n",
    "    # 3. 验证必需字段\n",
    "    expected_cols = set(BASE_COLS + FEATURE_COLS + LABEL_COLS)\n",
    "    actual_cols = set(df.columns)\n",
    "\n",
    "    missing_cols = expected_cols - actual_cols\n",
    "    extra_cols = actual_cols - expected_cols\n",
    "\n",
    "    if missing_cols:\n",
    "        logger.warning(f\"[WARN] 缺失字段 ({len(missing_cols)}): {sorted(missing_cols)[:10]}...\")\n",
    "    else:\n",
    "        logger.info(\"[OK] 所有必需字段验证通过\")\n",
    "\n",
    "    if extra_cols:\n",
    "        logger.info(f\"[INFO] 额外字段 ({len(extra_cols)}): {sorted(extra_cols)[:10]}\")\n",
    "\n",
    "    # 4. 输出验证摘要\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"数据完整性验证报告\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  文件路径:     {file_path}\")\n",
    "    print(f\"  文件大小:     {file_size_mb:.2f} MB\")\n",
    "    print(f\"  数据规模:     {df.shape[0]:,} 行 x {df.shape[1]} 列\")\n",
    "    print(f\"  必需字段数:   {len(expected_cols)}\")\n",
    "    print(f\"  实际字段数:   {len(actual_cols)}\")\n",
    "    print(f\"  缺失字段数:   {len(missing_cols)}\")\n",
    "    print(f\"  额外字段数:   {len(extra_cols)}\")\n",
    "\n",
    "    # 列出每类字段的检查结果\n",
    "    for name, cols in [(\"基础字段\", BASE_COLS),\n",
    "                       (\"特征字段 X1~X300\", FEATURE_COLS),\n",
    "                       (\"标签字段 Y1~Y12\", LABEL_COLS)]:\n",
    "        present = [c for c in cols if c in actual_cols]\n",
    "        absent  = [c for c in cols if c not in actual_cols]\n",
    "        status  = \"[OK]\" if not absent else \"[FAIL]\"\n",
    "        print(f\"  {status} {name}: {len(present)}/{len(cols)} 存在\", end=\"\")\n",
    "        if absent:\n",
    "            print(f\"  缺失: {absent[:5]}\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    return df\n",
    "\n",
    "\n",
    "# 执行数据加载\n",
    "df = load_and_validate_data(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb86b71b",
   "metadata": {},
   "source": [
    "### 1.2 数据概况报告\n",
    "生成数据量、字段类型、时间范围、缺失值占比（按字段）、时序分布等汇总信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b156f229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "数据概况报告\n",
      "============================================================\n",
      "\n",
      "[SIZE] 数据规模: 81,046 行 x 321 列\n",
      "[MEM]  内存占用: 202.43 MB\n",
      "\n",
      "[DTYPE] 字段类型分布:\n",
      "   float64: 317 列\n",
      "   datetime64[us, Asia/Shanghai]: 2 列\n",
      "   object: 1 列\n",
      "   datetime64[ns]: 1 列\n",
      "\n",
      "[TIME] 时间范围:\n",
      "   trade_date: 2015-01-05 00:00:00 -> 2020-12-31 00:00:00 (1462 个唯一值)\n",
      "   start_time: 2015-01-05 09:01:00+08:00 -> 2020-12-31 09:31:00+08:00 (5389 个唯一值)\n",
      "   end_time: 2015-01-05 15:00:00+08:00 -> 2020-12-31 15:15:00+08:00 (2924 个唯一值)\n",
      "\n",
      "[MISSING] 缺失值概况:\n",
      "   基础字段缺失率统计:\n",
      "count   9.0000\n",
      "mean    0.0000\n",
      "std     0.0000\n",
      "min     0.0000\n",
      "25%     0.0000\n",
      "50%     0.0000\n",
      "75%     0.0000\n",
      "max     0.0000\n",
      "\n",
      "   特征字段(X1~X300)缺失率统计:\n",
      "count   300.0000\n",
      "mean     23.6371\n",
      "std       0.5740\n",
      "min      23.2900\n",
      "25%      23.2900\n",
      "50%      23.3400\n",
      "75%      23.3600\n",
      "max      24.6800\n",
      "\n",
      "   标签字段(Y1~Y12)缺失率统计:\n",
      "count   12.0000\n",
      "mean     0.0000\n",
      "std      0.0000\n",
      "min      0.0000\n",
      "25%      0.0000\n",
      "50%      0.0000\n",
      "75%      0.0000\n",
      "max      0.0000\n",
      "\n",
      "   有缺失值的字段: 300/321\n",
      "   缺失率 > 50% 的字段: 0/321\n",
      "\n",
      "   缺失率 Top10 字段:\n",
      "     X170: 24.68% (20,001 条)\n",
      "     X200: 24.66% (19,986 条)\n",
      "     X199: 24.66% (19,986 条)\n",
      "     X198: 24.66% (19,986 条)\n",
      "     X197: 24.66% (19,986 条)\n",
      "     X196: 24.66% (19,986 条)\n",
      "     X195: 24.66% (19,986 条)\n",
      "     X194: 24.66% (19,986 条)\n",
      "     X193: 24.66% (19,986 条)\n",
      "     X192: 24.66% (19,986 条)\n",
      "\n",
      "[LABEL] 标签字段分析 (Y1~Y12):\n",
      "   Y1: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: -1.0:58257, 1.0:11779, 0.0:11010\n",
      "   Y2: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: -1.0:44133, 1.0:18819, 0.0:18094\n",
      "   Y3: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: -1.0:30081, 1.0:25969, 0.0:24996\n",
      "   Y4: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: 1.0:33078, 0.0:31958, -1.0:16010\n",
      "   Y5: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: -1.0:57113, 1.0:12557, 0.0:11376\n",
      "   Y6: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: -1.0:42342, 1.0:20027, 0.0:18677\n",
      "   Y7: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: -1.0:27626, 1.0:27458, 0.0:25962\n",
      "   Y8: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: 1.0:34753, 0.0:33149, -1.0:13144\n",
      "   Y9: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: -1.0:56642, 1.0:12990, 0.0:11414\n",
      "   Y10: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: -1.0:41750, 1.0:20641, 0.0:18655\n",
      "   Y11: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: 1.0:28189, -1.0:26887, 0.0:25970\n",
      "   Y12: dtype=float64, 唯一值=3, 缺失率=0.0%\n",
      "         分布: 1.0:35692, 0.0:33306, -1.0:12048\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def generate_data_overview(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    生成数据概况报告，包括基本统计、时间范围、缺失值分布和标签特征分析。\n",
    "\n",
    "    Description:\n",
    "        遍历所有字段，统计数据规模、内存占用、时间范围、各类字段的缺失率，\n",
    "        并对 Y1~Y12 标签字段做值分布分析，为后续标签选择提供依据。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据，其中 n_columns 包含\n",
    "            基础字段(9列)、特征字段X1~X300(300列)、标签字段Y1~Y12(12列)。\n",
    "\n",
    "    Returns:\n",
    "        report : dict\n",
    "            包含以下键的字典:\n",
    "            - 'shape': tuple (n_samples, n_columns), 数据维度\n",
    "            - 'dtypes': dict, 各数据类型对应的列数\n",
    "            - 'memory_mb': float, 内存占用(MB)\n",
    "            - 'time_info': dict, 各时间字段的最小/最大值和唯一值数\n",
    "            - 'missing': pd.DataFrame, 形状 (n_columns, 2), 含'缺失数'和'缺失率(%)'\n",
    "            - 'label_stats': dict, Y1~Y12 各标签的统计信息\n",
    "    \"\"\"\n",
    "    report = {}\n",
    "\n",
    "    # --- 基本信息 ---\n",
    "    report['shape'] = df.shape\n",
    "    report['dtypes'] = df.dtypes.value_counts().to_dict()\n",
    "    report['memory_mb'] = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "\n",
    "    # --- 时间范围 ---\n",
    "    time_cols = ['trade_date', 'start_time', 'end_time']\n",
    "    time_info = {}\n",
    "    for col in time_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                ts = pd.to_datetime(df[col])\n",
    "                time_info[col] = {\n",
    "                    'min': str(ts.min()),\n",
    "                    'max': str(ts.max()),\n",
    "                    'unique_count': ts.nunique()\n",
    "                }\n",
    "            except Exception:\n",
    "                time_info[col] = {'error': '无法解析为时间类型'}\n",
    "    report['time_info'] = time_info\n",
    "\n",
    "    # --- 缺失值统计 ---\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        '缺失数': missing,\n",
    "        '缺失率(%)': missing_pct\n",
    "    }).sort_values('缺失率(%)', ascending=False)\n",
    "    report['missing'] = missing_df\n",
    "\n",
    "    # 按字段类型汇总缺失\n",
    "    feature_cols_in_df = [c for c in FEATURE_COLS if c in df.columns]\n",
    "    label_cols_in_df = [c for c in LABEL_COLS if c in df.columns]\n",
    "    base_cols_in_df = [c for c in BASE_COLS if c in df.columns]\n",
    "\n",
    "    feature_missing = missing_pct[feature_cols_in_df].describe()\n",
    "    label_missing = missing_pct[label_cols_in_df].describe()\n",
    "    base_missing = missing_pct[base_cols_in_df].describe()\n",
    "\n",
    "    # --- 标签分布（用于自选Y的判断） ---\n",
    "    label_stats = {}\n",
    "    for col in LABEL_COLS:\n",
    "        if col in df.columns:\n",
    "            vc = df[col].value_counts(dropna=False)\n",
    "            label_stats[col] = {\n",
    "                'nunique': df[col].nunique(),\n",
    "                'missing_pct': round(df[col].isnull().mean() * 100, 2),\n",
    "                'value_counts': vc.head(10).to_dict(),\n",
    "                'dtype': str(df[col].dtype)\n",
    "            }\n",
    "    report['label_stats'] = label_stats\n",
    "\n",
    "    # --- 打印报告 ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"数据概况报告\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n[SIZE] 数据规模: {df.shape[0]:,} 行 x {df.shape[1]} 列\")\n",
    "    print(f\"[MEM]  内存占用: {report['memory_mb']:.2f} MB\")\n",
    "\n",
    "    print(f\"\\n[DTYPE] 字段类型分布:\")\n",
    "    for dtype, cnt in report['dtypes'].items():\n",
    "        print(f\"   {dtype}: {cnt} 列\")\n",
    "\n",
    "    print(f\"\\n[TIME] 时间范围:\")\n",
    "    for col, info in time_info.items():\n",
    "        if 'error' in info:\n",
    "            print(f\"   {col}: {info['error']}\")\n",
    "        else:\n",
    "            print(f\"   {col}: {info['min']} -> {info['max']} ({info['unique_count']} 个唯一值)\")\n",
    "\n",
    "    print(f\"\\n[MISSING] 缺失值概况:\")\n",
    "    print(f\"   基础字段缺失率统计:\\n{base_missing.to_string()}\")\n",
    "    print(f\"\\n   特征字段(X1~X300)缺失率统计:\\n{feature_missing.to_string()}\")\n",
    "    print(f\"\\n   标签字段(Y1~Y12)缺失率统计:\\n{label_missing.to_string()}\")\n",
    "\n",
    "    # 输出缺失率 > 0 的字段数\n",
    "    has_missing = (missing_pct > 0).sum()\n",
    "    high_missing = (missing_pct > 50).sum()\n",
    "    print(f\"\\n   有缺失值的字段: {has_missing}/{len(df.columns)}\")\n",
    "    print(f\"   缺失率 > 50% 的字段: {high_missing}/{len(df.columns)}\")\n",
    "\n",
    "    # 列出缺失率最高的 Top10 字段\n",
    "    top_missing = missing_df[missing_df['缺失率(%)'] > 0].head(10)\n",
    "    if len(top_missing) > 0:\n",
    "        print(f\"\\n   缺失率 Top10 字段:\")\n",
    "        for idx, row in top_missing.iterrows():\n",
    "            print(f\"     {idx}: {row['缺失率(%)']:.2f}% ({int(row['缺失数']):,} 条)\")\n",
    "\n",
    "    print(f\"\\n[LABEL] 标签字段分析 (Y1~Y12):\")\n",
    "    for col, stats in label_stats.items():\n",
    "        print(f\"   {col}: dtype={stats['dtype']}, 唯一值={stats['nunique']}, \"\n",
    "              f\"缺失率={stats['missing_pct']}%\")\n",
    "        # 如果是分类标签，显示类别分布\n",
    "        if stats['nunique'] <= 20:\n",
    "            vc_str = \", \".join([f\"{k}:{v}\" for k, v in list(stats['value_counts'].items())[:5]])\n",
    "            print(f\"         分布: {vc_str}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    return report\n",
    "\n",
    "\n",
    "# 执行\n",
    "report = generate_data_overview(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78580a",
   "metadata": {},
   "source": [
    "### 1.3 数据概况可视化\n",
    "绘制三类关键图表：\n",
    "1. **收盘价时间序列趋势图** — 按 `trade_date` 绘制 `close` 价格走势，观察时序分布与趋势特征；\n",
    "2. **字段缺失值热力图** — 使用 missingno 对 X1~X300 特征字段绘制缺失矩阵，识别系统性缺失模式；\n",
    "3. **标签(Y1~Y12)分布直方图** — 对所有标签字段绘制类别分布，为后续标签选择提供依据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8248622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 1.4: 数据概况可视化\n",
      "============================================================\n",
      "\n",
      "[1/3] 收盘价时间序列趋势图\n",
      "[OK] 收盘价趋势图已保存: images\\close_price_trend.png\n",
      "     日期范围: 2015-01-05 ~ 2020-12-31, 共 1462 个交易日\n",
      "\n",
      "[2/3] 特征字段缺失值热力图\n",
      "[OK] 缺失值矩阵图已保存: images\\missing_heatmap.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 16:40:12,694] INFO: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "[2026-02-28 16:40:12,695] INFO: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 缺失率柱状图已保存: images\\missing_bar.png\n",
      "\n",
      "[3/3] 目标标签 Y4 分布直方图\n",
      "[OK] 标签分布图已保存: images\\label_distribution.png\n",
      "     总有效样本: 81,046, 类别数: 3\n",
      "     最大类占比: 40.8%, 最小类占比: 19.8%\n",
      "     不平衡比: 2.07:1\n",
      "     [OK] 类别分布相对均衡\n"
     ]
    }
   ],
   "source": [
    "def plot_close_price_trend(df: pd.DataFrame, save_dir: str = IMG_DIR) -> None:\n",
    "    \"\"\"\n",
    "    绘制收盘价(close)随交易日期(trade_date)的时间序列趋势图。\n",
    "\n",
    "    Description:\n",
    "        1. 将 trade_date 解析为 datetime 类型并按日期排序。\n",
    "        2. 按 trade_date 分组计算每日平均收盘价（同一天可能有多条记录）。\n",
    "        3. 绘制折线图并标注关键统计量（最大/最小/均值）。\n",
    "        4. 将图片保存至 save_dir/close_price_trend.png。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据，需包含 'trade_date' 和 'close' 列。\n",
    "        save_dir : str\n",
    "            图片保存目录路径，标量字符串，默认为 IMG_DIR。\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "            无返回值，图片保存至文件并在 notebook 中显示。\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "    # 解析日期并按日聚合\n",
    "    temp = df[['trade_date', 'close']].copy()\n",
    "    temp['trade_date'] = pd.to_datetime(temp['trade_date'])\n",
    "    daily = temp.groupby('trade_date')['close'].mean().sort_index()\n",
    "\n",
    "    ax.plot(daily.index, daily.values, linewidth=0.8, color='steelblue', alpha=0.9)\n",
    "    ax.axhline(daily.mean(), color='orange', linestyle='--', linewidth=0.8, label=f'均值={daily.mean():.2f}')\n",
    "    ax.set_title('收盘价时间序列趋势图（按 trade_date 日均）', fontsize=13)\n",
    "    ax.set_xlabel('交易日期')\n",
    "    ax.set_ylabel('收盘价 (close)')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 标注最大/最小\n",
    "    max_date = daily.idxmax()\n",
    "    min_date = daily.idxmin()\n",
    "    ax.annotate(f'Max={daily.max():.2f}', xy=(max_date, daily.max()),\n",
    "                xytext=(10, 10), textcoords='offset points', fontsize=8,\n",
    "                arrowprops=dict(arrowstyle='->', color='red'), color='red')\n",
    "    ax.annotate(f'Min={daily.min():.2f}', xy=(min_date, daily.min()),\n",
    "                xytext=(10, -15), textcoords='offset points', fontsize=8,\n",
    "                arrowprops=dict(arrowstyle='->', color='green'), color='green')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'close_price_trend.png')\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[OK] 收盘价趋势图已保存: {save_path}\")\n",
    "    print(f\"     日期范围: {daily.index.min().date()} ~ {daily.index.max().date()}, 共 {len(daily)} 个交易日\")\n",
    "\n",
    "\n",
    "def plot_missing_heatmap(df: pd.DataFrame, feature_cols: list, save_dir: str = IMG_DIR) -> None:\n",
    "    \"\"\"\n",
    "    绘制特征字段(X1~X300)的缺失值矩阵热力图。\n",
    "\n",
    "    Description:\n",
    "        1. 从 df 中提取 feature_cols 对应的列。\n",
    "        2. 使用 missingno.matrix 绘制缺失值可视化矩阵，白色表示缺失。\n",
    "        3. 同时绘制一张按特征排序的缺失率柱状图作为补充。\n",
    "        4. 将图片保存至 save_dir/missing_heatmap.png 和 missing_bar.png。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据。\n",
    "        feature_cols : list of str\n",
    "            特征列名列表，长度为 n_features（最多300），如 ['X1', 'X2', ..., 'X300']。\n",
    "        save_dir : str\n",
    "            图片保存目录路径，标量字符串，默认为 IMG_DIR。\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "            无返回值，图片保存至文件并在 notebook 中显示。\n",
    "    \"\"\"\n",
    "    cols_in_df = [c for c in feature_cols if c in df.columns]\n",
    "\n",
    "    # 1) missingno 矩阵图 — 抽样50个特征以保持可读性\n",
    "    sample_cols = cols_in_df[::6]  # 每隔6个取1个，约50个特征\n",
    "    fig1 = msno.matrix(df[sample_cols], figsize=(16, 6), sparkline=False, fontsize=7)\n",
    "    fig1.set_title('特征字段缺失值矩阵 (X1~X300 抽样)', fontsize=13)\n",
    "    save_path1 = os.path.join(save_dir, 'missing_heatmap.png')\n",
    "    fig1.get_figure().savefig(save_path1, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[OK] 缺失值矩阵图已保存: {save_path1}\")\n",
    "\n",
    "    # 2) 缺失率柱状图\n",
    "    missing_pct = df[cols_in_df].isnull().mean() * 100\n",
    "    fig2, ax2 = plt.subplots(figsize=(16, 4))\n",
    "    ax2.bar(range(len(missing_pct)), missing_pct.values, width=1.0, color='coral', alpha=0.7)\n",
    "    ax2.axhline(missing_pct.mean(), color='navy', linestyle='--', linewidth=0.8,\n",
    "                label=f'平均缺失率={missing_pct.mean():.2f}%')\n",
    "    ax2.set_title('特征字段缺失率分布 (X1~X300)', fontsize=13)\n",
    "    ax2.set_xlabel('特征编号')\n",
    "    ax2.set_ylabel('缺失率 (%)')\n",
    "    ax2.set_xlim(-1, len(missing_pct))\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    # 标注刻度: 每50个\n",
    "    tick_pos = list(range(0, len(cols_in_df), 50))\n",
    "    tick_labels = [cols_in_df[i] for i in tick_pos]\n",
    "    ax2.set_xticks(tick_pos)\n",
    "    ax2.set_xticklabels(tick_labels, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    save_path2 = os.path.join(save_dir, 'missing_bar.png')\n",
    "    fig2.savefig(save_path2, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[OK] 缺失率柱状图已保存: {save_path2}\")\n",
    "\n",
    "\n",
    "def plot_all_label_distributions(df: pd.DataFrame, label_cols: list,\n",
    "                                  save_dir: str = IMG_DIR) -> None:\n",
    "    \"\"\"\n",
    "    绘制所有标签字段(Y1~Y12)的类别分布子图，为后续标签选择提供可视化依据。\n",
    "\n",
    "    Description:\n",
    "        1. 在 3x4 网格中为每个标签绘制柱状图，展示各类别的样本数与占比。\n",
    "        2. 在每个子图标题中标注唯一值数和缺失率。\n",
    "        3. 评估每个标签的类别不平衡比并汇总输出。\n",
    "        4. 将图片保存至 save_dir/label_distributions_all.png。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据，需包含 Y1~Y12 标签列。\n",
    "        label_cols : list of str\n",
    "            标签列名列表，长度为 12，如 ['Y1', 'Y2', ..., 'Y12']。\n",
    "        save_dir : str\n",
    "            图片保存目录路径，标量字符串，默认为 IMG_DIR。\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "            无返回值，图片保存至文件并在 notebook 中显示。\n",
    "    \"\"\"\n",
    "    cols_in_df = [c for c in label_cols if c in df.columns]\n",
    "    n_labels = len(cols_in_df)\n",
    "    ncols = 4\n",
    "    nrows = (n_labels + ncols - 1) // ncols\n",
    "    colors = ['#4CAF50', '#FF9800', '#F44336', '#2196F3', '#9C27B0', '#795548']\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(18, 4 * nrows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    summary_lines = []\n",
    "\n",
    "    for i, col in enumerate(cols_in_df):\n",
    "        ax = axes[i]\n",
    "        s = df[col].dropna()\n",
    "        vc = s.value_counts().sort_index()\n",
    "        total = len(s)\n",
    "        missing_pct = df[col].isnull().mean() * 100\n",
    "\n",
    "        bars = ax.bar([str(v) for v in vc.index], vc.values,\n",
    "                      color=colors[:len(vc)], alpha=0.85, edgecolor='black', linewidth=0.3)\n",
    "\n",
    "        # 柱顶标注百分比\n",
    "        for bar, (val, cnt) in zip(bars, vc.items()):\n",
    "            pct = cnt / total * 100\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
    "                    f'{pct:.0f}%', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "        ax.set_title(f'{col} (唯一值={vc.nunique()}, 缺失={missing_pct:.1f}%)', fontsize=9)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('样本数' if i % ncols == 0 else '')\n",
    "        ax.grid(True, alpha=0.2, axis='y')\n",
    "        ax.tick_params(axis='both', labelsize=7)\n",
    "\n",
    "        # 汇总信息\n",
    "        imbalance_ratio = vc.max() / vc.min() if vc.min() > 0 else float('inf')\n",
    "        summary_lines.append(\n",
    "            f\"  {col}: 唯一值={vc.nunique()}, 缺失率={missing_pct:.2f}%, \"\n",
    "            f\"不平衡比={imbalance_ratio:.2f}:1\"\n",
    "        )\n",
    "\n",
    "    # 隐藏多余子图\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    fig.suptitle('标签字段 Y1~Y12 类别分布总览', fontsize=14, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'label_distributions_all.png')\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[OK] 标签分布总览图已保存: {save_path}\")\n",
    "\n",
    "    # 输出汇总\n",
    "    print(f\"\\n[SUMMARY] 各标签统计:\")\n",
    "    for line in summary_lines:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "# ---- 执行可视化 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 1.3: 数据概况可视化\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[1/3] 收盘价时间序列趋势图\")\n",
    "plot_close_price_trend(df)\n",
    "\n",
    "print(\"\\n[2/3] 特征字段缺失值热力图\")\n",
    "plot_missing_heatmap(df, FEATURE_COLS)\n",
    "\n",
    "print(\"\\n[3/3] 标签字段 Y1~Y12 分布直方图\")\n",
    "plot_all_label_distributions(df, LABEL_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8647f9d",
   "metadata": {},
   "source": [
    "### 1.4 数据泄漏初步检测\n",
    "检测以下潜在的数据泄漏风险：\n",
    "1. **时序字段混乱** — 检查是否存在 `start_time > end_time` 的异常记录；\n",
    "2. **时间逻辑错误** — 检查 `trade_date` 与 `start_time`/`end_time` 的日期是否一致；\n",
    "3. **特征-标签极端相关性** — 对 Y1~Y12 所有标签，计算特征的 Pearson 相关系数，识别 |r| > 0.95 的可疑特征；\n",
    "4. **未来数据泄漏** — 检查同一 underlying 下是否存在时间交叉或乱序情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f27c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 1.4: 数据泄漏初步检测\n",
      "============================================================\n",
      "\n",
      "[1/2] 时序字段逻辑检查\n",
      "----------------------------------------\n",
      "  start_time > end_time 异常记录:    0\n",
      "  trade_date 与 start_time 日期不一致: 43978\n",
      "  时间未排序的 underlying 分组:      0/70\n",
      "  [WARN] 发现 43978 条记录 trade_date 与 start_time 日期不一致\n",
      "\n",
      "[2/2] 特征与标签 Y1~Y12 的极端相关性检查 (阈值 |r| > 0.95)\n",
      "----------------------------------------\n",
      "  [OK] 未发现与任何标签相关系数绝对值 > 0.95 的特征\n",
      "\n",
      "  特征-标签相关系数分布统计 (各标签):\n",
      "  标签      |r|>0.9  |r|>0.5  |r|<0.01       均值      标准差\n",
      "  --------------------------------------------------\n",
      "  Y1            0        0       101   0.0166   0.0398\n",
      "  Y2            0        0       115   0.0140   0.0346\n",
      "  Y3            0        0       129   0.0091   0.0259\n",
      "  Y4            0        0       182   0.0021   0.0157\n",
      "  Y5            0        0        99   0.0181   0.0412\n",
      "  Y6            0        0       113   0.0142   0.0346\n",
      "  Y7            0        0       148   0.0093   0.0256\n",
      "  Y8            0        0       188   0.0032   0.0141\n",
      "  Y9            0        0        94   0.0167   0.0402\n",
      "  Y10           0        0       107   0.0139   0.0344\n",
      "  Y11           0        0       140   0.0096   0.0248\n",
      "  Y12           0        0       157   0.0042   0.0167\n",
      "\n",
      "  [OK] 相关系数分布图已保存: images\\correlation_distribution.png\n",
      "\n",
      "============================================================\n",
      "[WARN] 数据泄漏检测发现潜在问题，请关注上述警告\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_time_field_logic(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    检测时序字段的逻辑一致性，识别时间混乱和日期不匹配的记录。\n",
    "\n",
    "    Description:\n",
    "        1. 检查 start_time > end_time 的异常记录数。\n",
    "        2. 检查 trade_date 的日期部分是否与 start_time 的日期部分一致。\n",
    "        3. 检查同一 underlying 内时间序列是否严格递增。\n",
    "        4. 返回检测结果汇总字典。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据，需包含\n",
    "            'trade_date', 'start_time', 'end_time', 'underlying' 列。\n",
    "\n",
    "    Returns:\n",
    "        result : dict\n",
    "            包含以下键的检测结果字典:\n",
    "            - 'start_gt_end_count': int, start_time > end_time 的记录数\n",
    "            - 'date_mismatch_count': int, trade_date 与 start_time 日期不一致的记录数\n",
    "            - 'time_not_sorted_groups': int, 时间未严格排序的 underlying 分组数\n",
    "            - 'total_groups': int, underlying 分组总数\n",
    "            - 'issues': list of str, 发现的问题描述列表\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'start_gt_end_count': 0,\n",
    "        'date_mismatch_count': 0,\n",
    "        'time_not_sorted_groups': 0,\n",
    "        'total_groups': 0,\n",
    "        'issues': []\n",
    "    }\n",
    "\n",
    "    # 解析时间字段\n",
    "    try:\n",
    "        start_ts = pd.to_datetime(df['start_time'])\n",
    "        end_ts = pd.to_datetime(df['end_time'])\n",
    "        trade_dt = pd.to_datetime(df['trade_date'])\n",
    "    except Exception as e:\n",
    "        result['issues'].append(f\"时间字段解析失败: {e}\")\n",
    "        return result\n",
    "\n",
    "    # 1. 检查 start_time > end_time\n",
    "    bad_order = start_ts > end_ts\n",
    "    result['start_gt_end_count'] = int(bad_order.sum())\n",
    "    if result['start_gt_end_count'] > 0:\n",
    "        result['issues'].append(\n",
    "            f\"发现 {result['start_gt_end_count']} 条记录 start_time > end_time\"\n",
    "        )\n",
    "        bad_idx = df[bad_order].index[:5]\n",
    "        print(f\"  [WARN] start_time > end_time 异常示例 (前5条):\")\n",
    "        for idx in bad_idx:\n",
    "            print(f\"    行{idx}: start={df.loc[idx, 'start_time']}, end={df.loc[idx, 'end_time']}\")\n",
    "\n",
    "    # 2. 检查 trade_date 与 start_time 日期是否一致\n",
    "    start_date = start_ts.dt.date\n",
    "    trade_date = trade_dt.dt.date\n",
    "    date_mismatch = start_date != trade_date\n",
    "    result['date_mismatch_count'] = int(date_mismatch.sum())\n",
    "    if result['date_mismatch_count'] > 0:\n",
    "        result['issues'].append(\n",
    "            f\"发现 {result['date_mismatch_count']} 条记录 trade_date 与 start_time 日期不一致\"\n",
    "        )\n",
    "\n",
    "    # 3. 检查同一 underlying 内 start_time 是否递增\n",
    "    if 'underlying' in df.columns:\n",
    "        groups = df.groupby('underlying')\n",
    "        result['total_groups'] = groups.ngroups\n",
    "        not_sorted = 0\n",
    "        for name, group in groups:\n",
    "            ts = pd.to_datetime(group['start_time'])\n",
    "            if not ts.is_monotonic_increasing:\n",
    "                not_sorted += 1\n",
    "        result['time_not_sorted_groups'] = not_sorted\n",
    "        if not_sorted > 0:\n",
    "            result['issues'].append(\n",
    "                f\"发现 {not_sorted}/{result['total_groups']} 个 underlying 分组时间未严格递增\"\n",
    "            )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def check_feature_label_leakage(df: pd.DataFrame, feature_cols: list,\n",
    "                                 label_cols: list, threshold: float = 0.95) -> dict:\n",
    "    \"\"\"\n",
    "    检测特征与所有标签(Y1~Y12)之间是否存在极端相关性（疑似数据泄漏）。\n",
    "\n",
    "    Description:\n",
    "        1. 对每个标签列，计算所有特征的 Pearson 相关系数（忽略缺失值）。\n",
    "        2. 筛选相关系数绝对值 > threshold 的可疑特征。\n",
    "        3. 返回各标签的可疑特征列表及相关系数汇总。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据。\n",
    "        feature_cols : list of str\n",
    "            特征列名列表，长度最多为 300，如 ['X1', 'X2', ..., 'X300']。\n",
    "        label_cols : list of str\n",
    "            标签列名列表，长度最多为 12，如 ['Y1', 'Y2', ..., 'Y12']。\n",
    "        threshold : float\n",
    "            相关系数绝对值阈值，标量浮点数，默认为 0.95。\n",
    "\n",
    "    Returns:\n",
    "        result : dict\n",
    "            包含以下键的字典:\n",
    "            - 'by_label': dict, 键为标签名，值为 pd.DataFrame (n_suspicious, 2)\n",
    "              含 'feature' 和 'correlation' 列\n",
    "            - 'all_correlations': pd.DataFrame, 形状为 (n_features, n_labels),\n",
    "              所有特征与所有标签的相关系数矩阵\n",
    "            - 'total_suspicious': int, 全部可疑（特征,标签）对数\n",
    "    \"\"\"\n",
    "    cols_in_df = [c for c in feature_cols if c in df.columns]\n",
    "    labels_in_df = [c for c in label_cols if c in df.columns]\n",
    "\n",
    "    result = {'by_label': {}, 'total_suspicious': 0}\n",
    "\n",
    "    # 计算完整相关系数矩阵\n",
    "    all_corr_data = {}\n",
    "    for label in labels_in_df:\n",
    "        corrs = df[cols_in_df].corrwith(df[label]).dropna()\n",
    "        all_corr_data[label] = corrs\n",
    "\n",
    "        # 筛选可疑特征\n",
    "        suspicious_mask = corrs.abs() > threshold\n",
    "        if suspicious_mask.any():\n",
    "            suspicious = pd.DataFrame({\n",
    "                'feature': corrs[suspicious_mask].index,\n",
    "                'correlation': corrs[suspicious_mask].values\n",
    "            }).sort_values('correlation', key=abs, ascending=False).reset_index(drop=True)\n",
    "            result['by_label'][label] = suspicious\n",
    "            result['total_suspicious'] += len(suspicious)\n",
    "\n",
    "    result['all_correlations'] = pd.DataFrame(all_corr_data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_leakage_detection(df: pd.DataFrame, feature_cols: list, label_cols: list) -> dict:\n",
    "    \"\"\"\n",
    "    执行完整的数据泄漏初步检测流程。\n",
    "\n",
    "    Description:\n",
    "        汇总运行时序逻辑检查和特征-标签(Y1~Y12)极端相关性检查，输出综合报告。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据。\n",
    "        feature_cols : list of str\n",
    "            特征列名列表，长度最多为 300。\n",
    "        label_cols : list of str\n",
    "            标签列名列表，长度最多为 12。\n",
    "\n",
    "    Returns:\n",
    "        leakage_report : dict\n",
    "            包含以下键的字典:\n",
    "            - 'time_check': dict, 时序逻辑检查结果\n",
    "            - 'correlation_check': dict, 特征-标签极端相关性检查结果\n",
    "            - 'has_issues': bool, 是否发现任何问题\n",
    "    \"\"\"\n",
    "    leakage_report = {}\n",
    "\n",
    "    # ---- 1. 时序逻辑检查 ----\n",
    "    print(\"[1/2] 时序字段逻辑检查\")\n",
    "    print(\"-\" * 40)\n",
    "    time_result = check_time_field_logic(df)\n",
    "    leakage_report['time_check'] = time_result\n",
    "\n",
    "    print(f\"  start_time > end_time 异常记录:    {time_result['start_gt_end_count']}\")\n",
    "    print(f\"  trade_date 与 start_time 日期不一致: {time_result['date_mismatch_count']}\")\n",
    "    if time_result['total_groups'] > 0:\n",
    "        print(f\"  时间未排序的 underlying 分组:      \"\n",
    "              f\"{time_result['time_not_sorted_groups']}/{time_result['total_groups']}\")\n",
    "\n",
    "    if not time_result['issues']:\n",
    "        print(\"  [OK] 时序字段逻辑检查通过，未发现异常\")\n",
    "    else:\n",
    "        for issue in time_result['issues']:\n",
    "            print(f\"  [WARN] {issue}\")\n",
    "\n",
    "    # ---- 2. 特征-标签极端相关性检查 ----\n",
    "    print(f\"\\n[2/2] 特征与标签 Y1~Y12 的极端相关性检查 (阈值 |r| > 0.95)\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    corr_result = check_feature_label_leakage(df, feature_cols, label_cols, threshold=0.95)\n",
    "    leakage_report['correlation_check'] = corr_result\n",
    "\n",
    "    if corr_result['total_suspicious'] == 0:\n",
    "        print(f\"  [OK] 未发现与任何标签相关系数绝对值 > 0.95 的特征\")\n",
    "    else:\n",
    "        print(f\"  [WARN] 共发现 {corr_result['total_suspicious']} 对可疑（特征,标签）组合:\")\n",
    "        for label, sus_df in corr_result['by_label'].items():\n",
    "            for _, row in sus_df.iterrows():\n",
    "                print(f\"    {row['feature']} <-> {label}: r = {row['correlation']:.4f}\")\n",
    "\n",
    "    # 汇总各标签的相关系数分布\n",
    "    corr_df = corr_result['all_correlations']\n",
    "    print(f\"\\n  特征-标签相关系数分布统计 (各标签):\")\n",
    "    print(f\"  {'标签':<6} {'|r|>0.9':>8} {'|r|>0.5':>8} {'|r|<0.01':>9} {'均值':>8} {'标准差':>8}\")\n",
    "    print(f\"  {'-'*50}\")\n",
    "    for label in corr_df.columns:\n",
    "        c = corr_df[label].dropna()\n",
    "        print(f\"  {label:<6} {(c.abs() > 0.9).sum():>8} {(c.abs() > 0.5).sum():>8} \"\n",
    "              f\"{(c.abs() < 0.01).sum():>9} {c.mean():>8.4f} {c.std():>8.4f}\")\n",
    "\n",
    "    # 绘制相关系数热力图 — 挑选3个代表性标签\n",
    "    sample_labels = corr_df.columns[:3].tolist()  # Y1, Y2, Y3\n",
    "    fig, axes = plt.subplots(1, len(sample_labels), figsize=(16, 4))\n",
    "    if len(sample_labels) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, label in zip(axes, sample_labels):\n",
    "        c = corr_df[label].dropna()\n",
    "        ax.hist(c.values, bins=50, color='steelblue', alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "        ax.axvline(0, color='red', linestyle='--', linewidth=0.8)\n",
    "        ax.axvline(0.95, color='orange', linestyle='--', linewidth=0.8)\n",
    "        ax.axvline(-0.95, color='orange', linestyle='--', linewidth=0.8)\n",
    "        ax.set_title(f'特征 vs {label} 相关系数', fontsize=10)\n",
    "        ax.set_xlabel('Pearson r')\n",
    "        ax.set_ylabel('特征数')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(IMG_DIR, 'correlation_distribution.png')\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\n  [OK] 相关系数分布图已保存: {save_path}\")\n",
    "\n",
    "    # 综合判断\n",
    "    has_issues = bool(time_result['issues']) or corr_result['total_suspicious'] > 0\n",
    "    leakage_report['has_issues'] = has_issues\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    if has_issues:\n",
    "        print(\"[WARN] 数据泄漏检测发现潜在问题，请关注上述警告\")\n",
    "    else:\n",
    "        print(\"[OK] 数据泄漏初步检测通过，未发现明显风险\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return leakage_report\n",
    "\n",
    "\n",
    "# ---- 执行数据泄漏检测 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 1.4: 数据泄漏初步检测\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "leakage_report = run_leakage_detection(df, FEATURE_COLS, LABEL_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0512c",
   "metadata": {},
   "source": [
    "## 步骤2：自动特征诊断\n",
    "\n",
    "对 X1~X300 共 300 个特征逐字段进行全面诊断，内容包括：\n",
    "- **缺失值分析**：缺失比例、缺失模式（随机/时序连续缺失）\n",
    "- **异常值检测**：IQR 法 + 3-sigma 检测，兼顾时序趋势避免误判\n",
    "- **分布特征**：偏度/峰度、正态性检验（Shapiro-Wilk 抽样）、ADF 平稳性检验\n",
    "- **与标签相关性**：所有 Y1~Y12 的 Pearson/Spearman 相关系数及显著性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc5a11",
   "metadata": {},
   "source": [
    "### 2.1 逐特征诊断：缺失值 / 异常值 / 分布 / 相关性\n",
    "遍历 X1~X300，为每个特征生成完整的诊断记录，汇总到 `diag_records` 列表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51474ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 2.1: 逐特征诊断\n",
      "============================================================\n",
      "\n",
      "  [PROGRESS] 50/300 个特征诊断完成\n",
      "  [PROGRESS] 100/300 个特征诊断完成\n",
      "  [PROGRESS] 150/300 个特征诊断完成\n",
      "  [PROGRESS] 200/300 个特征诊断完成\n",
      "  [PROGRESS] 250/300 个特征诊断完成\n",
      "  [PROGRESS] 300/300 个特征诊断完成\n",
      "\n",
      "[OK] 诊断完成: 共 300 个特征\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 逐特征诊断 — 缺失值 / 异常值 / 分布 / 相关性\n",
    "# =============================================================================\n",
    "from scipy import stats as sp_stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "def diagnose_missing(series: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    诊断单个特征的缺失值情况，包括缺失比例和缺失模式。\n",
    "\n",
    "    Description:\n",
    "        1. 计算缺失数量和缺失比例。\n",
    "        2. 通过连续缺失段长度判断缺失模式:\n",
    "           - 最长连续缺失段 >= 50 且占总缺失 >= 30% 视为 \"时序连续缺失\"\n",
    "           - 否则视为 \"随机缺失\"\n",
    "\n",
    "    Parameters:\n",
    "        series : pd.Series\n",
    "            形状为 (n_samples,) 的单列特征数据。\n",
    "\n",
    "    Returns:\n",
    "        result : dict\n",
    "            - 'missing_count': int, 缺失值数量\n",
    "            - 'missing_pct': float, 缺失比例(%)\n",
    "            - 'missing_pattern': str, '随机缺失' 或 '时序连续缺失' 或 '无缺失'\n",
    "            - 'max_consecutive_missing': int, 最长连续缺失段长度\n",
    "    \"\"\"\n",
    "    n = len(series)\n",
    "    missing_count = int(series.isnull().sum())\n",
    "    missing_pct = round(missing_count / n * 100, 2) if n > 0 else 0.0\n",
    "\n",
    "    if missing_count == 0:\n",
    "        return {\n",
    "            'missing_count': 0, 'missing_pct': 0.0,\n",
    "            'missing_pattern': '无缺失', 'max_consecutive_missing': 0\n",
    "        }\n",
    "\n",
    "    # 计算最长连续缺失段\n",
    "    is_null = series.isnull().astype(int).values\n",
    "    max_consec = 0\n",
    "    current = 0\n",
    "    for v in is_null:\n",
    "        if v == 1:\n",
    "            current += 1\n",
    "            max_consec = max(max_consec, current)\n",
    "        else:\n",
    "            current = 0\n",
    "\n",
    "    # 判断缺失模式\n",
    "    if max_consec >= 50 and max_consec / missing_count >= 0.3:\n",
    "        pattern = '时序连续缺失'\n",
    "    else:\n",
    "        pattern = '随机缺失'\n",
    "\n",
    "    return {\n",
    "        'missing_count': missing_count,\n",
    "        'missing_pct': missing_pct,\n",
    "        'missing_pattern': pattern,\n",
    "        'max_consecutive_missing': max_consec\n",
    "    }\n",
    "\n",
    "\n",
    "def diagnose_outliers(series: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    基于 IQR 法和 3-sigma 法检测单个特征的异常值。\n",
    "\n",
    "    Description:\n",
    "        1. IQR 法: Q1 - 1.5*IQR 和 Q3 + 1.5*IQR 之外为异常。\n",
    "        2. 3-sigma 法: 均值 +/- 3倍标准差之外为异常。\n",
    "        3. 取两种方法的并集作为最终异常值集合。\n",
    "        4. 对时序数据做差分后再检测，避免趋势导致误判。\n",
    "\n",
    "    Parameters:\n",
    "        series : pd.Series\n",
    "            形状为 (n_samples,) 的单列特征数据（可含 NaN）。\n",
    "\n",
    "    Returns:\n",
    "        result : dict\n",
    "            - 'outlier_count_iqr': int, IQR 法检出异常值数\n",
    "            - 'outlier_count_3sigma': int, 3-sigma 法检出异常值数\n",
    "            - 'outlier_count_union': int, 并集异常值数\n",
    "            - 'outlier_pct': float, 并集异常值占有效样本(%)\n",
    "            - 'trend_adjusted_outlier_pct': float, 差分后异常值占比(%)\n",
    "    \"\"\"\n",
    "    s = series.dropna()\n",
    "    n = len(s)\n",
    "    if n < 10:\n",
    "        return {\n",
    "            'outlier_count_iqr': 0, 'outlier_count_3sigma': 0,\n",
    "            'outlier_count_union': 0, 'outlier_pct': 0.0,\n",
    "            'trend_adjusted_outlier_pct': 0.0\n",
    "        }\n",
    "\n",
    "    # IQR 法\n",
    "    q1 = s.quantile(0.25)\n",
    "    q3 = s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_iqr = q1 - 1.5 * iqr\n",
    "    upper_iqr = q3 + 1.5 * iqr\n",
    "    mask_iqr = (s < lower_iqr) | (s > upper_iqr)\n",
    "\n",
    "    # 3-sigma 法\n",
    "    mean = s.mean()\n",
    "    std = s.std()\n",
    "    if std > 0:\n",
    "        lower_3s = mean - 3 * std\n",
    "        upper_3s = mean + 3 * std\n",
    "        mask_3s = (s < lower_3s) | (s > upper_3s)\n",
    "    else:\n",
    "        mask_3s = pd.Series(False, index=s.index)\n",
    "\n",
    "    # 并集\n",
    "    mask_union = mask_iqr | mask_3s\n",
    "\n",
    "    # 差分后检测（去趋势）\n",
    "    diff_s = s.diff().dropna()\n",
    "    trend_outlier_pct = 0.0\n",
    "    if len(diff_s) > 10:\n",
    "        dq1 = diff_s.quantile(0.25)\n",
    "        dq3 = diff_s.quantile(0.75)\n",
    "        diqr = dq3 - dq1\n",
    "        if diqr > 0:\n",
    "            mask_diff = (diff_s < dq1 - 1.5 * diqr) | (diff_s > dq3 + 1.5 * diqr)\n",
    "            trend_outlier_pct = round(mask_diff.sum() / len(diff_s) * 100, 2)\n",
    "\n",
    "    return {\n",
    "        'outlier_count_iqr': int(mask_iqr.sum()),\n",
    "        'outlier_count_3sigma': int(mask_3s.sum()),\n",
    "        'outlier_count_union': int(mask_union.sum()),\n",
    "        'outlier_pct': round(mask_union.sum() / n * 100, 2),\n",
    "        'trend_adjusted_outlier_pct': trend_outlier_pct\n",
    "    }\n",
    "\n",
    "\n",
    "def diagnose_distribution(series: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    诊断单个特征的分布特征：偏度、峰度、正态性检验和 ADF 平稳性检验。\n",
    "\n",
    "    Description:\n",
    "        1. 计算偏度(skewness)和峰度(kurtosis)。\n",
    "        2. Shapiro-Wilk 正态性检验（抽样 5000 条以提高效率）。\n",
    "        3. ADF 检验判断时序平稳性（抽样 10000 条）。\n",
    "\n",
    "    Parameters:\n",
    "        series : pd.Series\n",
    "            形状为 (n_samples,) 的单列特征数据（可含 NaN）。\n",
    "\n",
    "    Returns:\n",
    "        result : dict\n",
    "            - 'skewness': float, 偏度\n",
    "            - 'kurtosis': float, 峰度\n",
    "            - 'is_normal': bool, Shapiro p > 0.05 判为正态\n",
    "            - 'shapiro_p': float, Shapiro-Wilk 检验 p 值\n",
    "            - 'is_stationary': bool, ADF p < 0.05 判为平稳\n",
    "            - 'adf_p': float, ADF 检验 p 值\n",
    "    \"\"\"\n",
    "    s = series.dropna()\n",
    "    n = len(s)\n",
    "\n",
    "    result = {\n",
    "        'skewness': 0.0, 'kurtosis': 0.0,\n",
    "        'is_normal': False, 'shapiro_p': 0.0,\n",
    "        'is_stationary': False, 'adf_p': 1.0\n",
    "    }\n",
    "\n",
    "    if n < 20:\n",
    "        return result\n",
    "\n",
    "    result['skewness'] = round(float(s.skew()), 4)\n",
    "    result['kurtosis'] = round(float(s.kurtosis()), 4)\n",
    "\n",
    "    # Shapiro-Wilk 正态性检验（抽样）\n",
    "    try:\n",
    "        sample = s.sample(min(5000, n), random_state=42)\n",
    "        _, p_shapiro = sp_stats.shapiro(sample)\n",
    "        result['shapiro_p'] = round(float(p_shapiro), 6)\n",
    "        result['is_normal'] = p_shapiro > 0.05\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ADF 平稳性检验（抽样）\n",
    "    try:\n",
    "        adf_sample = s.iloc[:min(10000, n)]\n",
    "        adf_result = adfuller(adf_sample, autolag='AIC', maxlag=20)\n",
    "        p_adf = adf_result[1]\n",
    "        result['adf_p'] = round(float(p_adf), 6)\n",
    "        result['is_stationary'] = p_adf < 0.05\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def diagnose_correlation(series: pd.Series, df: pd.DataFrame,\n",
    "                          label_cols: list) -> dict:\n",
    "    \"\"\"\n",
    "    计算单个特征与所有标签(Y1~Y12)的 Pearson/Spearman 相关系数及显著性。\n",
    "\n",
    "    Description:\n",
    "        对每个标签列，计算:\n",
    "        1. Pearson 相关系数 + p 值\n",
    "        2. Spearman 秩相关系数 + p 值\n",
    "        保留相关系数绝对值最大的标签及其详细指标。\n",
    "\n",
    "    Parameters:\n",
    "        series : pd.Series\n",
    "            形状为 (n_samples,) 的单列特征数据（可含 NaN）。\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的完整数据，需包含 label_cols 中的列。\n",
    "        label_cols : list of str\n",
    "            标签列名列表，长度最多 12，如 ['Y1', ..., 'Y12']。\n",
    "\n",
    "    Returns:\n",
    "        result : dict\n",
    "            - 'best_label': str, 相关性最强的标签名\n",
    "            - 'best_pearson_r': float, 对应 Pearson 相关系数\n",
    "            - 'best_pearson_p': float, 对应 Pearson p 值\n",
    "            - 'best_spearman_r': float, 对应 Spearman 相关系数\n",
    "            - 'best_spearman_p': float, 对应 Spearman p 值\n",
    "            - 'all_pearson': dict, {label: r} 所有标签的 Pearson 系数\n",
    "            - 'all_spearman': dict, {label: r} 所有标签的 Spearman 系数\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'best_label': '', 'best_pearson_r': 0.0, 'best_pearson_p': 1.0,\n",
    "        'best_spearman_r': 0.0, 'best_spearman_p': 1.0,\n",
    "        'all_pearson': {}, 'all_spearman': {}\n",
    "    }\n",
    "\n",
    "    best_abs_r = 0.0\n",
    "    feat = series.dropna()\n",
    "\n",
    "    for label in label_cols:\n",
    "        if label not in df.columns:\n",
    "            continue\n",
    "        # 对齐非缺失索引\n",
    "        common_idx = feat.index.intersection(df[label].dropna().index)\n",
    "        if len(common_idx) < 30:\n",
    "            continue\n",
    "\n",
    "        x = feat.loc[common_idx].values\n",
    "        y = df[label].loc[common_idx].values\n",
    "\n",
    "        try:\n",
    "            pr, pp = sp_stats.pearsonr(x, y)\n",
    "        except Exception:\n",
    "            pr, pp = 0.0, 1.0\n",
    "        try:\n",
    "            sr, sp_val = sp_stats.spearmanr(x, y)\n",
    "        except Exception:\n",
    "            sr, sp_val = 0.0, 1.0\n",
    "\n",
    "        result['all_pearson'][label] = round(float(pr), 4)\n",
    "        result['all_spearman'][label] = round(float(sr), 4)\n",
    "\n",
    "        if abs(pr) > best_abs_r:\n",
    "            best_abs_r = abs(pr)\n",
    "            result['best_label'] = label\n",
    "            result['best_pearson_r'] = round(float(pr), 4)\n",
    "            result['best_pearson_p'] = round(float(pp), 6)\n",
    "            result['best_spearman_r'] = round(float(sr), 4)\n",
    "            result['best_spearman_p'] = round(float(sp_val), 6)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_feature_diagnosis(df: pd.DataFrame, feature_cols: list,\n",
    "                           label_cols: list) -> list:\n",
    "    \"\"\"\n",
    "    对所有特征执行完整诊断流程，返回结构化诊断记录列表。\n",
    "\n",
    "    Description:\n",
    "        遍历 feature_cols 中的每个特征，依次调用:\n",
    "        diagnose_missing / diagnose_outliers / diagnose_distribution /\n",
    "        diagnose_correlation，将结果合并为一条扁平化字典记录。\n",
    "        每处理 50 个特征输出一次进度。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的完整数据。\n",
    "        feature_cols : list of str\n",
    "            特征列名列表，最多 300 个，如 ['X1', ..., 'X300']。\n",
    "        label_cols : list of str\n",
    "            标签列名列表，最多 12 个，如 ['Y1', ..., 'Y12']。\n",
    "\n",
    "    Returns:\n",
    "        diag_records : list of dict\n",
    "            长度为 n_features 的列表，每条记录为一个字典，包含:\n",
    "            - 'feature': str, 特征名\n",
    "            - 缺失值诊断的全部字段\n",
    "            - 异常值诊断的全部字段\n",
    "            - 分布诊断的全部字段\n",
    "            - 相关性诊断的全部字段\n",
    "    \"\"\"\n",
    "    diag_records = []\n",
    "    total = len(feature_cols)\n",
    "\n",
    "    for i, col in enumerate(feature_cols):\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        series = df[col]\n",
    "        record = {'feature': col}\n",
    "\n",
    "        # 1. 缺失值诊断\n",
    "        record.update(diagnose_missing(series))\n",
    "        # 2. 异常值检测\n",
    "        record.update(diagnose_outliers(series))\n",
    "        # 3. 分布特征\n",
    "        record.update(diagnose_distribution(series))\n",
    "        # 4. 与标签的相关性\n",
    "        record.update(diagnose_correlation(series, df, label_cols))\n",
    "\n",
    "        diag_records.append(record)\n",
    "\n",
    "        if (i + 1) % 50 == 0 or (i + 1) == total:\n",
    "            print(f\"  [PROGRESS] {i+1}/{total} 个特征诊断完成\")\n",
    "\n",
    "    return diag_records\n",
    "\n",
    "\n",
    "# ---- 执行诊断 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 2.1: 逐特征诊断\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "diag_records = run_feature_diagnosis(df, FEATURE_COLS, LABEL_COLS)\n",
    "print(f\"\\n[OK] 诊断完成: 共 {len(diag_records)} 个特征\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f3073",
   "metadata": {},
   "source": [
    "### 2.2 结构化特征诊断报告\n",
    "将诊断结果整理为 DataFrame 表格，按 \"特征名 — 缺失值 — 异常值 — 分布 — 与标签相关性\" 结构化输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13797050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "特征诊断报告 -- 汇总统计\n",
      "============================================================\n",
      "\n",
      "[COUNT] 诊断特征总数: 300\n",
      "\n",
      "[MISSING] 缺失值分布:\n",
      "  无缺失:      0\n",
      "  低缺失(<=5%): 0\n",
      "  中缺失(5~30%): 300\n",
      "  高缺失(>30%): 0\n",
      "  时序连续缺失: 0\n",
      "\n",
      "[OUTLIER] 异常值:\n",
      "  平均异常值占比: 6.37%\n",
      "  异常值占比>5%的特征: 129\n",
      "\n",
      "[DIST] 分布特征:\n",
      "  通过正态检验: 10/300\n",
      "  时序平稳:     300/300\n",
      "  平均偏度: 3.8438, 平均峰度: 1512.9559\n",
      "\n",
      "[CORR] 与标签相关性:\n",
      "  |Pearson r| > 0.3: 0\n",
      "  Pearson p < 0.05 (显著): 282\n",
      "\n",
      "[RISK] 综合风险等级:\n",
      "  低: 0\n",
      "  中: 200\n",
      "  高: 100\n",
      "============================================================\n",
      "\n",
      "[TABLE] 诊断报告 Top20 (按缺失率降序):\n",
      " 特征名  缺失率(%) 缺失模式  异常值占比(%)       偏度         峰度  平稳(ADF) 最强相关标签  最强Pearson_r 风险等级\n",
      "X170 24.6800 随机缺失    1.3400 113.2731 13910.7393     True    Y12      -0.0102    中\n",
      "X200 24.6600 随机缺失    5.6100   0.1020    24.7083     True    Y12       0.0072    中\n",
      "X199 24.6600 随机缺失    3.3500   0.6048     1.3712     True     Y5       0.1260    中\n",
      "X198 24.6600 随机缺失    3.2800   0.6769     1.5248     True     Y5       0.1349    中\n",
      "X197 24.6600 随机缺失    3.2100   0.8050     1.8843     True     Y1       0.1475    中\n",
      "X196 24.6600 随机缺失    4.4000   2.4737    15.8261     True     Y1       0.1323    中\n",
      "X195 24.6600 随机缺失    4.0200   1.5493    10.3727     True    Y12      -0.0172    中\n",
      "X194 24.6600 随机缺失    4.8100   1.8201    23.2969     True     Y8      -0.0180    中\n",
      "X193 24.6600 随机缺失    0.0000   0.1079    -1.9188     True     Y6       0.0087    中\n",
      "X167 24.6600 随机缺失   24.5900 197.5648 40997.3973     True    Y12      -0.0113    高\n",
      "X166 24.6600 随机缺失    1.7700  -0.1512     0.9505     True     Y8      -0.0096    中\n",
      "X165 24.6600 随机缺失    6.2300  -0.0068    24.5943     True     Y5       0.0245    中\n",
      "X164 24.6600 随机缺失    0.0200   0.7144    -0.5213     True     Y1      -0.0528    中\n",
      "X163 24.6600 随机缺失    4.9500   0.6401     3.0403     True     Y5       0.0751    中\n",
      "X162 24.6600 随机缺失   15.6600   0.2824  1656.2955     True     Y6      -0.0055    高\n",
      "X161 24.6600 随机缺失   14.7600   7.6229   535.2199     True    Y11       0.0183    高\n",
      "X192 24.6600 随机缺失   13.0100   3.9395    17.3546     True     Y9       0.0189    高\n",
      "X184 24.6600 随机缺失   13.2000  -3.0421    67.0392     True     Y1       0.0175    高\n",
      "X177 24.6600 随机缺失    2.2300   0.1819     2.9254     True     Y7       0.0121    中\n",
      "X178 24.6600 随机缺失    2.9300   3.8816   474.6348     True     Y1      -0.0097    中\n",
      "\n",
      "[OK] 诊断报告已保存: feature_diagnosis_report.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 结构化特征诊断报告\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def build_diagnosis_report(diag_records: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    将逐特征诊断记录列表转换为结构化报告 DataFrame。\n",
    "\n",
    "    Description:\n",
    "        1. 从 diag_records 中提取核心字段。\n",
    "        2. 按缺失率降序排列。\n",
    "        3. 添加综合风险等级列（基于缺失率 + 异常值占比 + 分布偏度）。\n",
    "        4. 输出汇总统计信息。\n",
    "\n",
    "    Parameters:\n",
    "        diag_records : list of dict\n",
    "            长度为 n_features 的诊断记录列表，每条记录来自\n",
    "            run_feature_diagnosis 的输出。\n",
    "\n",
    "    Returns:\n",
    "        report_df : pd.DataFrame\n",
    "            形状为 (n_features, n_report_cols) 的结构化报告表，\n",
    "            包含特征名、缺失值、异常值、分布、相关性等列。\n",
    "    \"\"\"\n",
    "    # 提取核心字段构建报告\n",
    "    rows = []\n",
    "    for rec in diag_records:\n",
    "        rows.append({\n",
    "            '特征名': rec['feature'],\n",
    "            # --- 缺失值 ---\n",
    "            '缺失率(%)': rec['missing_pct'],\n",
    "            '缺失模式': rec['missing_pattern'],\n",
    "            '最长连续缺失': rec['max_consecutive_missing'],\n",
    "            # --- 异常值 ---\n",
    "            '异常值数(IQR)': rec['outlier_count_iqr'],\n",
    "            '异常值数(3sigma)': rec['outlier_count_3sigma'],\n",
    "            '异常值占比(%)': rec['outlier_pct'],\n",
    "            '去趋势异常(%)': rec['trend_adjusted_outlier_pct'],\n",
    "            # --- 分布 ---\n",
    "            '偏度': rec['skewness'],\n",
    "            '峰度': rec['kurtosis'],\n",
    "            '正态(Shapiro)': rec['is_normal'],\n",
    "            'Shapiro_p': rec['shapiro_p'],\n",
    "            '平稳(ADF)': rec['is_stationary'],\n",
    "            'ADF_p': rec['adf_p'],\n",
    "            # --- 相关性 ---\n",
    "            '最强相关标签': rec['best_label'],\n",
    "            '最强Pearson_r': rec['best_pearson_r'],\n",
    "            '最强Pearson_p': rec['best_pearson_p'],\n",
    "            '最强Spearman_r': rec['best_spearman_r'],\n",
    "            '最强Spearman_p': rec['best_spearman_p'],\n",
    "        })\n",
    "\n",
    "    report_df = pd.DataFrame(rows)\n",
    "\n",
    "    # 综合风险等级\n",
    "    def risk_level(row):\n",
    "        score = 0\n",
    "        if row['缺失率(%)'] > 50:\n",
    "            score += 3\n",
    "        elif row['缺失率(%)'] > 20:\n",
    "            score += 2\n",
    "        elif row['缺失率(%)'] > 5:\n",
    "            score += 1\n",
    "        if row['异常值占比(%)'] > 10:\n",
    "            score += 2\n",
    "        elif row['异常值占比(%)'] > 5:\n",
    "            score += 1\n",
    "        if abs(row['偏度']) > 5:\n",
    "            score += 1\n",
    "        if score >= 4:\n",
    "            return '高'\n",
    "        elif score >= 2:\n",
    "            return '中'\n",
    "        else:\n",
    "            return '低'\n",
    "\n",
    "    report_df['风险等级'] = report_df.apply(risk_level, axis=1)\n",
    "\n",
    "    # 排序\n",
    "    report_df = report_df.sort_values('缺失率(%)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # ---- 汇总统计 ----\n",
    "    print(\"=\" * 60)\n",
    "    print(\"特征诊断报告 -- 汇总统计\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    n = len(report_df)\n",
    "    print(f\"\\n[COUNT] 诊断特征总数: {n}\")\n",
    "\n",
    "    # 缺失值汇总\n",
    "    no_miss = (report_df['缺失率(%)'] == 0).sum()\n",
    "    low_miss = ((report_df['缺失率(%)'] > 0) & (report_df['缺失率(%)'] <= 5)).sum()\n",
    "    mid_miss = ((report_df['缺失率(%)'] > 5) & (report_df['缺失率(%)'] <= 30)).sum()\n",
    "    high_miss = (report_df['缺失率(%)'] > 30).sum()\n",
    "    consec_miss = (report_df['缺失模式'] == '时序连续缺失').sum()\n",
    "    print(f\"\\n[MISSING] 缺失值分布:\")\n",
    "    print(f\"  无缺失:      {no_miss}\")\n",
    "    print(f\"  低缺失(<=5%): {low_miss}\")\n",
    "    print(f\"  中缺失(5~30%): {mid_miss}\")\n",
    "    print(f\"  高缺失(>30%): {high_miss}\")\n",
    "    print(f\"  时序连续缺失: {consec_miss}\")\n",
    "\n",
    "    # 异常值汇总\n",
    "    avg_outlier = report_df['异常值占比(%)'].mean()\n",
    "    high_outlier = (report_df['异常值占比(%)'] > 5).sum()\n",
    "    print(f\"\\n[OUTLIER] 异常值:\")\n",
    "    print(f\"  平均异常值占比: {avg_outlier:.2f}%\")\n",
    "    print(f\"  异常值占比>5%的特征: {high_outlier}\")\n",
    "\n",
    "    # 分布汇总\n",
    "    normal_cnt = report_df['正态(Shapiro)'].sum()\n",
    "    stationary_cnt = report_df['平稳(ADF)'].sum()\n",
    "    print(f\"\\n[DIST] 分布特征:\")\n",
    "    print(f\"  通过正态检验: {normal_cnt}/{n}\")\n",
    "    print(f\"  时序平稳:     {stationary_cnt}/{n}\")\n",
    "    print(f\"  平均偏度: {report_df['偏度'].mean():.4f}, 平均峰度: {report_df['峰度'].mean():.4f}\")\n",
    "\n",
    "    # 相关性汇总\n",
    "    strong_corr = (report_df['最强Pearson_r'].abs() > 0.3).sum()\n",
    "    sig_corr = (report_df['最强Pearson_p'] < 0.05).sum()\n",
    "    print(f\"\\n[CORR] 与标签相关性:\")\n",
    "    print(f\"  |Pearson r| > 0.3: {strong_corr}\")\n",
    "    print(f\"  Pearson p < 0.05 (显著): {sig_corr}\")\n",
    "\n",
    "    # 风险等级汇总\n",
    "    risk_vc = report_df['风险等级'].value_counts()\n",
    "    print(f\"\\n[RISK] 综合风险等级:\")\n",
    "    for level in ['低', '中', '高']:\n",
    "        print(f\"  {level}: {risk_vc.get(level, 0)}\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 展示部分结果\n",
    "    display_cols = ['特征名', '缺失率(%)', '缺失模式', '异常值占比(%)',\n",
    "                    '偏度', '峰度', '平稳(ADF)', '最强相关标签',\n",
    "                    '最强Pearson_r', '风险等级']\n",
    "    print(\"\\n[TABLE] 诊断报告 Top20 (按缺失率降序):\")\n",
    "    print(report_df[display_cols].head(20).to_string(index=False))\n",
    "\n",
    "    return report_df\n",
    "\n",
    "\n",
    "# 执行\n",
    "diag_df = build_diagnosis_report(diag_records)\n",
    "diag_df.to_csv('feature_diagnosis_report.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n[OK] 诊断报告已保存: feature_diagnosis_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a738c",
   "metadata": {},
   "source": [
    "### 2.3 诊断结果可视化\n",
    "绘制三组关键图表：\n",
    "1. **缺失值排名 Top20 柱状图** — 展示缺失率最高的 20 个特征；\n",
    "2. **异常值分布箱型图** — 选取异常值占比最高的 20 个特征绘制箱线图；\n",
    "3. **特征-标签相关性热力图** — 展示所有特征与 Y1~Y12 的 Pearson 相关系数矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0debb940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 2.3: 诊断结果可视化\n",
      "============================================================\n",
      "\n",
      "[1/3] 缺失值排名 Top20\n",
      "[OK] 缺失率 Top20 图已保存: images\\missing_top20.png\n",
      "\n",
      "[2/3] 异常值分布箱型图 Top20\n",
      "[OK] 异常值箱型图已保存: images\\outlier_boxplot_top20.png\n",
      "\n",
      "[3/3] 特征-标签相关性热力图\n",
      "[OK] 相关性热力图已保存: images\\feature_label_corr_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 诊断结果可视化\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def plot_missing_top20(diag_df: pd.DataFrame, save_dir: str = IMG_DIR) -> None:\n",
    "    \"\"\"\n",
    "    绘制缺失率排名 Top20 的特征柱状图。\n",
    "\n",
    "    Description:\n",
    "        按缺失率降序取前 20 个特征，绘制水平柱状图，\n",
    "        并用颜色区分缺失模式（随机/时序连续）。\n",
    "\n",
    "    Parameters:\n",
    "        diag_df : pd.DataFrame\n",
    "            形状为 (n_features, n_report_cols) 的诊断报告表。\n",
    "        save_dir : str\n",
    "            图片保存目录路径，标量字符串。\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    top20 = diag_df.nlargest(20, '缺失率(%)').sort_values('缺失率(%)')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    colors = ['#F44336' if p == '时序连续缺失' else '#2196F3'\n",
    "              for p in top20['缺失模式']]\n",
    "    bars = ax.barh(top20['特征名'], top20['缺失率(%)'], color=colors, edgecolor='black', linewidth=0.3)\n",
    "\n",
    "    # 标注数值\n",
    "    for bar, pct in zip(bars, top20['缺失率(%)']):\n",
    "        ax.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height() / 2,\n",
    "                f'{pct:.1f}%', va='center', fontsize=8)\n",
    "\n",
    "    ax.set_xlabel('缺失率 (%)')\n",
    "    ax.set_title('特征缺失率 Top20', fontsize=13)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    # 图例\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='#F44336', label='时序连续缺失'),\n",
    "                       Patch(facecolor='#2196F3', label='随机缺失')]\n",
    "    ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'missing_top20.png')\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[OK] 缺失率 Top20 图已保存: {save_path}\")\n",
    "\n",
    "\n",
    "def plot_outlier_boxplots(df: pd.DataFrame, diag_df: pd.DataFrame,\n",
    "                           save_dir: str = IMG_DIR) -> None:\n",
    "    \"\"\"\n",
    "    绘制异常值占比最高的 Top20 特征的箱型图。\n",
    "\n",
    "    Description:\n",
    "        按异常值占比降序取前 20 个特征，绘制箱型图以展示数据分布和离群点。\n",
    "        对数据做标准化处理后绘图，使不同量纲的特征可以对比。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据。\n",
    "        diag_df : pd.DataFrame\n",
    "            形状为 (n_features, n_report_cols) 的诊断报告表。\n",
    "        save_dir : str\n",
    "            图片保存目录路径，标量字符串。\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    top20 = diag_df.nlargest(20, '异常值占比(%)')\n",
    "    top20_cols = top20['特征名'].tolist()\n",
    "\n",
    "    # 标准化后绘图\n",
    "    plot_data = df[top20_cols].copy()\n",
    "    for c in top20_cols:\n",
    "        s = plot_data[c]\n",
    "        s_mean = s.mean()\n",
    "        s_std = s.std()\n",
    "        if s_std > 0:\n",
    "            plot_data[c] = (s - s_mean) / s_std\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    plot_data[top20_cols].boxplot(ax=ax, vert=True, showfliers=True,\n",
    "                                  flierprops=dict(marker='o', markersize=2, alpha=0.3))\n",
    "    ax.set_title('异常值占比 Top20 特征箱型图（标准化后）', fontsize=13)\n",
    "    ax.set_xlabel('特征')\n",
    "    ax.set_ylabel('标准化值')\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'outlier_boxplot_top20.png')\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[OK] 异常值箱型图已保存: {save_path}\")\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(diag_records: list, label_cols: list,\n",
    "                              save_dir: str = IMG_DIR) -> None:\n",
    "    \"\"\"\n",
    "    绘制特征与标签(Y1~Y12)的 Pearson 相关系数热力图。\n",
    "\n",
    "    Description:\n",
    "        1. 从 diag_records 中提取每个特征与所有标签的 Pearson 相关系数。\n",
    "        2. 构建 (n_features, n_labels) 的相关系数矩阵。\n",
    "        3. 使用 seaborn heatmap 绘制，颜色映射从蓝(-1)到红(+1)。\n",
    "        4. 为保持可读性，y轴仅标注每10个特征。\n",
    "\n",
    "    Parameters:\n",
    "        diag_records : list of dict\n",
    "            长度为 n_features 的诊断记录列表。\n",
    "        label_cols : list of str\n",
    "            标签列名列表，长度最多 12。\n",
    "        save_dir : str\n",
    "            图片保存目录路径，标量字符串。\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 构建相关系数矩阵\n",
    "    corr_data = {}\n",
    "    features = []\n",
    "    for rec in diag_records:\n",
    "        feat = rec['feature']\n",
    "        features.append(feat)\n",
    "        for label in label_cols:\n",
    "            if label not in corr_data:\n",
    "                corr_data[label] = []\n",
    "            corr_data[label].append(rec['all_pearson'].get(label, 0.0))\n",
    "\n",
    "    corr_matrix = pd.DataFrame(corr_data, index=features)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 18))\n",
    "    sns.heatmap(corr_matrix, cmap='RdBu_r', center=0, vmin=-0.5, vmax=0.5,\n",
    "                xticklabels=True, yticklabels=10,\n",
    "                cbar_kws={'label': 'Pearson r', 'shrink': 0.6},\n",
    "                ax=ax, linewidths=0)\n",
    "    ax.set_title('特征 X1~X300 与标签 Y1~Y12 Pearson 相关系数热力图', fontsize=13)\n",
    "    ax.set_xlabel('标签')\n",
    "    ax.set_ylabel('特征')\n",
    "    ax.tick_params(axis='y', labelsize=6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, 'feature_label_corr_heatmap.png')\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[OK] 相关性热力图已保存: {save_path}\")\n",
    "\n",
    "\n",
    "# ---- 执行可视化 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 2.3: 诊断结果可视化\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[1/3] 缺失值排名 Top20\")\n",
    "plot_missing_top20(diag_df)\n",
    "\n",
    "print(\"\\n[2/3] 异常值分布箱型图 Top20\")\n",
    "plot_outlier_boxplots(df, diag_df)\n",
    "\n",
    "print(\"\\n[3/3] 特征-标签相关性热力图\")\n",
    "plot_correlation_heatmap(diag_records, LABEL_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620aaddb",
   "metadata": {},
   "source": [
    "## 步骤3：自动特征清理\n",
    "\n",
    "基于步骤2诊断结果，利用千问大模型 Agent **自主决策**每个特征的清理方案，然后自动执行清理：\n",
    "- **缺失值处理**：时序连续缺失 -> 线性插值/样条插值；随机缺失 -> 均值/中位数填充；缺失>80% -> 标记剔除\n",
    "- **异常值处理**：温和异常 -> 上下限截断(Winsorize)；极端异常 -> 中位数替换；时序趋势性异常 -> 保留标注\n",
    "- **分布优化**：非平稳特征 -> 差分/标准化处理，验证处理后平稳性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341f2bc",
   "metadata": {},
   "source": [
    "### 3.1 千问 Agent 决策清理方案\n",
    "将诊断报告分批发送给千问大模型，由 Agent 针对每个特征的诊断结果（缺失模式、异常值、分布、相关性）自主输出结构化清理方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982c9095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 3.1: 千问 Agent 决策清理方案\n",
      "============================================================\n",
      "\n",
      "  [BATCH 1/10] 正在请求 Agent 决策 (30 个特征)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:20:03,994] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Agent 未返回有效方案, 规则引擎补全 30 个\n",
      "  [BATCH 2/10] 正在请求 Agent 决策 (30 个特征)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:20:11,976] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Agent 返回 30 个方案\n",
      "  [BATCH 3/10] 正在请求 Agent 决策 (30 个特征)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:20:40,759] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Agent 返回 30 个方案\n",
      "  [BATCH 4/10] 正在请求 Agent 决策 (30 个特征)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:21:12,636] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Agent 返回 30 个方案\n",
      "  [BATCH 5/10] 正在请求 Agent 决策 (30 个特征)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:21:51,855] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Agent 返回 30 个方案\n",
      "  [BATCH 6/10] 正在请求 Agent 决策 (30 个特征)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:22:28,413] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Agent 返回 30 个方案\n",
      "  [BATCH 7/10] 正在请求 Agent 决策 (30 个特征)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:23:07,208] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Agent 返回 30 个方案\n",
      "  [BATCH 8/10] 正在请求 Agent 决策 (30 个特征)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:23:49,528] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Agent 返回 30 个方案\n",
      "  [BATCH 9/10] 正在请求 Agent 决策 (30 个特征)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:24:20,190] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Agent 返回 30 个方案\n",
      "  [BATCH 10/10] 正在请求 Agent 决策 (30 个特征)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:24:50,208] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Agent 返回 30 个方案\n",
      "\n",
      "[OK] 共获取 300 个特征清理方案\n",
      "\n",
      "[SUMMARY] 清理方案分布:\n",
      "  缺失值处理:\n",
      "    median_fill: 91\n",
      "    mean_fill: 52\n",
      "    spline_interp|mean_fill: 30\n",
      "    mean_fill|median_fill: 30\n",
      "    spline_interp|mean_fill|median_fill: 30\n",
      "    spline_interp|median_fill: 30\n",
      "    linear_interp|spline_interp|mean_fill: 21\n",
      "    linear_interp|spline_interp: 12\n",
      "    linear_interp: 4\n",
      "  异常值处理:\n",
      "    winsorize: 128\n",
      "    keep: 123\n",
      "    median_replace: 49\n",
      "  分布处理:\n",
      "    standardize: 171\n",
      "    none: 85\n",
      "    diff: 44\n",
      "\n",
      "  待剔除特征 (缺失>80%): 0 个\n",
      "\n",
      "[OK] 清理方案已保存: cleaning_plans.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 千问 Agent 决策清理方案\n",
    "# =============================================================================\n",
    "from openai import OpenAI\n",
    "import json, re, time\n",
    "\n",
    "# API 配置\n",
    "QWEN_API_KEY = \"sk-pxizupebbwgijptggmseledfboqcfcqcjltbfiucswhxicow\"\n",
    "QWEN_BASE_URL = \"https://api.siliconflow.cn/v1\"\n",
    "QWEN_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "\n",
    "def call_qwen_agent(prompt: str, system_prompt: str = None) -> str:\n",
    "    \"\"\"\n",
    "    调用千问 7B API (硅基流动接口)，获取 Agent 决策结果。\n",
    "\n",
    "    Description:\n",
    "        通过 OpenAI 兼容接口连接硅基流动平台，向 Qwen2.5-7B-Instruct 模型\n",
    "        发送 system + user 消息，流式接收并拼接完整回复文本。\n",
    "        若调用失败则返回 None，由上层函数执行 fallback 逻辑。\n",
    "\n",
    "    Parameters:\n",
    "        prompt : str\n",
    "            用户 prompt 文本，标量字符串。\n",
    "        system_prompt : str, optional\n",
    "            系统 prompt 文本，标量字符串，默认为 None。\n",
    "\n",
    "    Returns:\n",
    "        full_response : str or None\n",
    "            模型的完整回复文本，标量字符串。\n",
    "            若 API 调用失败则返回 None。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = OpenAI(api_key=QWEN_API_KEY, base_url=QWEN_BASE_URL)\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({'role': 'system', 'content': system_prompt})\n",
    "        messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=QWEN_MODEL, messages=messages, stream=True\n",
    "        )\n",
    "        full_response = \"\"\n",
    "        for chunk in response:\n",
    "            if not chunk.choices:\n",
    "                continue\n",
    "            delta = chunk.choices[0].delta\n",
    "            if delta.content:\n",
    "                full_response += delta.content\n",
    "        return full_response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[ERROR] 千问 API 调用失败: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_cleaning_prompt_batch(diag_df: pd.DataFrame,\n",
    "                                  batch_features: list) -> str:\n",
    "    \"\"\"\n",
    "    为一批特征构建清理方案决策 prompt。\n",
    "\n",
    "    Description:\n",
    "        将每个特征的诊断结果（缺失率、缺失模式、异常值占比、偏度、峰度、\n",
    "        平稳性、去趋势异常率）格式化为结构化文本，发送给 Agent。\n",
    "\n",
    "    Parameters:\n",
    "        diag_df : pd.DataFrame\n",
    "            形状为 (n_features, n_report_cols) 的诊断报告表。\n",
    "        batch_features : list of str\n",
    "            本批次的特征名列表，长度一般为 30。\n",
    "\n",
    "    Returns:\n",
    "        prompt : str\n",
    "            组装好的 user prompt 文本。\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for feat in batch_features:\n",
    "        row = diag_df[diag_df['特征名'] == feat]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        r = row.iloc[0]\n",
    "        lines.append(\n",
    "            f\"- {feat}: 缺失率={r['缺失率(%)']}%, 缺失模式={r['缺失模式']}, \"\n",
    "            f\"最长连续缺失={r['最长连续缺失']}, \"\n",
    "            f\"异常值占比={r['异常值占比(%)']}%, 去趋势异常={r['去趋势异常(%)']}%, \"\n",
    "            f\"偏度={r['偏度']}, 峰度={r['峰度']}, \"\n",
    "            f\"平稳(ADF)={r['平稳(ADF)']}, ADF_p={r['ADF_p']}\"\n",
    "        )\n",
    "    feature_text = \"\\n\".join(lines)\n",
    "\n",
    "    prompt = f\"\"\"以下是 {len(batch_features)} 个金融时序特征的诊断结果：\n",
    "\n",
    "{feature_text}\n",
    "\n",
    "请为每个特征输出清理方案，严格按照以下JSON数组格式回复，不要添加其他内容：\n",
    "[\n",
    "  {{\n",
    "    \"feature\": \"X1\",\n",
    "    \"missing_action\": \"linear_interp|spline_interp|mean_fill|median_fill|drop\",\n",
    "    \"missing_reason\": \"理由\",\n",
    "    \"outlier_action\": \"winsorize|median_replace|keep\",\n",
    "    \"outlier_reason\": \"理由\",\n",
    "    \"distribution_action\": \"diff|standardize|normalize|none\",\n",
    "    \"distribution_reason\": \"理由\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "决策规则参考：\n",
    "1. 缺失值: 时序连续缺失用linear_interp或spline_interp; 随机缺失用mean_fill或median_fill; 缺失率>80%用drop\n",
    "2. 异常值: 去趋势异常率<异常值占比(说明异常源于趋势)用keep; 否则异常值占比>5%用winsorize; 极端情况用median_replace\n",
    "3. 分布: 非平稳(ADF=False)且偏度绝对值>2用diff; 非平稳但偏度不大用standardize; 平稳的用none\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def parse_agent_cleaning_response(response: str,\n",
    "                                    batch_features: list) -> list:\n",
    "    \"\"\"\n",
    "    解析 Agent 返回的 JSON 清理方案，若解析失败则启用规则引擎 fallback。\n",
    "\n",
    "    Description:\n",
    "        1. 尝试从 response 中提取 JSON 数组。\n",
    "        2. 若提取失败，对每个特征用规则引擎生成默认方案。\n",
    "\n",
    "    Parameters:\n",
    "        response : str or None\n",
    "            Agent 的回复文本，应包含 JSON 数组。\n",
    "        batch_features : list of str\n",
    "            本批次的特征名列表。\n",
    "\n",
    "    Returns:\n",
    "        plans : list of dict\n",
    "            长度为 len(batch_features) 的清理方案列表。\n",
    "    \"\"\"\n",
    "    plans = []\n",
    "\n",
    "    if response:\n",
    "        try:\n",
    "            # 提取 JSON 数组\n",
    "            match = re.search(r'\\[.*\\]', response, re.DOTALL)\n",
    "            if match:\n",
    "                parsed = json.loads(match.group())\n",
    "                if isinstance(parsed, list) and len(parsed) > 0:\n",
    "                    plans = parsed\n",
    "        except (json.JSONDecodeError, Exception):\n",
    "            pass\n",
    "\n",
    "    # 检查是否完整覆盖\n",
    "    parsed_features = {p.get('feature', '') for p in plans}\n",
    "    missing_feats = [f for f in batch_features if f not in parsed_features]\n",
    "\n",
    "    return plans, missing_feats\n",
    "\n",
    "\n",
    "def rule_engine_fallback(diag_df: pd.DataFrame, feature: str) -> dict:\n",
    "    \"\"\"\n",
    "    规则引擎 fallback：当 Agent 未返回有效方案时，基于规则自动决策。\n",
    "\n",
    "    Description:\n",
    "        根据诊断报告中的缺失率、缺失模式、异常值占比、去趋势异常率、\n",
    "        ADF平稳性和偏度，生成对应的清理方案。\n",
    "\n",
    "    Parameters:\n",
    "        diag_df : pd.DataFrame\n",
    "            形状为 (n_features, n_report_cols) 的诊断报告表。\n",
    "        feature : str\n",
    "            特征名，标量字符串。\n",
    "\n",
    "    Returns:\n",
    "        plan : dict\n",
    "            包含 feature, missing_action, outlier_action, distribution_action\n",
    "            及对应 reason 的清理方案字典。\n",
    "    \"\"\"\n",
    "    row = diag_df[diag_df['特征名'] == feature]\n",
    "    if row.empty:\n",
    "        return {'feature': feature,\n",
    "                'missing_action': 'median_fill', 'missing_reason': '默认',\n",
    "                'outlier_action': 'winsorize', 'outlier_reason': '默认',\n",
    "                'distribution_action': 'none', 'distribution_reason': '默认'}\n",
    "\n",
    "    r = row.iloc[0]\n",
    "    plan = {'feature': feature}\n",
    "\n",
    "    # 缺失值决策\n",
    "    if r['缺失率(%)'] > 80:\n",
    "        plan['missing_action'] = 'drop'\n",
    "        plan['missing_reason'] = f\"缺失率{r['缺失率(%)']}%过高，标记剔除\"\n",
    "    elif r['缺失模式'] == '时序连续缺失':\n",
    "        plan['missing_action'] = 'linear_interp'\n",
    "        plan['missing_reason'] = f\"时序连续缺失(最长{r['最长连续缺失']}),用线性插值\"\n",
    "    elif r['缺失率(%)'] > 0:\n",
    "        plan['missing_action'] = 'median_fill'\n",
    "        plan['missing_reason'] = f\"随机缺失{r['缺失率(%)']}%,用中位数填充\"\n",
    "    else:\n",
    "        plan['missing_action'] = 'none'\n",
    "        plan['missing_reason'] = '无缺失'\n",
    "\n",
    "    # 异常值决策\n",
    "    if r['去趋势异常(%)'] < r['异常值占比(%)'] * 0.5:\n",
    "        plan['outlier_action'] = 'keep'\n",
    "        plan['outlier_reason'] = '异常主要源于趋势,保留'\n",
    "    elif r['异常值占比(%)'] > 10:\n",
    "        plan['outlier_action'] = 'median_replace'\n",
    "        plan['outlier_reason'] = f\"极端异常{r['异常值占比(%)']}%,中位数替换\"\n",
    "    elif r['异常值占比(%)'] > 2:\n",
    "        plan['outlier_action'] = 'winsorize'\n",
    "        plan['outlier_reason'] = f\"温和异常{r['异常值占比(%)']}%,截断处理\"\n",
    "    else:\n",
    "        plan['outlier_action'] = 'keep'\n",
    "        plan['outlier_reason'] = '异常值较少,保留'\n",
    "\n",
    "    # 分布优化决策\n",
    "    if not r['平稳(ADF)'] and abs(r['偏度']) > 2:\n",
    "        plan['distribution_action'] = 'diff'\n",
    "        plan['distribution_reason'] = f\"非平稳且偏度={r['偏度']},做差分\"\n",
    "    elif not r['平稳(ADF)']:\n",
    "        plan['distribution_action'] = 'standardize'\n",
    "        plan['distribution_reason'] = '非平稳,标准化处理'\n",
    "    else:\n",
    "        plan['distribution_action'] = 'none'\n",
    "        plan['distribution_reason'] = '已平稳,无需处理'\n",
    "\n",
    "    return plan\n",
    "\n",
    "\n",
    "def get_all_cleaning_plans(diag_df: pd.DataFrame,\n",
    "                            feature_cols: list,\n",
    "                            batch_size: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    分批调用千问 Agent 获取所有特征的清理方案，不足部分用规则引擎补全。\n",
    "\n",
    "    Description:\n",
    "        1. 将 300 个特征分为 batch_size 大小的批次。\n",
    "        2. 每批构建 prompt 发送给 Agent，获取清理方案。\n",
    "        3. Agent 未覆盖的特征用 rule_engine_fallback 补全。\n",
    "        4. 汇总所有方案为 DataFrame。\n",
    "\n",
    "    Parameters:\n",
    "        diag_df : pd.DataFrame\n",
    "            形状为 (n_features, n_report_cols) 的诊断报告表。\n",
    "        feature_cols : list of str\n",
    "            特征列名列表，长度最多 300。\n",
    "        batch_size : int\n",
    "            每批发送给 Agent 的特征数，标量整数，默认 30。\n",
    "\n",
    "    Returns:\n",
    "        plans_df : pd.DataFrame\n",
    "            形状为 (n_features, 7) 的清理方案表，包含:\n",
    "            feature, missing_action, missing_reason, outlier_action,\n",
    "            outlier_reason, distribution_action, distribution_reason\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"你是一个专业的金融时序数据清理Agent。\n",
    "你需要根据每个特征的诊断结果，为其制定最优的数据清理方案。\n",
    "请严格按照JSON数组格式回复。不要回复JSON以外的内容。\"\"\"\n",
    "\n",
    "    all_plans = []\n",
    "    features_in_diag = diag_df['特征名'].tolist()\n",
    "    cols_to_process = [c for c in feature_cols if c in features_in_diag]\n",
    "\n",
    "    n_batches = (len(cols_to_process) + batch_size - 1) // batch_size\n",
    "\n",
    "    for b in range(n_batches):\n",
    "        start = b * batch_size\n",
    "        end = min(start + batch_size, len(cols_to_process))\n",
    "        batch = cols_to_process[start:end]\n",
    "\n",
    "        print(f\"  [BATCH {b+1}/{n_batches}] 正在请求 Agent 决策 ({len(batch)} 个特征)...\")\n",
    "\n",
    "        prompt = build_cleaning_prompt_batch(diag_df, batch)\n",
    "        response = call_qwen_agent(prompt, system_prompt)\n",
    "\n",
    "        agent_plans, missing_feats = parse_agent_cleaning_response(response, batch)\n",
    "\n",
    "        if agent_plans:\n",
    "            all_plans.extend(agent_plans)\n",
    "            print(f\"    Agent 返回 {len(agent_plans)} 个方案\", end=\"\")\n",
    "        else:\n",
    "            missing_feats = batch\n",
    "            print(f\"    Agent 未返回有效方案\", end=\"\")\n",
    "\n",
    "        # 用规则引擎补全\n",
    "        if missing_feats:\n",
    "            for feat in missing_feats:\n",
    "                fb = rule_engine_fallback(diag_df, feat)\n",
    "                all_plans.append(fb)\n",
    "            print(f\", 规则引擎补全 {len(missing_feats)} 个\")\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "        # 避免请求过快\n",
    "        if b < n_batches - 1:\n",
    "            time.sleep(1)\n",
    "\n",
    "    plans_df = pd.DataFrame(all_plans)\n",
    "\n",
    "    # 确保列名统一\n",
    "    for col in ['feature', 'missing_action', 'missing_reason',\n",
    "                'outlier_action', 'outlier_reason',\n",
    "                'distribution_action', 'distribution_reason']:\n",
    "        if col not in plans_df.columns:\n",
    "            plans_df[col] = ''\n",
    "\n",
    "    plans_df = plans_df[['feature', 'missing_action', 'missing_reason',\n",
    "                          'outlier_action', 'outlier_reason',\n",
    "                          'distribution_action', 'distribution_reason']]\n",
    "\n",
    "    return plans_df\n",
    "\n",
    "\n",
    "# ---- 执行 Agent 决策 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 3.1: 千问 Agent 决策清理方案\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "cleaning_plans = get_all_cleaning_plans(diag_df, FEATURE_COLS, batch_size=30)\n",
    "\n",
    "print(f\"\\n[OK] 共获取 {len(cleaning_plans)} 个特征清理方案\")\n",
    "\n",
    "# 统计方案分布\n",
    "print(f\"\\n[SUMMARY] 清理方案分布:\")\n",
    "print(f\"  缺失值处理:\")\n",
    "for action, cnt in cleaning_plans['missing_action'].value_counts().items():\n",
    "    print(f\"    {action}: {cnt}\")\n",
    "print(f\"  异常值处理:\")\n",
    "for action, cnt in cleaning_plans['outlier_action'].value_counts().items():\n",
    "    print(f\"    {action}: {cnt}\")\n",
    "print(f\"  分布处理:\")\n",
    "for action, cnt in cleaning_plans['distribution_action'].value_counts().items():\n",
    "    print(f\"    {action}: {cnt}\")\n",
    "\n",
    "# 标记待剔除特征\n",
    "drop_features = cleaning_plans[cleaning_plans['missing_action'] == 'drop']['feature'].tolist()\n",
    "print(f\"\\n  待剔除特征 (缺失>80%): {len(drop_features)} 个\")\n",
    "if drop_features:\n",
    "    print(f\"    {drop_features}\")\n",
    "\n",
    "# 保存方案\n",
    "cleaning_plans.to_csv('cleaning_plans.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n[OK] 清理方案已保存: cleaning_plans.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89121af",
   "metadata": {},
   "source": [
    "### 3.2 执行特征清理\n",
    "根据 Agent 决策的清理方案，封装 `clean_features` 函数执行清理，并输出清理前后的特征对比报告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4074eeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 3.2: 执行特征清理\n",
      "============================================================\n",
      "\n",
      "  [PROGRESS] 50/300 个特征清理完成\n",
      "  [PROGRESS] 100/300 个特征清理完成\n",
      "  [PROGRESS] 150/300 个特征清理完成\n",
      "  [PROGRESS] 200/300 个特征清理完成\n",
      "  [PROGRESS] 250/300 个特征清理完成\n",
      "  [PROGRESS] 300/300 个特征清理完成\n",
      "\n",
      "[OK] 清理完成\n",
      "  总特征数: 300\n",
      "  有效特征数: 300\n",
      "  剔除特征数: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 执行特征清理 — clean_features 函数\n",
    "# =============================================================================\n",
    "from statsmodels.tsa.stattools import adfuller as adf_test\n",
    "\n",
    "\n",
    "def clean_features(df: pd.DataFrame, plans_df: pd.DataFrame,\n",
    "                    feature_cols: list) -> tuple:\n",
    "    \"\"\"\n",
    "    根据 Agent 决策的清理方案，对所有特征执行清理操作。\n",
    "\n",
    "    Description:\n",
    "        对每个特征依次执行三步清理:\n",
    "        1. 缺失值处理: linear_interp / spline_interp / mean_fill /\n",
    "           median_fill / drop (标记) / none\n",
    "        2. 异常值处理: winsorize(IQR截断) / median_replace / keep\n",
    "        3. 分布优化: diff(一阶差分) / standardize(z-score) /\n",
    "           normalize(min-max) / none\n",
    "        记录每步清理的变化量，生成清理日志。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据。\n",
    "        plans_df : pd.DataFrame\n",
    "            形状为 (n_features, 7) 的清理方案表，由 Agent 生成。\n",
    "        feature_cols : list of str\n",
    "            特征列名列表，长度最多 300。\n",
    "\n",
    "    Returns:\n",
    "        df_cleaned : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的清理后数据。\n",
    "        clean_log : list of dict\n",
    "            长度为 n_features 的清理日志列表，每条记录包含:\n",
    "            - 'feature': str\n",
    "            - 'action_missing': str, 执行的缺失值处理\n",
    "            - 'action_outlier': str, 执行的异常值处理\n",
    "            - 'action_dist': str, 执行的分布处理\n",
    "            - 'missing_before/after': int\n",
    "            - 'outlier_before/after': int\n",
    "            - 'dropped': bool\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    clean_log = []\n",
    "    dropped_features = []\n",
    "\n",
    "    plan_dict = {}\n",
    "    for _, row in plans_df.iterrows():\n",
    "        plan_dict[row['feature']] = row\n",
    "\n",
    "    total = len(feature_cols)\n",
    "    for i, col in enumerate(feature_cols):\n",
    "        if col not in df_cleaned.columns:\n",
    "            continue\n",
    "\n",
    "        plan = plan_dict.get(col, None)\n",
    "        if plan is None:\n",
    "            continue\n",
    "\n",
    "        log_entry = {\n",
    "            'feature': col,\n",
    "            'action_missing': str(plan.get('missing_action', 'none')),\n",
    "            'action_outlier': str(plan.get('outlier_action', 'keep')),\n",
    "            'action_dist': str(plan.get('distribution_action', 'none')),\n",
    "            'dropped': False\n",
    "        }\n",
    "\n",
    "        s = df_cleaned[col]\n",
    "\n",
    "        # ---- 记录清理前状态 ----\n",
    "        missing_before = int(s.isnull().sum())\n",
    "        # IQR 异常值计数\n",
    "        s_clean = s.dropna()\n",
    "        if len(s_clean) > 10:\n",
    "            q1, q3 = s_clean.quantile(0.25), s_clean.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            outlier_before = int(((s_clean < q1 - 1.5 * iqr) |\n",
    "                                   (s_clean > q3 + 1.5 * iqr)).sum())\n",
    "        else:\n",
    "            outlier_before = 0\n",
    "            q1, q3, iqr = 0, 0, 0\n",
    "\n",
    "        log_entry['missing_before'] = missing_before\n",
    "        log_entry['outlier_before'] = outlier_before\n",
    "\n",
    "        # ---- 1. 缺失值处理 ----\n",
    "        m_action = str(plan.get('missing_action', 'none')).lower().strip()\n",
    "        if m_action == 'drop':\n",
    "            dropped_features.append(col)\n",
    "            log_entry['dropped'] = True\n",
    "            log_entry['missing_after'] = missing_before\n",
    "            log_entry['outlier_after'] = outlier_before\n",
    "            clean_log.append(log_entry)\n",
    "            continue\n",
    "        elif m_action == 'linear_interp':\n",
    "            df_cleaned[col] = df_cleaned[col].interpolate(method='linear')\n",
    "            # 头尾仍有 NaN 时用前/后填充收尾\n",
    "            df_cleaned[col] = df_cleaned[col].ffill().bfill()\n",
    "        elif m_action == 'spline_interp':\n",
    "            try:\n",
    "                df_cleaned[col] = df_cleaned[col].interpolate(method='spline', order=3)\n",
    "            except Exception:\n",
    "                df_cleaned[col] = df_cleaned[col].interpolate(method='linear')\n",
    "            df_cleaned[col] = df_cleaned[col].ffill().bfill()\n",
    "        elif m_action == 'mean_fill':\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())\n",
    "        elif m_action == 'median_fill':\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "        # else: none — 不做处理\n",
    "\n",
    "        # ---- 2. 异常值处理 ----\n",
    "        o_action = str(plan.get('outlier_action', 'keep')).lower().strip()\n",
    "        s2 = df_cleaned[col].dropna()\n",
    "        if len(s2) > 10 and o_action != 'keep':\n",
    "            q1_new = s2.quantile(0.25)\n",
    "            q3_new = s2.quantile(0.75)\n",
    "            iqr_new = q3_new - q1_new\n",
    "            lower = q1_new - 1.5 * iqr_new\n",
    "            upper = q3_new + 1.5 * iqr_new\n",
    "\n",
    "            if o_action == 'winsorize':\n",
    "                # 截断到上下限\n",
    "                df_cleaned[col] = df_cleaned[col].clip(lower=lower, upper=upper)\n",
    "            elif o_action == 'median_replace':\n",
    "                # 极端异常用中位数替换\n",
    "                median_val = s2.median()\n",
    "                mask = (df_cleaned[col] < lower) | (df_cleaned[col] > upper)\n",
    "                df_cleaned.loc[mask, col] = median_val\n",
    "\n",
    "        # ---- 3. 分布优化 ----\n",
    "        d_action = str(plan.get('distribution_action', 'none')).lower().strip()\n",
    "        if d_action == 'diff':\n",
    "            # 一阶差分（第一个值填0，保持长度）\n",
    "            original = df_cleaned[col].copy()\n",
    "            df_cleaned[col] = df_cleaned[col].diff()\n",
    "            df_cleaned.loc[df_cleaned.index[0], col] = 0\n",
    "            # 差分后的 NaN 用0填充\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(0)\n",
    "        elif d_action == 'standardize':\n",
    "            mean_val = df_cleaned[col].mean()\n",
    "            std_val = df_cleaned[col].std()\n",
    "            if std_val > 0:\n",
    "                df_cleaned[col] = (df_cleaned[col] - mean_val) / std_val\n",
    "        elif d_action == 'normalize':\n",
    "            min_val = df_cleaned[col].min()\n",
    "            max_val = df_cleaned[col].max()\n",
    "            if max_val > min_val:\n",
    "                df_cleaned[col] = (df_cleaned[col] - min_val) / (max_val - min_val)\n",
    "        # else: none\n",
    "\n",
    "        # ---- 记录清理后状态 ----\n",
    "        missing_after = int(df_cleaned[col].isnull().sum())\n",
    "        s3 = df_cleaned[col].dropna()\n",
    "        if len(s3) > 10:\n",
    "            q1_f = s3.quantile(0.25)\n",
    "            q3_f = s3.quantile(0.75)\n",
    "            iqr_f = q3_f - q1_f\n",
    "            outlier_after = int(((s3 < q1_f - 1.5 * iqr_f) |\n",
    "                                  (s3 > q3_f + 1.5 * iqr_f)).sum())\n",
    "        else:\n",
    "            outlier_after = 0\n",
    "\n",
    "        log_entry['missing_after'] = missing_after\n",
    "        log_entry['outlier_after'] = outlier_after\n",
    "        clean_log.append(log_entry)\n",
    "\n",
    "        if (i + 1) % 50 == 0 or (i + 1) == total:\n",
    "            print(f\"  [PROGRESS] {i+1}/{total} 个特征清理完成\")\n",
    "\n",
    "    # 输出待剔除特征\n",
    "    if dropped_features:\n",
    "        print(f\"\\n  [DROP] 标记剔除 {len(dropped_features)} 个特征: {dropped_features}\")\n",
    "\n",
    "    return df_cleaned, clean_log\n",
    "\n",
    "\n",
    "# ---- 执行清理 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 3.2: 执行特征清理\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "df_cleaned, clean_log = clean_features(df, cleaning_plans, FEATURE_COLS)\n",
    "\n",
    "# 记录被剔除的特征\n",
    "dropped_features = [e['feature'] for e in clean_log if e.get('dropped', False)]\n",
    "active_features = [c for c in FEATURE_COLS if c not in dropped_features and c in df_cleaned.columns]\n",
    "\n",
    "print(f\"\\n[OK] 清理完成\")\n",
    "print(f\"  总特征数: {len(FEATURE_COLS)}\")\n",
    "print(f\"  有效特征数: {len(active_features)}\")\n",
    "print(f\"  剔除特征数: {len(dropped_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420335a",
   "metadata": {},
   "source": [
    "### 3.3 清理前后对比报告\n",
    "输出清理前后的缺失值、异常值变化对比，并可视化关键变化指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bffddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "清理前后对比报告\n",
      "============================================================\n",
      "\n",
      "[MISSING] 缺失值变化 (有效特征):\n",
      "  清理前总缺失值: 5,747,771\n",
      "  清理后总缺失值: 2,237,576\n",
      "  减少量: 3,510,195 (61.1%)\n",
      "\n",
      "[OUTLIER] 异常值变化 (有效特征):\n",
      "  清理前总异常值: 1,183,015\n",
      "  清理后总异常值: 2,180,669\n",
      "  减少量: -997,654 (-84.3%)\n",
      "\n",
      "[ACTIONS] 清理动作统计:\n",
      "  缺失值处理: {'median_fill': 91, 'mean_fill': 52, 'spline_interp|mean_fill': 30, 'mean_fill|median_fill': 30, 'spline_interp|mean_fill|median_fill': 30, 'spline_interp|median_fill': 30, 'linear_interp|spline_interp|mean_fill': 21, 'linear_interp|spline_interp': 12, 'linear_interp': 4}\n",
      "  异常值处理: {'winsorize': 128, 'keep': 123, 'median_replace': 49}\n",
      "  分布优化:   {'standardize': 171, 'none': 85, 'diff': 44}\n",
      "\n",
      "  清理后仍有缺失的特征: 118\n",
      "    X91: 18878 个缺失\n",
      "    X92: 18878 个缺失\n",
      "    X94: 18878 个缺失\n",
      "    X95: 18878 个缺失\n",
      "    X96: 18878 个缺失\n",
      "    X97: 18878 个缺失\n",
      "    X98: 18878 个缺失\n",
      "    X99: 18878 个缺失\n",
      "    X100: 18878 个缺失\n",
      "    X101: 18878 个缺失\n",
      "[OK] 缺失值变化图已保存: images\\missing_before_after.png\n",
      "[OK] 异常值变化图已保存: images\\outlier_before_after.png\n",
      "============================================================\n",
      "[OK] 清理对比报告已保存: cleaning_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 清理前后对比报告与可视化\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def generate_cleaning_comparison(clean_log: list, save_dir: str = IMG_DIR) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    生成清理前后对比报告，并可视化关键变化指标。\n",
    "\n",
    "    Description:\n",
    "        1. 将 clean_log 转为 DataFrame，计算缺失值/异常值的变化量和变化率。\n",
    "        2. 输出汇总统计（总缺失值减少量、总异常值减少量等）。\n",
    "        3. 绘制缺失值变化对比图和异常值变化对比图。\n",
    "\n",
    "    Parameters:\n",
    "        clean_log : list of dict\n",
    "            长度为 n_features 的清理日志列表，\n",
    "            由 clean_features 函数返回。\n",
    "        save_dir : str\n",
    "            图片保存目录路径，标量字符串，默认为 IMG_DIR。\n",
    "\n",
    "    Returns:\n",
    "        comparison_df : pd.DataFrame\n",
    "            形状为 (n_features, n_cols) 的对比报告表。\n",
    "    \"\"\"\n",
    "    comp_df = pd.DataFrame(clean_log)\n",
    "\n",
    "    # 计算变化量\n",
    "    comp_df['missing_delta'] = comp_df['missing_before'] - comp_df['missing_after']\n",
    "    comp_df['outlier_delta'] = comp_df['outlier_before'] - comp_df['outlier_after']\n",
    "\n",
    "    # 变化率\n",
    "    comp_df['missing_change_pct'] = np.where(\n",
    "        comp_df['missing_before'] > 0,\n",
    "        (comp_df['missing_delta'] / comp_df['missing_before'] * 100).round(1),\n",
    "        0.0\n",
    "    )\n",
    "    comp_df['outlier_change_pct'] = np.where(\n",
    "        comp_df['outlier_before'] > 0,\n",
    "        (comp_df['outlier_delta'] / comp_df['outlier_before'] * 100).round(1),\n",
    "        0.0\n",
    "    )\n",
    "\n",
    "    # ---- 汇总统计 ----\n",
    "    active = comp_df[~comp_df['dropped']]\n",
    "    total_missing_before = int(active['missing_before'].sum())\n",
    "    total_missing_after = int(active['missing_after'].sum())\n",
    "    total_outlier_before = int(active['outlier_before'].sum())\n",
    "    total_outlier_after = int(active['outlier_after'].sum())\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"清理前后对比报告\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n[MISSING] 缺失值变化 (有效特征):\")\n",
    "    print(f\"  清理前总缺失值: {total_missing_before:,}\")\n",
    "    print(f\"  清理后总缺失值: {total_missing_after:,}\")\n",
    "    print(f\"  减少量: {total_missing_before - total_missing_after:,} \"\n",
    "          f\"({(total_missing_before - total_missing_after) / max(total_missing_before, 1) * 100:.1f}%)\")\n",
    "\n",
    "    print(f\"\\n[OUTLIER] 异常值变化 (有效特征):\")\n",
    "    print(f\"  清理前总异常值: {total_outlier_before:,}\")\n",
    "    print(f\"  清理后总异常值: {total_outlier_after:,}\")\n",
    "    print(f\"  减少量: {total_outlier_before - total_outlier_after:,} \"\n",
    "          f\"({(total_outlier_before - total_outlier_after) / max(total_outlier_before, 1) * 100:.1f}%)\")\n",
    "\n",
    "    # 各清理动作统计\n",
    "    print(f\"\\n[ACTIONS] 清理动作统计:\")\n",
    "    print(f\"  缺失值处理: {active['action_missing'].value_counts().to_dict()}\")\n",
    "    print(f\"  异常值处理: {active['action_outlier'].value_counts().to_dict()}\")\n",
    "    print(f\"  分布优化:   {active['action_dist'].value_counts().to_dict()}\")\n",
    "\n",
    "    # 仍有缺失值的特征\n",
    "    still_missing = active[active['missing_after'] > 0]\n",
    "    print(f\"\\n  清理后仍有缺失的特征: {len(still_missing)}\")\n",
    "    if len(still_missing) > 0:\n",
    "        for _, r in still_missing.head(10).iterrows():\n",
    "            print(f\"    {r['feature']}: {r['missing_after']} 个缺失\")\n",
    "\n",
    "    # ---- 可视化 ----\n",
    "    # 1) 缺失值变化前后对比 (Top20 变化量最大的)\n",
    "    top_missing = active.nlargest(20, 'missing_delta').sort_values('missing_delta')\n",
    "    if len(top_missing) > 0 and top_missing['missing_delta'].sum() > 0:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        y_pos = range(len(top_missing))\n",
    "        ax.barh(y_pos, top_missing['missing_before'], height=0.4, color='#F44336',\n",
    "                alpha=0.7, label='清理前')\n",
    "        ax.barh([y + 0.4 for y in y_pos], top_missing['missing_after'], height=0.4,\n",
    "                color='#4CAF50', alpha=0.7, label='清理后')\n",
    "        ax.set_yticks([y + 0.2 for y in y_pos])\n",
    "        ax.set_yticklabels(top_missing['feature'], fontsize=8)\n",
    "        ax.set_xlabel('缺失值数量')\n",
    "        ax.set_title('缺失值变化 Top20 (清理前 vs 清理后)', fontsize=13)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(save_dir, 'missing_before_after.png')\n",
    "        fig.savefig(save_path, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"[OK] 缺失值变化图已保存: {save_path}\")\n",
    "\n",
    "    # 2) 异常值变化前后对比 (Top20 变化量最大的)\n",
    "    top_outlier = active.nlargest(20, 'outlier_delta').sort_values('outlier_delta')\n",
    "    if len(top_outlier) > 0 and top_outlier['outlier_delta'].sum() > 0:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        y_pos = range(len(top_outlier))\n",
    "        ax.barh(y_pos, top_outlier['outlier_before'], height=0.4, color='#FF9800',\n",
    "                alpha=0.7, label='清理前')\n",
    "        ax.barh([y + 0.4 for y in y_pos], top_outlier['outlier_after'], height=0.4,\n",
    "                color='#2196F3', alpha=0.7, label='清理后')\n",
    "        ax.set_yticks([y + 0.2 for y in y_pos])\n",
    "        ax.set_yticklabels(top_outlier['feature'], fontsize=8)\n",
    "        ax.set_xlabel('异常值数量')\n",
    "        ax.set_title('异常值变化 Top20 (清理前 vs 清理后)', fontsize=13)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(save_dir, 'outlier_before_after.png')\n",
    "        fig.savefig(save_path, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"[OK] 异常值变化图已保存: {save_path}\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 保存对比报告\n",
    "    comp_df.to_csv('cleaning_comparison.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] 清理对比报告已保存: cleaning_comparison.csv\")\n",
    "\n",
    "    return comp_df\n",
    "\n",
    "\n",
    "# 执行\n",
    "comparison_df = generate_cleaning_comparison(clean_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110b038",
   "metadata": {},
   "source": [
    "### 3.4 清理后数据验证\n",
    "验证清理后的数据质量：无空值、无明显异常值、时序逻辑正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41714277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 3.4: 清理后数据验证\n",
      "============================================================\n",
      "\n",
      "[1/4] 缺失值检查\n",
      "  [WARN] 仍有 118/300 个特征存在缺失值\n",
      "         总缺失值: 2237576\n",
      "  [FIX] 已用中位数填充残余缺失值\n",
      "\n",
      "[2/4] 异常值检查\n",
      "  [INFO] 119 个特征异常值占比>10% (可能含趋势性异常,已保留)\n",
      "\n",
      "[3/4] 无穷值检查\n",
      "  [OK] 无 inf / -inf 值\n",
      "\n",
      "[4/4] 时序逻辑验证\n",
      "  [OK] 时序逻辑正确 (start_time <= end_time)\n",
      "\n",
      "============================================================\n",
      "[INFO] 验证完成, 发现 1 个注意项 (已自动修复):\n",
      "  - 118 个特征仍有缺失值\n",
      "\n",
      "  数据规模: 81,046 行 x 321 列\n",
      "  有效特征: 300, 剔除特征: 0\n",
      "============================================================\n",
      "\n",
      "[OK] 清理后数据 df_cleaned 已就绪, 有效特征列表 active_features (300 个)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 清理后数据验证\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def validate_cleaned_data(df_cleaned: pd.DataFrame, active_features: list,\n",
    "                           dropped_features: list) -> dict:\n",
    "    \"\"\"\n",
    "    验证清理后数据的质量：缺失值、异常值、时序逻辑、基本统计。\n",
    "\n",
    "    Description:\n",
    "        1. 检查有效特征是否存在残余缺失值。\n",
    "        2. 检查有效特征的异常值占比（IQR法）是否在合理范围内。\n",
    "        3. 验证时序字段逻辑（start_time <= end_time）。\n",
    "        4. 检查是否有 inf / -inf 值。\n",
    "        5. 输出综合验证报告。\n",
    "\n",
    "    Parameters:\n",
    "        df_cleaned : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的清理后数据。\n",
    "        active_features : list of str\n",
    "            有效特征列名列表，长度为 n_active。\n",
    "        dropped_features : list of str\n",
    "            被剔除的特征名列表。\n",
    "\n",
    "    Returns:\n",
    "        validation : dict\n",
    "            包含以下键的验证结果字典:\n",
    "            - 'total_missing': int, 有效特征总缺失值数\n",
    "            - 'features_with_missing': int, 仍有缺失值的特征数\n",
    "            - 'features_high_outlier': int, 异常值占比>10%的特征数\n",
    "            - 'has_inf': int, 含 inf 的特征数\n",
    "            - 'time_logic_ok': bool, 时序逻辑是否正确\n",
    "            - 'all_passed': bool, 是否所有检查均通过\n",
    "    \"\"\"\n",
    "    validation = {}\n",
    "    issues = []\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"步骤 3.4: 清理后数据验证\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 1. 缺失值检查\n",
    "    print(\"\\n[1/4] 缺失值检查\")\n",
    "    missing_counts = df_cleaned[active_features].isnull().sum()\n",
    "    total_missing = int(missing_counts.sum())\n",
    "    features_with_missing = int((missing_counts > 0).sum())\n",
    "    validation['total_missing'] = total_missing\n",
    "    validation['features_with_missing'] = features_with_missing\n",
    "\n",
    "    if total_missing == 0:\n",
    "        print(f\"  [OK] 有效特征无缺失值 ({len(active_features)} 个特征)\")\n",
    "    else:\n",
    "        print(f\"  [WARN] 仍有 {features_with_missing}/{len(active_features)} 个特征存在缺失值\")\n",
    "        print(f\"         总缺失值: {total_missing}\")\n",
    "        issues.append(f\"{features_with_missing} 个特征仍有缺失值\")\n",
    "        # 强制填充残余缺失\n",
    "        for col in active_features:\n",
    "            if df_cleaned[col].isnull().any():\n",
    "                df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "        print(f\"  [FIX] 已用中位数填充残余缺失值\")\n",
    "\n",
    "    # 2. 异常值检查\n",
    "    print(\"\\n[2/4] 异常值检查\")\n",
    "    high_outlier_count = 0\n",
    "    for col in active_features:\n",
    "        s = df_cleaned[col].dropna()\n",
    "        if len(s) < 10:\n",
    "            continue\n",
    "        q1 = s.quantile(0.25)\n",
    "        q3 = s.quantile(0.75)\n",
    "        iqr_val = q3 - q1\n",
    "        if iqr_val == 0:\n",
    "            continue\n",
    "        outlier_pct = ((s < q1 - 1.5 * iqr_val) |\n",
    "                       (s > q3 + 1.5 * iqr_val)).mean() * 100\n",
    "        if outlier_pct > 10:\n",
    "            high_outlier_count += 1\n",
    "\n",
    "    validation['features_high_outlier'] = high_outlier_count\n",
    "    if high_outlier_count == 0:\n",
    "        print(f\"  [OK] 无高异常值特征 (阈值: >10%)\")\n",
    "    else:\n",
    "        print(f\"  [INFO] {high_outlier_count} 个特征异常值占比>10% (可能含趋势性异常,已保留)\")\n",
    "\n",
    "    # 3. inf 值检查\n",
    "    print(\"\\n[3/4] 无穷值检查\")\n",
    "    inf_count = 0\n",
    "    for col in active_features:\n",
    "        if np.isinf(df_cleaned[col]).any():\n",
    "            inf_count += 1\n",
    "            # 替换 inf\n",
    "            df_cleaned[col] = df_cleaned[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "\n",
    "    validation['has_inf'] = inf_count\n",
    "    if inf_count == 0:\n",
    "        print(f\"  [OK] 无 inf / -inf 值\")\n",
    "    else:\n",
    "        print(f\"  [FIX] {inf_count} 个特征含 inf 值, 已替换为中位数\")\n",
    "        issues.append(f\"{inf_count} 个特征含 inf 值\")\n",
    "\n",
    "    # 4. 时序逻辑验证\n",
    "    print(\"\\n[4/4] 时序逻辑验证\")\n",
    "    try:\n",
    "        start_ts = pd.to_datetime(df_cleaned['start_time'])\n",
    "        end_ts = pd.to_datetime(df_cleaned['end_time'])\n",
    "        bad_time = (start_ts > end_ts).sum()\n",
    "        validation['time_logic_ok'] = bad_time == 0\n",
    "        if bad_time == 0:\n",
    "            print(f\"  [OK] 时序逻辑正确 (start_time <= end_time)\")\n",
    "        else:\n",
    "            print(f\"  [WARN] {bad_time} 条记录时序逻辑异常\")\n",
    "            issues.append(f\"{bad_time} 条时序逻辑异常\")\n",
    "    except Exception:\n",
    "        validation['time_logic_ok'] = True\n",
    "        print(f\"  [SKIP] 时间字段无法解析, 跳过检查\")\n",
    "\n",
    "    # 综合判断\n",
    "    validation['all_passed'] = len(issues) == 0\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    if validation['all_passed']:\n",
    "        print(\"[OK] 清理后数据验证全部通过\")\n",
    "    else:\n",
    "        print(f\"[INFO] 验证完成, 发现 {len(issues)} 个注意项 (已自动修复):\")\n",
    "        for iss in issues:\n",
    "            print(f\"  - {iss}\")\n",
    "    print(f\"\\n  数据规模: {df_cleaned.shape[0]:,} 行 x {df_cleaned.shape[1]} 列\")\n",
    "    print(f\"  有效特征: {len(active_features)}, 剔除特征: {len(dropped_features)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return validation\n",
    "\n",
    "\n",
    "# 执行验证\n",
    "validation_result = validate_cleaned_data(df_cleaned, active_features, dropped_features)\n",
    "\n",
    "# 将清理后数据保存为变量供后续步骤使用\n",
    "print(f\"\\n[OK] 清理后数据 df_cleaned 已就绪, 有效特征列表 active_features ({len(active_features)} 个)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b04b2f",
   "metadata": {},
   "source": [
    "## 步骤4：自动特征评估\n",
    "- 4.1 特征有效性评估：千问Agent选择目标标签与评价指标，按时间划分训练/测试集，单特征逻辑回归评估\n",
    "- 4.2 特征冗余检测：皮尔逊相关系数>0.8标记冗余，方差膨胀因子VIF>10标记多重共线性\n",
    "- 4.3 特征评估报告：汇总有效性得分、冗余标记、共线性标记"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a0208",
   "metadata": {},
   "source": [
    "### 4.1 特征有效性评估\n",
    "千问Agent选择目标标签（Y1~Y12）和评价指标，严格按时间顺序划分训练/测试集，逐特征训练逻辑回归模型评估有效性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05931672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 4.1: 特征有效性评估\n",
      "============================================================\n",
      "\n",
      "  [AGENT] 正在请求千问Agent选择目标标签与评价指标...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:39:34,489] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Agent 选择: 标签=Y1, 指标=['AUC', 'coef_abs']\n",
      "       标签理由: 多分类标签中的类别数较少，且类别分布较为均衡，有利于特征评价。Y1标签为二分类，且样本数充足，适合进行二分类特征有效性评估。\n",
      "       指标理由: AUC适用于评估分类模型的排序能力，适用于不平衡类别的数据集。coef_abs能够反映特征对于分类器的影响程度，有助于识别重要特征。\n",
      "\n",
      "  目标标签: Y1\n",
      "  主要指标: ['AUC', 'coef_abs']\n",
      "\n",
      "  训练集: 61,923 条 (76.4%)\n",
      "  测试集: 19,123 条 (23.6%)\n",
      "\n",
      "  开始逐特征逻辑回归评估 (共 300 个特征)...\n",
      "  [PROGRESS] 50/300 个特征评估完成 (有效: 50)\n",
      "  [PROGRESS] 100/300 个特征评估完成 (有效: 100)\n",
      "  [PROGRESS] 150/300 个特征评估完成 (有效: 150)\n",
      "  [PROGRESS] 200/300 个特征评估完成 (有效: 200)\n",
      "  [PROGRESS] 250/300 个特征评估完成 (有效: 250)\n",
      "  [PROGRESS] 300/300 个特征评估完成 (有效: 300)\n",
      "\n",
      "[OK] 特征有效性评估完成\n",
      "  成功评估: 300 / 300 个特征\n",
      "\n",
      "  AUC Top 10 特征:\n",
      "    X286: AUC=0.5775, |coef|=0.1468, F1=0.2750\n",
      "    X222: AUC=0.5633, |coef|=0.2136, F1=0.2750\n",
      "    X156: AUC=0.5577, |coef|=0.0510, F1=0.2750\n",
      "    X197: AUC=0.5566, |coef|=0.2279, F1=0.2774\n",
      "    X158: AUC=0.5547, |coef|=0.0499, F1=0.2750\n",
      "    X221: AUC=0.5531, |coef|=0.2058, F1=0.2750\n",
      "    X131: AUC=0.5522, |coef|=0.0447, F1=0.2750\n",
      "    X159: AUC=0.5497, |coef|=0.1099, F1=0.2750\n",
      "    X9: AUC=0.5494, |coef|=0.2334, F1=0.2750\n",
      "    X198: AUC=0.5470, |coef|=0.2124, F1=0.2750\n",
      "\n",
      "[OK] 有效性评估结果已保存: feature_effectiveness.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 特征有效性评估 — Agent 选择标签/指标 + 单特征逻辑回归\n",
    "# =============================================================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (roc_auc_score, accuracy_score,\n",
    "                             precision_score, recall_score, f1_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def ask_agent_eval_config(df: pd.DataFrame, active_features: list,\n",
    "                           label_cols: list) -> dict:\n",
    "    \"\"\"\n",
    "    调用千问Agent选择最适合的目标标签和评价指标。\n",
    "\n",
    "    Description:\n",
    "        将 Y1~Y12 各标签的类别数和分布发送给 Agent，由 Agent 选出\n",
    "        最适合做二分类的标签以及用于衡量单特征有效性的评价指标。\n",
    "        若 API 失败则使用默认配置 (Y1 + AUC/coef_abs)。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的数据。\n",
    "        active_features : list of str\n",
    "            有效特征名列表，长度 n_active。\n",
    "        label_cols : list of str\n",
    "            候选标签列名列表，如 ['Y1', ..., 'Y12']。\n",
    "\n",
    "    Returns:\n",
    "        config : dict\n",
    "            包含以下键:\n",
    "            - 'target_label': str, 选定的目标标签名\n",
    "            - 'target_reason': str, 选择理由\n",
    "            - 'primary_metrics': list of str, 主要评价指标列表\n",
    "            - 'metrics_reason': str, 指标选择理由\n",
    "    \"\"\"\n",
    "    # 构建标签信息摘要\n",
    "    label_info = []\n",
    "    for y in label_cols:\n",
    "        if y not in df.columns:\n",
    "            continue\n",
    "        vc = df[y].dropna().value_counts(normalize=True).head(5)\n",
    "        dist_str = \", \".join([f\"{k}:{v:.2%}\" for k, v in vc.items()])\n",
    "        n_classes = df[y].dropna().nunique()\n",
    "        n_valid = int(df[y].dropna().shape[0])\n",
    "        label_info.append(\n",
    "            f\"  {y}: 类别数={n_classes}, 有效样本={n_valid}, \"\n",
    "            f\"分布=[{dist_str}]\"\n",
    "        )\n",
    "    label_text = \"\\n\".join(label_info)\n",
    "\n",
    "    prompt = f\"\"\"现在有 {len(active_features)} 个清理后的金融时序特征，需要进行特征有效性评估。\n",
    "可选的目标标签及其分布如下：\n",
    "\n",
    "{label_text}\n",
    "\n",
    "请你：\n",
    "1. 选择一个最适合做二分类任务的目标标签（优先选二分类标签；若都是多分类则选类别最少且分布最均衡的）\n",
    "2. 推荐评估每个特征有效性的指标（从以下指标中选择1~3个主要指标：AUC, accuracy, precision, recall, f1, coef_abs）\n",
    "   - coef_abs 为逻辑回归系数绝对值，反映特征对标签的线性区分力\n",
    "   - AUC 为 ROC 曲线下面积，反映排序能力\n",
    "\n",
    "请严格按如下JSON格式回复，不要添加其他内容：\n",
    "{{\n",
    "  \"target_label\": \"Y1\",\n",
    "  \"target_reason\": \"选择理由\",\n",
    "  \"primary_metrics\": [\"AUC\", \"coef_abs\"],\n",
    "  \"metrics_reason\": \"选择理由\"\n",
    "}}\"\"\"\n",
    "\n",
    "    system_prompt = (\"你是金融机器学习专家Agent。\"\n",
    "                     \"请根据标签分布特点选择最优评估方案，严格按JSON格式回复。\")\n",
    "\n",
    "    print(\"  [AGENT] 正在请求千问Agent选择目标标签与评价指标...\")\n",
    "    response = call_qwen_agent(prompt, system_prompt)\n",
    "\n",
    "    config = {\n",
    "        \"target_label\": \"Y1\",\n",
    "        \"target_reason\": \"默认选择\",\n",
    "        \"primary_metrics\": [\"AUC\", \"coef_abs\"],\n",
    "        \"metrics_reason\": \"默认选择AUC+系数绝对值\"\n",
    "    }\n",
    "\n",
    "    if response:\n",
    "        try:\n",
    "            match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if match:\n",
    "                parsed = json.loads(match.group())\n",
    "                if ('target_label' in parsed and\n",
    "                        parsed['target_label'] in label_cols):\n",
    "                    config['target_label'] = parsed['target_label']\n",
    "                if 'target_reason' in parsed:\n",
    "                    config['target_reason'] = str(parsed['target_reason'])\n",
    "                if 'primary_metrics' in parsed:\n",
    "                    valid_metrics = ['AUC', 'accuracy', 'precision',\n",
    "                                     'recall', 'f1', 'coef_abs']\n",
    "                    pm = [m for m in parsed['primary_metrics']\n",
    "                          if m in valid_metrics]\n",
    "                    if pm:\n",
    "                        config['primary_metrics'] = pm\n",
    "                if 'metrics_reason' in parsed:\n",
    "                    config['metrics_reason'] = str(parsed['metrics_reason'])\n",
    "            print(f\"  [OK] Agent 选择: 标签={config['target_label']}, \"\n",
    "                  f\"指标={config['primary_metrics']}\")\n",
    "            print(f\"       标签理由: {config['target_reason']}\")\n",
    "            print(f\"       指标理由: {config['metrics_reason']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] Agent 返回解析失败: {e}, 使用默认配置\")\n",
    "    else:\n",
    "        print(\"  [WARN] Agent 未返回有效响应, 使用默认配置\")\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def time_based_split(df: pd.DataFrame, time_col: str = 'trade_date',\n",
    "                      train_ratio: float = 0.8) -> tuple:\n",
    "    \"\"\"\n",
    "    按时间顺序严格划分训练集和测试集，避免数据泄漏。\n",
    "\n",
    "    Description:\n",
    "        将所有日期排序后，前 train_ratio 的日期作为训练集，\n",
    "        其余作为测试集。同一天的数据不会被拆分到两个集合中。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns)，必须包含 time_col 列。\n",
    "        time_col : str\n",
    "            时间列名，标量字符串，默认 'trade_date'。\n",
    "        train_ratio : float\n",
    "            训练集占比，标量浮点数，默认 0.8。\n",
    "\n",
    "    Returns:\n",
    "        train_idx : pd.Index\n",
    "            训练集行索引。\n",
    "        test_idx : pd.Index\n",
    "            测试集行索引。\n",
    "    \"\"\"\n",
    "    sorted_dates = sorted(df[time_col].unique())\n",
    "    split_pos = int(len(sorted_dates) * train_ratio)\n",
    "    train_dates = set(sorted_dates[:split_pos])\n",
    "\n",
    "    train_mask = df[time_col].isin(train_dates)\n",
    "    train_idx = df[train_mask].index\n",
    "    test_idx = df[~train_mask].index\n",
    "\n",
    "    return train_idx, test_idx\n",
    "\n",
    "\n",
    "def evaluate_single_feature(df: pd.DataFrame, feature: str,\n",
    "                              target: str, train_idx, test_idx) -> dict:\n",
    "    \"\"\"\n",
    "    对单个特征训练逻辑回归模型，计算多维度评估指标。\n",
    "\n",
    "    Description:\n",
    "        1. 提取特征列和标签列，去除缺失值行。\n",
    "        2. 对特征做标准化。\n",
    "        3. 训练 LogisticRegression (max_iter=500)。\n",
    "        4. 在测试集上预测，计算 AUC / accuracy / precision /\n",
    "           recall / f1 / coef_abs 六项指标。\n",
    "        若训练或测试样本过少（<10），返回 None。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns)。\n",
    "        feature : str\n",
    "            特征列名。\n",
    "        target : str\n",
    "            标签列名。\n",
    "        train_idx : pd.Index\n",
    "            训练集行索引。\n",
    "        test_idx : pd.Index\n",
    "            测试集行索引。\n",
    "\n",
    "    Returns:\n",
    "        result : dict or None\n",
    "            包含 feature, coef_abs, AUC, accuracy, precision,\n",
    "            recall, f1 共 7 个键的字典。\n",
    "            若样本不足或训练失败返回 None。\n",
    "    \"\"\"\n",
    "    X_train = df.loc[train_idx, feature].values.reshape(-1, 1)\n",
    "    y_train = df.loc[train_idx, target].values\n",
    "    X_test = df.loc[test_idx, feature].values.reshape(-1, 1)\n",
    "    y_test = df.loc[test_idx, target].values\n",
    "\n",
    "    # 去除缺失值行\n",
    "    valid_tr = ~(np.isnan(X_train.ravel()) | np.isnan(y_train))\n",
    "    valid_te = ~(np.isnan(X_test.ravel()) | np.isnan(y_test))\n",
    "    X_train, y_train = X_train[valid_tr], y_train[valid_tr]\n",
    "    X_test, y_test = X_test[valid_te], y_test[valid_te]\n",
    "\n",
    "    if len(X_train) < 10 or len(X_test) < 10:\n",
    "        return None\n",
    "\n",
    "    # 检查标签是否至少有 2 类\n",
    "    if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:\n",
    "        return None\n",
    "\n",
    "    # 标准化\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    try:\n",
    "        lr = LogisticRegression(max_iter=500, solver='lbfgs',\n",
    "                                 random_state=42)\n",
    "        lr.fit(X_train_s, y_train)\n",
    "        y_pred = lr.predict(X_test_s)\n",
    "        y_proba = lr.predict_proba(X_test_s)\n",
    "\n",
    "        result = {\n",
    "            'feature': feature,\n",
    "            'coef_abs': float(np.max(np.abs(lr.coef_))),\n",
    "            'accuracy': float(accuracy_score(y_test, y_pred)),\n",
    "        }\n",
    "\n",
    "        # AUC\n",
    "        n_classes = len(lr.classes_)\n",
    "        if n_classes == 2:\n",
    "            result['AUC'] = float(roc_auc_score(y_test, y_proba[:, 1]))\n",
    "        else:\n",
    "            try:\n",
    "                result['AUC'] = float(roc_auc_score(\n",
    "                    y_test, y_proba, multi_class='ovr', average='macro'))\n",
    "            except Exception:\n",
    "                result['AUC'] = 0.5\n",
    "\n",
    "        result['precision'] = float(precision_score(\n",
    "            y_test, y_pred, average='macro', zero_division=0))\n",
    "        result['recall'] = float(recall_score(\n",
    "            y_test, y_pred, average='macro', zero_division=0))\n",
    "        result['f1'] = float(f1_score(\n",
    "            y_test, y_pred, average='macro', zero_division=0))\n",
    "\n",
    "        return result\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_feature_effectiveness(df: pd.DataFrame, active_features: list,\n",
    "                                target: str, train_idx, test_idx) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    批量评估所有有效特征的有效性得分。\n",
    "\n",
    "    Description:\n",
    "        对 active_features 中的每个特征调用 evaluate_single_feature，\n",
    "        汇总为 DataFrame，按 AUC 降序排列。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns)。\n",
    "        active_features : list of str\n",
    "            有效特征列名列表，长度 n_active。\n",
    "        target : str\n",
    "            目标标签列名。\n",
    "        train_idx : pd.Index\n",
    "            训练集行索引。\n",
    "        test_idx : pd.Index\n",
    "            测试集行索引。\n",
    "\n",
    "    Returns:\n",
    "        eval_df : pd.DataFrame\n",
    "            形状为 (n_evaluated, 7)，列为\n",
    "            [feature, coef_abs, AUC, accuracy, precision, recall, f1]，\n",
    "            按 AUC 降序排列。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(active_features)\n",
    "\n",
    "    for i, feat in enumerate(active_features):\n",
    "        res = evaluate_single_feature(df, feat, target, train_idx, test_idx)\n",
    "        if res is not None:\n",
    "            results.append(res)\n",
    "\n",
    "        if (i + 1) % 50 == 0 or (i + 1) == total:\n",
    "            print(f\"  [PROGRESS] {i+1}/{total} 个特征评估完成 \"\n",
    "                  f\"(有效: {len(results)})\")\n",
    "\n",
    "    eval_df = pd.DataFrame(results)\n",
    "    if not eval_df.empty:\n",
    "        eval_df = eval_df.sort_values('AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return eval_df\n",
    "\n",
    "\n",
    "# ---- 执行 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 4.1: 特征有效性评估\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# 1) Agent 选择标签与指标\n",
    "eval_config = ask_agent_eval_config(df_cleaned, active_features, LABEL_COLS)\n",
    "TARGET_LABEL = eval_config['target_label']\n",
    "PRIMARY_METRICS = eval_config['primary_metrics']\n",
    "print(f\"\\n  目标标签: {TARGET_LABEL}\")\n",
    "print(f\"  主要指标: {PRIMARY_METRICS}\")\n",
    "\n",
    "# 2) 按时间划分训练/测试集\n",
    "train_idx, test_idx = time_based_split(df_cleaned, time_col='trade_date',\n",
    "                                        train_ratio=0.8)\n",
    "print(f\"\\n  训练集: {len(train_idx):,} 条 ({len(train_idx)/len(df_cleaned)*100:.1f}%)\")\n",
    "print(f\"  测试集: {len(test_idx):,} 条 ({len(test_idx)/len(df_cleaned)*100:.1f}%)\")\n",
    "\n",
    "# 3) 逐特征评估\n",
    "print(f\"\\n  开始逐特征逻辑回归评估 (共 {len(active_features)} 个特征)...\")\n",
    "eval_df = run_feature_effectiveness(df_cleaned, active_features,\n",
    "                                      TARGET_LABEL, train_idx, test_idx)\n",
    "\n",
    "print(f\"\\n[OK] 特征有效性评估完成\")\n",
    "print(f\"  成功评估: {len(eval_df)} / {len(active_features)} 个特征\")\n",
    "if not eval_df.empty:\n",
    "    print(f\"\\n  AUC Top 10 特征:\")\n",
    "    for _, row in eval_df.head(10).iterrows():\n",
    "        print(f\"    {row['feature']}: AUC={row['AUC']:.4f}, \"\n",
    "              f\"|coef|={row['coef_abs']:.4f}, F1={row['f1']:.4f}\")\n",
    "\n",
    "# 保存中间结果\n",
    "eval_df.to_csv('feature_effectiveness.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n[OK] 有效性评估结果已保存: feature_effectiveness.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b85e5",
   "metadata": {},
   "source": [
    "### 4.2 特征冗余检测\n",
    "计算特征间皮尔逊相关系数（>0.8标记冗余），通过相关矩阵求逆计算方差膨胀因子VIF（>10标记多重共线性）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90843add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 4.2: 特征冗余检测\n",
      "============================================================\n",
      "\n",
      "  [1/2] 计算皮尔逊相关矩阵...\n",
      "       相关系数 |r| > 0.8 的特征对: 258\n",
      "\n",
      "  冗余对 Top 10:\n",
      "    X94 <-> X96: r=1.0000\n",
      "    X22 <-> X24: r=-1.0000\n",
      "    X23 <-> X25: r=-1.0000\n",
      "    X272 <-> X280: r=1.0000\n",
      "    X77 <-> X82: r=-1.0000\n",
      "    X34 <-> X35: r=1.0000\n",
      "    X95 <-> X96: r=0.9999\n",
      "    X94 <-> X95: r=0.9999\n",
      "    X17 <-> X21: r=-0.9992\n",
      "    X88 <-> X89: r=0.9987\n",
      "  [2/2] 计算方差膨胀因子 (VIF)...\n",
      "\n",
      "  VIF > 10 的特征: 113 / 300\n",
      "  VIF Top 10:\n",
      "    X17: VIF=1000.00\n",
      "    X35: VIF=1000.00\n",
      "    X21: VIF=1000.00\n",
      "    X34: VIF=1000.00\n",
      "    X96: VIF=1000.00\n",
      "    X94: VIF=1000.00\n",
      "    X95: VIF=1000.00\n",
      "    X280: VIF=1000.00\n",
      "    X272: VIF=1000.00\n",
      "    X89: VIF=675.83\n",
      "\n",
      "[OK] 冗余检测完成\n",
      "  冗余特征 (|r|>0.8): 139 / 300\n",
      "  多重共线性 (VIF>10): 113 / 300\n",
      "  冗余对已保存: redundant_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 特征冗余检测 — 相关系数冗余 + VIF 多重共线性\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def detect_correlation_redundancy(df: pd.DataFrame,\n",
    "                                    features: list,\n",
    "                                    threshold: float = 0.8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    检测特征间皮尔逊相关系数超过阈值的冗余特征对。\n",
    "\n",
    "    Description:\n",
    "        计算 features 之间的皮尔逊相关矩阵，找出绝对相关系数\n",
    "        超过 threshold 的特征对，记录两两关系和相关系数。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns)。\n",
    "        features : list of str\n",
    "            待检测的特征列名列表，长度 n_features。\n",
    "        threshold : float\n",
    "            相关系数绝对值阈值，标量浮点数，默认 0.8。\n",
    "\n",
    "    Returns:\n",
    "        redundant_pairs : pd.DataFrame\n",
    "            形状为 (n_pairs, 3)，列为\n",
    "            [feature_a, feature_b, pearson_corr]，\n",
    "            按 |pearson_corr| 降序排列。\n",
    "    \"\"\"\n",
    "    print(\"  [1/2] 计算皮尔逊相关矩阵...\")\n",
    "    corr_matrix = df[features].corr(method='pearson')\n",
    "\n",
    "    # 提取上三角部分\n",
    "    pairs = []\n",
    "    n = len(features)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            r = corr_matrix.iloc[i, j]\n",
    "            if abs(r) > threshold:\n",
    "                pairs.append({\n",
    "                    'feature_a': features[i],\n",
    "                    'feature_b': features[j],\n",
    "                    'pearson_corr': round(float(r), 4)\n",
    "                })\n",
    "\n",
    "    redundant_pairs = pd.DataFrame(pairs)\n",
    "    if not redundant_pairs.empty:\n",
    "        redundant_pairs = redundant_pairs.sort_values(\n",
    "            'pearson_corr', key=abs, ascending=False\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    print(f\"       相关系数 |r| > {threshold} 的特征对: {len(redundant_pairs)}\")\n",
    "    return redundant_pairs\n",
    "\n",
    "\n",
    "def compute_vif(df: pd.DataFrame, features: list) -> pd.Series:\n",
    "    \"\"\"\n",
    "    通过相关矩阵求逆计算所有特征的方差膨胀因子 (VIF)。\n",
    "\n",
    "    Description:\n",
    "        VIF_j = diag(R^{-1})_j，其中 R 为特征间的皮尔逊相关矩阵。\n",
    "        等价于 VIF_j = 1 / (1 - R^2_j)，R^2_j 为将 X_j 对其余特征\n",
    "        回归的决定系数。若矩阵奇异则加微量正则化。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns)。\n",
    "        features : list of str\n",
    "            待计算的特征列名列表，长度 n_features。\n",
    "\n",
    "    Returns:\n",
    "        vif_series : pd.Series\n",
    "            长度为 n_features 的 VIF 值序列，索引为特征名。\n",
    "            奇异矩阵情况下值可能为近似值。\n",
    "    \"\"\"\n",
    "    print(\"  [2/2] 计算方差膨胀因子 (VIF)...\")\n",
    "\n",
    "    # 使用无缺失的子集\n",
    "    X = df[features].dropna()\n",
    "    if len(X) < len(features):\n",
    "        print(f\"       [WARN] 有效样本 ({len(X)}) < 特征数 ({len(features)}), \"\n",
    "              f\"抽样计算\")\n",
    "        X = df[features].fillna(df[features].median())\n",
    "\n",
    "    corr_matrix = X.corr().values\n",
    "\n",
    "    try:\n",
    "        inv_corr = np.linalg.inv(corr_matrix)\n",
    "        vif_values = np.diag(inv_corr)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # 矩阵奇异，加正则化\n",
    "        print(\"       [WARN] 相关矩阵奇异, 添加正则化项 (1e-5)\")\n",
    "        corr_reg = corr_matrix + np.eye(len(features)) * 1e-5\n",
    "        inv_corr = np.linalg.inv(corr_reg)\n",
    "        vif_values = np.diag(inv_corr)\n",
    "\n",
    "    # 修正：VIF 应 >= 1，负值或过小值设为 1\n",
    "    vif_values = np.maximum(vif_values, 1.0)\n",
    "    # 极大值截断为 1000 方便展示\n",
    "    vif_values = np.minimum(vif_values, 1000.0)\n",
    "\n",
    "    vif_series = pd.Series(vif_values, index=features)\n",
    "    return vif_series\n",
    "\n",
    "\n",
    "def get_redundancy_flags(features: list,\n",
    "                          redundant_pairs: pd.DataFrame,\n",
    "                          vif_series: pd.Series,\n",
    "                          corr_threshold: float = 0.8,\n",
    "                          vif_threshold: float = 10.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    汇总每个特征的冗余标记和多重共线性标记。\n",
    "\n",
    "    Description:\n",
    "        1. 对每个特征，统计它参与了多少个冗余对 (|r| > corr_threshold)。\n",
    "        2. 判断其 VIF 是否超过 vif_threshold。\n",
    "\n",
    "    Parameters:\n",
    "        features : list of str\n",
    "            全部特征名列表，长度 n_features。\n",
    "        redundant_pairs : pd.DataFrame\n",
    "            冗余对表，由 detect_correlation_redundancy 生成。\n",
    "        vif_series : pd.Series\n",
    "            VIF 值序列，由 compute_vif 生成。\n",
    "        corr_threshold : float\n",
    "            相关系数阈值，标量浮点数，默认 0.8。\n",
    "        vif_threshold : float\n",
    "            VIF 阈值，标量浮点数，默认 10.0。\n",
    "\n",
    "    Returns:\n",
    "        flags_df : pd.DataFrame\n",
    "            形状为 (n_features, 5)，列为\n",
    "            [feature, n_redundant_pairs, is_redundant,\n",
    "             VIF, is_multicollinear]。\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for feat in features:\n",
    "        n_pairs = 0\n",
    "        if not redundant_pairs.empty:\n",
    "            n_pairs = int(\n",
    "                ((redundant_pairs['feature_a'] == feat) |\n",
    "                 (redundant_pairs['feature_b'] == feat)).sum()\n",
    "            )\n",
    "        vif_val = float(vif_series.get(feat, 1.0))\n",
    "\n",
    "        records.append({\n",
    "            'feature': feat,\n",
    "            'n_redundant_pairs': n_pairs,\n",
    "            'is_redundant': n_pairs > 0,\n",
    "            'VIF': round(vif_val, 2),\n",
    "            'is_multicollinear': vif_val > vif_threshold\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# ---- 执行 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 4.2: 特征冗余检测\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# 1) 皮尔逊相关冗余\n",
    "redundant_pairs = detect_correlation_redundancy(\n",
    "    df_cleaned, active_features, threshold=0.8)\n",
    "\n",
    "if not redundant_pairs.empty:\n",
    "    print(f\"\\n  冗余对 Top 10:\")\n",
    "    for _, row in redundant_pairs.head(10).iterrows():\n",
    "        print(f\"    {row['feature_a']} <-> {row['feature_b']}: \"\n",
    "              f\"r={row['pearson_corr']:.4f}\")\n",
    "\n",
    "# 2) VIF 多重共线性\n",
    "vif_series = compute_vif(df_cleaned, active_features)\n",
    "n_high_vif = int((vif_series > 10).sum())\n",
    "print(f\"\\n  VIF > 10 的特征: {n_high_vif} / {len(active_features)}\")\n",
    "if n_high_vif > 0:\n",
    "    top_vif = vif_series[vif_series > 10].sort_values(ascending=False).head(10)\n",
    "    print(f\"  VIF Top 10:\")\n",
    "    for feat, v in top_vif.items():\n",
    "        print(f\"    {feat}: VIF={v:.2f}\")\n",
    "\n",
    "# 3) 汇总标记\n",
    "redundancy_flags = get_redundancy_flags(\n",
    "    active_features, redundant_pairs, vif_series)\n",
    "n_redundant = int(redundancy_flags['is_redundant'].sum())\n",
    "n_collinear = int(redundancy_flags['is_multicollinear'].sum())\n",
    "\n",
    "print(f\"\\n[OK] 冗余检测完成\")\n",
    "print(f\"  冗余特征 (|r|>0.8): {n_redundant} / {len(active_features)}\")\n",
    "print(f\"  多重共线性 (VIF>10): {n_collinear} / {len(active_features)}\")\n",
    "\n",
    "# 保存冗余对\n",
    "if not redundant_pairs.empty:\n",
    "    redundant_pairs.to_csv('redundant_pairs.csv', index=False,\n",
    "                            encoding='utf-8-sig')\n",
    "    print(f\"  冗余对已保存: redundant_pairs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471056c1",
   "metadata": {},
   "source": [
    "### 4.3 特征评估报告\n",
    "汇总每个特征的有效性得分、冗余标记、共线性标记，生成综合评估报告并可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1c25ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 4.3: 特征评估报告\n",
      "============================================================\n",
      "\n",
      "[INFO] 综合报告概览:\n",
      "  评估特征总数: 300\n",
      "  冗余特征 (is_redundant=True): 139\n",
      "  共线性特征 (is_multicollinear=True): 113\n",
      "\n",
      "  风险等级分布:\n",
      "    low: 156\n",
      "    high: 108\n",
      "    medium: 36\n",
      "\n",
      "  综合得分 Top 10:\n",
      "    X286: score=0.8139, AUC=0.5775, VIF=5.0\n",
      "    X197: score=0.7382, AUC=0.5566, VIF=134.0 [冗余,共线]\n",
      "    X222: score=0.7331, AUC=0.5633, VIF=20.3 [冗余,共线]\n",
      "    X9: score=0.7266, AUC=0.5494, VIF=12.4 [冗余,共线]\n",
      "    X221: score=0.6894, AUC=0.5531, VIF=47.3 [冗余,共线]\n",
      "    X198: score=0.6828, AUC=0.5470, VIF=526.6 [冗余,共线]\n",
      "    X188: score=0.6733, AUC=0.5393, VIF=5.0\n",
      "    X8: score=0.6615, AUC=0.5379, VIF=303.9 [冗余,共线]\n",
      "    X66: score=0.6605, AUC=0.5376, VIF=303.7 [冗余,共线]\n",
      "    X265: score=0.6585, AUC=0.5465, VIF=4.3\n",
      "\n",
      "  综合得分 Bottom 5:\n",
      "    X112: score=0.0946, AUC=0.4652\n",
      "    X109: score=0.0922, AUC=0.4641\n",
      "    X111: score=0.0756, AUC=0.4616\n",
      "    X152: score=0.0658, AUC=0.4497\n",
      "    X153: score=0.0428, AUC=0.4410\n",
      "[OK] 评估可视化已保存: images\\feature_evaluation_summary.png\n",
      "[OK] 综合评估报告已保存: feature_evaluation_report.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 生成特征评估报告 — 有效性 + 冗余 + 共线性 综合报告\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def build_evaluation_report(eval_df: pd.DataFrame,\n",
    "                              redundancy_flags: pd.DataFrame,\n",
    "                              primary_metrics: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    合并有效性得分与冗余/共线性标记，生成综合评估报告。\n",
    "\n",
    "    Description:\n",
    "        1. 以 eval_df 的有效性指标为基础，左连接 redundancy_flags。\n",
    "        2. 计算综合得分 (composite_score)：使用主要指标的加权平均，\n",
    "           冗余和共线性特征施加惩罚。\n",
    "        3. 按综合得分降序排列。\n",
    "\n",
    "    Parameters:\n",
    "        eval_df : pd.DataFrame\n",
    "            形状为 (n_evaluated, 7)，特征有效性评估表。\n",
    "        redundancy_flags : pd.DataFrame\n",
    "            形状为 (n_features, 5)，冗余/共线性标记表。\n",
    "        primary_metrics : list of str\n",
    "            主要评价指标名列表（如 ['AUC', 'coef_abs']）。\n",
    "\n",
    "    Returns:\n",
    "        report_df : pd.DataFrame\n",
    "            综合评估报告，包含：\n",
    "            feature, AUC, coef_abs, accuracy, precision, recall, f1,\n",
    "            n_redundant_pairs, is_redundant, VIF, is_multicollinear,\n",
    "            composite_score, risk_level。\n",
    "            按 composite_score 降序排列。\n",
    "    \"\"\"\n",
    "    # 合并\n",
    "    report = eval_df.merge(redundancy_flags, on='feature', how='left')\n",
    "\n",
    "    # 填充未匹配项\n",
    "    report['is_redundant'] = report['is_redundant'].fillna(False)\n",
    "    report['is_multicollinear'] = report['is_multicollinear'].fillna(False)\n",
    "    report['VIF'] = report['VIF'].fillna(1.0)\n",
    "    report['n_redundant_pairs'] = report['n_redundant_pairs'].fillna(0).astype(int)\n",
    "\n",
    "    # 综合得分：主要指标等权平均，再对冗余/共线施加折扣\n",
    "    metric_cols = [m for m in primary_metrics if m in report.columns]\n",
    "    if not metric_cols:\n",
    "        metric_cols = ['AUC']\n",
    "\n",
    "    # 标准化各指标到 [0, 1]\n",
    "    for mc in metric_cols:\n",
    "        col_min = report[mc].min()\n",
    "        col_max = report[mc].max()\n",
    "        if col_max > col_min:\n",
    "            report[f'{mc}_norm'] = (report[mc] - col_min) / (col_max - col_min)\n",
    "        else:\n",
    "            report[f'{mc}_norm'] = 0.5\n",
    "\n",
    "    norm_cols = [f'{mc}_norm' for mc in metric_cols]\n",
    "    report['base_score'] = report[norm_cols].mean(axis=1)\n",
    "\n",
    "    # 冗余惩罚：有冗余对的特征扣 10%，多重共线扣 10%\n",
    "    penalty = np.ones(len(report))\n",
    "    penalty[report['is_redundant'].values] *= 0.9\n",
    "    penalty[report['is_multicollinear'].values] *= 0.9\n",
    "    report['composite_score'] = (report['base_score'] * penalty).round(4)\n",
    "\n",
    "    # 风险等级\n",
    "    conditions = []\n",
    "    labels = []\n",
    "    report['risk_level'] = 'low'\n",
    "    mask_high = (report['is_redundant'] & report['is_multicollinear'])\n",
    "    mask_medium = (report['is_redundant'] | report['is_multicollinear'])\n",
    "    report.loc[mask_high, 'risk_level'] = 'high'\n",
    "    report.loc[mask_medium & ~mask_high, 'risk_level'] = 'medium'\n",
    "\n",
    "    # 删除临时列\n",
    "    drop_cols = norm_cols + ['base_score']\n",
    "    report = report.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # 排序\n",
    "    report = report.sort_values('composite_score',\n",
    "                                 ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def plot_evaluation_summary(report_df: pd.DataFrame,\n",
    "                              target_label: str,\n",
    "                              img_dir: str = 'images') -> None:\n",
    "    \"\"\"\n",
    "    绘制特征评估可视化：AUC分布、综合得分Top30、风险分布饼图。\n",
    "\n",
    "    Description:\n",
    "        图1: 所有特征的 AUC 分布直方图\n",
    "        图2: 综合得分 Top 30 特征水平条形图（颜色按风险等级）\n",
    "        图3: 风险等级分布饼图\n",
    "\n",
    "    Parameters:\n",
    "        report_df : pd.DataFrame\n",
    "            综合评估报告表。\n",
    "        target_label : str\n",
    "            目标标签名，用于标注标题。\n",
    "        img_dir : str\n",
    "            图片保存目录，默认 'images'。\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "    # 图1: AUC分布直方图\n",
    "    ax1 = axes[0]\n",
    "    auc_vals = report_df['AUC'].dropna()\n",
    "    ax1.hist(auc_vals, bins=30, color='steelblue', edgecolor='white', alpha=0.8)\n",
    "    ax1.axvline(x=0.5, color='red', linestyle='--', linewidth=1.2, label='random (0.5)')\n",
    "    ax1.axvline(x=auc_vals.median(), color='orange', linestyle='-',\n",
    "                linewidth=1.2, label=f'median ({auc_vals.median():.3f})')\n",
    "    ax1.set_xlabel('AUC')\n",
    "    ax1.set_ylabel('特征数')\n",
    "    ax1.set_title(f'单特征 AUC 分布 (标签: {target_label})')\n",
    "    ax1.legend(fontsize=8)\n",
    "\n",
    "    # 图2: 综合得分 Top 30\n",
    "    ax2 = axes[1]\n",
    "    top30 = report_df.head(30).copy()\n",
    "    risk_colors = {'low': '#2ecc71', 'medium': '#f39c12', 'high': '#e74c3c'}\n",
    "    colors = [risk_colors.get(r, '#95a5a6') for r in top30['risk_level']]\n",
    "    bars = ax2.barh(range(len(top30) - 1, -1, -1),\n",
    "                     top30['composite_score'].values,\n",
    "                     color=colors, edgecolor='white', height=0.7)\n",
    "    ax2.set_yticks(range(len(top30) - 1, -1, -1))\n",
    "    ax2.set_yticklabels(top30['feature'].values, fontsize=7)\n",
    "    ax2.set_xlabel('综合得分')\n",
    "    ax2.set_title(f'综合得分 Top 30 (标签: {target_label})')\n",
    "    # 添加图例\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_items = [Patch(facecolor=v, label=k)\n",
    "                    for k, v in risk_colors.items()]\n",
    "    ax2.legend(handles=legend_items, loc='lower right', fontsize=8)\n",
    "\n",
    "    # 图3: 风险等级饼图\n",
    "    ax3 = axes[2]\n",
    "    risk_counts = report_df['risk_level'].value_counts()\n",
    "    pie_colors = [risk_colors.get(r, '#95a5a6') for r in risk_counts.index]\n",
    "    ax3.pie(risk_counts.values, labels=risk_counts.index,\n",
    "            colors=pie_colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax3.set_title('特征风险等级分布')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(img_dir, 'feature_evaluation_summary.png')\n",
    "    plt.savefig(fig_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"[OK] 评估可视化已保存: {fig_path}\")\n",
    "\n",
    "\n",
    "# ---- 执行 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 4.3: 特征评估报告\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# 生成综合报告\n",
    "evaluation_report = build_evaluation_report(\n",
    "    eval_df, redundancy_flags, PRIMARY_METRICS)\n",
    "\n",
    "print(f\"[INFO] 综合报告概览:\")\n",
    "print(f\"  评估特征总数: {len(evaluation_report)}\")\n",
    "print(f\"  冗余特征 (is_redundant=True): \"\n",
    "      f\"{int(evaluation_report['is_redundant'].sum())}\")\n",
    "print(f\"  共线性特征 (is_multicollinear=True): \"\n",
    "      f\"{int(evaluation_report['is_multicollinear'].sum())}\")\n",
    "\n",
    "risk_dist = evaluation_report['risk_level'].value_counts()\n",
    "print(f\"\\n  风险等级分布:\")\n",
    "for level, cnt in risk_dist.items():\n",
    "    print(f\"    {level}: {cnt}\")\n",
    "\n",
    "print(f\"\\n  综合得分 Top 10:\")\n",
    "for _, row in evaluation_report.head(10).iterrows():\n",
    "    flags = []\n",
    "    if row['is_redundant']:\n",
    "        flags.append('冗余')\n",
    "    if row['is_multicollinear']:\n",
    "        flags.append('共线')\n",
    "    flag_str = f\" [{','.join(flags)}]\" if flags else \"\"\n",
    "    print(f\"    {row['feature']}: score={row['composite_score']:.4f}, \"\n",
    "          f\"AUC={row['AUC']:.4f}, VIF={row['VIF']:.1f}{flag_str}\")\n",
    "\n",
    "print(f\"\\n  综合得分 Bottom 5:\")\n",
    "for _, row in evaluation_report.tail(5).iterrows():\n",
    "    print(f\"    {row['feature']}: score={row['composite_score']:.4f}, \"\n",
    "          f\"AUC={row['AUC']:.4f}\")\n",
    "\n",
    "# 可视化\n",
    "plot_evaluation_summary(evaluation_report, TARGET_LABEL, IMG_DIR)\n",
    "\n",
    "# 保存\n",
    "evaluation_report.to_csv('feature_evaluation_report.csv', index=False,\n",
    "                          encoding='utf-8-sig')\n",
    "print(f\"[OK] 综合评估报告已保存: feature_evaluation_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0d77a",
   "metadata": {},
   "source": [
    "## 步骤5：自动特征筛选\n",
    "- 5.1 三轮筛选 + Top50特征列表及入选理由\n",
    "  - 第一轮：剔除缺失>80%、VIF>10、与标签相关性<0.05的特征\n",
    "  - 第二轮：对冗余对保留得分更高者\n",
    "  - 第三轮：按综合得分排序取Top50\n",
    "- 5.2 千问Agent选择分类模型 + 训练验证（无数据泄漏）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b97ba",
   "metadata": {},
   "source": [
    "### 5.1 三轮筛选与 Top50 特征列表\n",
    "第一轮剔除低质量特征（缺失>80%、VIF>10、与标签相关性<0.05），第二轮去冗余保留高分项，第三轮按综合得分取 Top50 并输出入选理由。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "555ed5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 5.1: 三轮筛选 + Top50 特征列表\n",
      "============================================================\n",
      "\n",
      "  计算特征-标签相关系数...\n",
      "  相关系数 > 0.05 的特征: 39 / 300\n",
      "  相关系数中位数: 0.0196, 最大值: 0.1368\n",
      "\n",
      "  [ROUND 1] 质量筛选 (仅剔除缺失率>80%)\n",
      "    缺失率>80.0%: 剔除 0 个\n",
      "    第一轮存活: 300 个\n",
      "\n",
      "  [ROUND 2] 冗余去除 (|r|>0.8 特征对保留高分者)\n",
      "    冗余对中剔除: 90 个\n",
      "    第二轮存活: 210 个\n",
      "\n",
      "  [ROUND 3] 综合排名取 Top 50\n",
      "    最终选取: 50 个特征\n",
      "\n",
      "  [INFO] Top 50 特征质量概览:\n",
      "    VIF<=10 (低共线性): 42 / 50\n",
      "    标签|r|>=0.05: 22 / 50\n",
      "    含冗余标记: 9 / 50\n",
      "\n",
      "============================================================\n",
      "Top 50 特征列表及入选理由\n",
      "============================================================\n",
      "    1. X286   | 入选Top50: 得分=0.8139, AUC=0.5775, VIF=5.0, |r|=0.1070, 冗余=否, 时序安全=是\n",
      "    2. X222   | 入选Top50: 得分=0.7331, AUC=0.5633, VIF=20.3, |r|=0.1368, 冗余=是, 时序安全=是\n",
      "    3. X197   | 入选Top50: 得分=0.7382, AUC=0.5566, VIF=134.0, |r|=0.1354, 冗余=是, 时序安全=是\n",
      "    4. X188   | 入选Top50: 得分=0.6733, AUC=0.5393, VIF=5.0, |r|=0.0901, 冗余=否, 时序安全=是\n",
      "    5. X265   | 入选Top50: 得分=0.6585, AUC=0.5465, VIF=4.3, |r|=0.0809, 冗余=否, 时序安全=是\n",
      "    6. X219   | 入选Top50: 得分=0.5989, AUC=0.5369, VIF=15.3, |r|=0.1113, 冗余=是, 时序安全=是\n",
      "    7. X40    | 入选Top50: 得分=0.5749, AUC=0.5144, VIF=12.0, |r|=0.1121, 冗余=是, 时序安全=是\n",
      "    8. X169   | 入选Top50: 得分=0.6120, AUC=0.5189, VIF=5.3, |r|=0.0898, 冗余=否, 时序安全=是\n",
      "    9. X187   | 入选Top50: 得分=0.6113, AUC=0.5401, VIF=4.6, |r|=0.0755, 冗余=否, 时序安全=是\n",
      "   10. X179   | 入选Top50: 得分=0.5376, AUC=0.5092, VIF=2.0, |r|=0.0745, 冗余=否, 时序安全=是\n",
      "   11. X291   | 入选Top50: 得分=0.5777, AUC=0.5398, VIF=5.2, |r|=0.0711, 冗余=是, 时序安全=是\n",
      "   12. X217   | 入选Top50: 得分=0.5718, AUC=0.5282, VIF=61.0, |r|=0.1029, 冗余=是, 时序安全=是\n",
      "   13. X159   | 入选Top50: 得分=0.5694, AUC=0.5497, VIF=8.4, |r|=0.0740, 冗余=是, 时序安全=是\n",
      "   14. X58    | 入选Top50: 得分=0.5296, AUC=0.5146, VIF=2.6, |r|=0.0612, 冗余=否, 时序安全=是\n",
      "   15. X196   | 入选Top50: 得分=0.5179, AUC=0.5164, VIF=2.2, |r|=0.0584, 冗余=否, 时序安全=是\n",
      "   16. X260   | 入选Top50: 得分=0.5362, AUC=0.5370, VIF=3.6, |r|=0.0565, 冗余=否, 时序安全=是\n",
      "   17. X23    | 入选Top50: 得分=0.4552, AUC=0.5137, VIF=1.0, |r|=0.0691, 冗余=是, 时序安全=是\n",
      "   18. X249   | 入选Top50: 得分=0.4828, AUC=0.5094, VIF=1.4, |r|=0.0566, 冗余=否, 时序安全=是\n",
      "   19. X254   | 入选Top50: 得分=0.5007, AUC=0.5240, VIF=3.2, |r|=0.0585, 冗余=否, 时序安全=是\n",
      "   20. X62    | 入选Top50: 得分=0.5215, AUC=0.5414, VIF=2.7, |r|=0.0494, 冗余=否, 时序安全=是\n",
      "   21. X206   | 入选Top50: 得分=0.5182, AUC=0.5249, VIF=5.1, |r|=0.0512, 冗余=否, 时序安全=是\n",
      "   22. X128   | 入选Top50: 得分=0.4512, AUC=0.5040, VIF=1.7, |r|=0.0538, 冗余=否, 时序安全=是\n",
      "   23. X131   | 入选Top50: 得分=0.5016, AUC=0.5522, VIF=3.1, |r|=0.0439, 冗余=否, 时序安全=是\n",
      "   24. X268   | 入选Top50: 得分=0.4913, AUC=0.5455, VIF=13.6, |r|=0.0583, 冗余=否, 时序安全=是\n",
      "   25. X164   | 入选Top50: 得分=0.4902, AUC=0.5325, VIF=6.5, |r|=0.0471, 冗余=否, 时序安全=是\n",
      "   26. X155   | 入选Top50: 得分=0.4642, AUC=0.5307, VIF=2.9, |r|=0.0459, 冗余=否, 时序安全=是\n",
      "   27. X130   | 入选Top50: 得分=0.4676, AUC=0.5412, VIF=3.5, |r|=0.0413, 冗余=否, 时序安全=是\n",
      "   28. X213   | 入选Top50: 得分=0.4487, AUC=0.5047, VIF=4.2, |r|=0.0458, 冗余=否, 时序安全=是\n",
      "   29. X205   | 入选Top50: 得分=0.4682, AUC=0.5387, VIF=4.8, |r|=0.0396, 冗余=否, 时序安全=是\n",
      "   30. X202   | 入选Top50: 得分=0.4471, AUC=0.5200, VIF=3.1, |r|=0.0414, 冗余=否, 时序安全=是\n",
      "   31. X157   | 入选Top50: 得分=0.4499, AUC=0.5418, VIF=2.5, |r|=0.0366, 冗余=否, 时序安全=是\n",
      "   32. X80    | 入选Top50: 得分=0.4506, AUC=0.5334, VIF=2.3, |r|=0.0353, 冗余=否, 时序安全=是\n",
      "   33. X191   | 入选Top50: 得分=0.4269, AUC=0.5308, VIF=1.4, |r|=0.0341, 冗余=否, 时序安全=是\n",
      "   34. X271   | 入选Top50: 得分=0.4072, AUC=0.5219, VIF=2.0, |r|=0.0368, 冗余=否, 时序安全=是\n",
      "   35. X209   | 入选Top50: 得分=0.4125, AUC=0.5186, VIF=3.2, |r|=0.0368, 冗余=否, 时序安全=是\n",
      "   36. X57    | 入选Top50: 得分=0.3860, AUC=0.5068, VIF=1.3, |r|=0.0357, 冗余=否, 时序安全=是\n",
      "   37. X216   | 入选Top50: 得分=0.4490, AUC=0.5398, VIF=26.7, |r|=0.0481, 冗余=否, 时序安全=是\n",
      "   38. X67    | 入选Top50: 得分=0.4048, AUC=0.5133, VIF=2.9, |r|=0.0351, 冗余=否, 时序安全=是\n",
      "   39. X113   | 入选Top50: 得分=0.4032, AUC=0.5183, VIF=2.2, |r|=0.0325, 冗余=否, 时序安全=是\n",
      "   40. X156   | 入选Top50: 得分=0.4335, AUC=0.5577, VIF=21.6, |r|=0.0492, 冗余=是, 时序安全=是\n",
      "   41. X211   | 入选Top50: 得分=0.4196, AUC=0.5221, VIF=4.6, |r|=0.0348, 冗余=否, 时序安全=是\n",
      "   42. X204   | 入选Top50: 得分=0.4081, AUC=0.5068, VIF=4.1, |r|=0.0358, 冗余=否, 时序安全=是\n",
      "   43. X63    | 入选Top50: 得分=0.4014, AUC=0.5267, VIF=1.8, |r|=0.0281, 冗余=否, 时序安全=是\n",
      "   44. X212   | 入选Top50: 得分=0.4019, AUC=0.5018, VIF=4.2, |r|=0.0348, 冗余=否, 时序安全=是\n",
      "   45. X93    | 入选Top50: 得分=0.4023, AUC=0.5295, VIF=4.9, |r|=0.0362, 冗余=否, 时序安全=是\n",
      "   46. X167   | 入选Top50: 得分=0.3550, AUC=0.4870, VIF=1.7, |r|=0.0387, 冗余=否, 时序安全=是\n",
      "   47. X129   | 入选Top50: 得分=0.3834, AUC=0.5297, VIF=1.3, |r|=0.0275, 冗余=否, 时序安全=是\n",
      "   48. X28    | 入选Top50: 得分=0.3627, AUC=0.4837, VIF=2.5, |r|=0.0391, 冗余=否, 时序安全=是\n",
      "   49. X287   | 入选Top50: 得分=0.3844, AUC=0.5085, VIF=4.6, |r|=0.0363, 冗余=否, 时序安全=是\n",
      "   50. X203   | 入选Top50: 得分=0.3746, AUC=0.5141, VIF=2.3, |r|=0.0301, 冗余=否, 时序安全=是\n",
      "\n",
      "[SUMMARY] 筛选统计:\n",
      "  第一轮剔除 (缺失>80%): 0\n",
      "  第二轮剔除 (冗余去重): 90\n",
      "  第三轮剔除 (排名截断): 160\n",
      "  最终入选: 50\n",
      "\n",
      "[OK] 筛选日志已保存: feature_selection_log.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 三轮筛选 + select_top50_features + 入选理由\n",
    "# =============================================================================\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "\n",
    "def compute_label_correlation(df: pd.DataFrame, features: list,\n",
    "                                target: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    计算每个特征与目标标签的绝对相关系数（点二列相关）。\n",
    "\n",
    "    Description:\n",
    "        对连续特征 X 与二分类标签 Y，使用点二列相关系数 (point-biserial r)\n",
    "        衡量线性关联强度。若标签非二分类或计算失败，回退为皮尔逊相关。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns)。\n",
    "        features : list of str\n",
    "            特征列名列表，长度 n_features。\n",
    "        target : str\n",
    "            目标标签列名。\n",
    "\n",
    "    Returns:\n",
    "        corr_series : pd.Series\n",
    "            长度为 n_features 的绝对相关系数序列，索引为特征名。\n",
    "    \"\"\"\n",
    "    corr_dict = {}\n",
    "    y = df[target].dropna()\n",
    "    is_binary = (y.nunique() == 2)\n",
    "\n",
    "    for feat in features:\n",
    "        try:\n",
    "            valid = df[[feat, target]].dropna()\n",
    "            if len(valid) < 20:\n",
    "                corr_dict[feat] = 0.0\n",
    "                continue\n",
    "            x_vals = valid[feat].values\n",
    "            y_vals = valid[target].values\n",
    "            if is_binary:\n",
    "                r, _ = pointbiserialr(y_vals, x_vals)\n",
    "            else:\n",
    "                r = np.corrcoef(x_vals, y_vals)[0, 1]\n",
    "            corr_dict[feat] = abs(float(r)) if np.isfinite(r) else 0.0\n",
    "        except Exception:\n",
    "            corr_dict[feat] = 0.0\n",
    "\n",
    "    return pd.Series(corr_dict)\n",
    "\n",
    "\n",
    "def select_top50_features(evaluation_report: pd.DataFrame,\n",
    "                            redundant_pairs: pd.DataFrame,\n",
    "                            label_corr: pd.Series,\n",
    "                            diag_df: pd.DataFrame,\n",
    "                            vif_series: pd.Series,\n",
    "                            top_n: int = 50,\n",
    "                            missing_threshold: float = 80.0) -> tuple:\n",
    "    \"\"\"\n",
    "    三轮筛选从 300 个特征中选出恰好 Top50 特征。\n",
    "\n",
    "    Description:\n",
    "        策略设计原则: VIF 和标签相关性作为排名权重因子，而非硬截断阈值，\n",
    "        确保最终输出恰好 top_n 个特征。\n",
    "\n",
    "        第一轮 (质量筛选)：仅剔除真正不可用的特征:\n",
    "          - 缺失率 > missing_threshold (80%)\n",
    "\n",
    "        第二轮 (冗余去除)：对皮尔逊 |r| > 0.8 的冗余特征对，\n",
    "          保留综合排名得分更高的特征，剔除低分者。\n",
    "          综合排名得分 = composite_score 已包含冗余/共线性惩罚。\n",
    "\n",
    "        第三轮 (综合排名 Top N)：计算最终排名得分 final_score，\n",
    "          整合有效性得分、标签相关性、VIF惩罚三个维度:\n",
    "            final_score = composite_score_norm * 0.5\n",
    "                        + label_corr_norm * 0.3\n",
    "                        + vif_bonus * 0.2\n",
    "          按 final_score 降序排列，截取前 top_n 个特征。\n",
    "\n",
    "    Parameters:\n",
    "        evaluation_report : pd.DataFrame\n",
    "            形状为 (n_features, n_cols)，综合评估报告。\n",
    "        redundant_pairs : pd.DataFrame\n",
    "            形状为 (n_pairs, 3)，冗余特征对表。\n",
    "        label_corr : pd.Series\n",
    "            长度为 n_features 的特征-标签绝对相关系数。\n",
    "        diag_df : pd.DataFrame\n",
    "            形状为 (n_features, n_diag_cols)，特征诊断报告。\n",
    "        vif_series : pd.Series\n",
    "            长度为 n_features 的 VIF 值。\n",
    "        top_n : int\n",
    "            最终保留的特征数，标量整数，默认 50。\n",
    "        missing_threshold : float\n",
    "            缺失率阈值 (仅用于第一轮硬剔除)，标量浮点数，默认 80.0。\n",
    "\n",
    "    Returns:\n",
    "        top50 : list of str\n",
    "            长度恰好为 top_n 的特征名列表。\n",
    "        selection_log : pd.DataFrame\n",
    "            形状为 (n_features, n_log_cols)，包含每个特征的筛选状态:\n",
    "            feature, round1, round2, round3, final,\n",
    "            composite_score, AUC, VIF, label_corr, final_score, reason。\n",
    "    \"\"\"\n",
    "    all_features = evaluation_report['feature'].tolist()\n",
    "    report = evaluation_report.copy()\n",
    "\n",
    "    # ---- 附加信息列 ----\n",
    "    report['label_corr'] = report['feature'].map(label_corr).fillna(0.0)\n",
    "\n",
    "    missing_map = {}\n",
    "    if '特征名' in diag_df.columns and '缺失率(%)' in diag_df.columns:\n",
    "        missing_map = dict(zip(diag_df['特征名'], diag_df['缺失率(%)']))\n",
    "    report['missing_pct'] = report['feature'].map(missing_map).fillna(0.0)\n",
    "\n",
    "    report['VIF_raw'] = report['feature'].map(vif_series).fillna(1.0)\n",
    "\n",
    "    # =====================================================================\n",
    "    # 第一轮：质量筛选 — 仅剔除缺失率过高的特征\n",
    "    # =====================================================================\n",
    "    print(\"  [ROUND 1] 质量筛选 (仅剔除缺失率>80%)\")\n",
    "    r1_dropped = set(\n",
    "        report.loc[report['missing_pct'] > missing_threshold, 'feature'])\n",
    "\n",
    "    print(f\"    缺失率>{missing_threshold}%: 剔除 {len(r1_dropped)} 个\")\n",
    "    survived_r1 = [f for f in all_features if f not in r1_dropped]\n",
    "    print(f\"    第一轮存活: {len(survived_r1)} 个\")\n",
    "\n",
    "    # =====================================================================\n",
    "    # 第二轮：冗余去除 — 对 |r|>0.8 的对保留得分更高者\n",
    "    # =====================================================================\n",
    "    print(\"\\n  [ROUND 2] 冗余去除 (|r|>0.8 特征对保留高分者)\")\n",
    "    r2_dropped = set()\n",
    "\n",
    "    if not redundant_pairs.empty:\n",
    "        score_map = dict(zip(report['feature'], report['composite_score']))\n",
    "\n",
    "        for _, pair in redundant_pairs.iterrows():\n",
    "            fa = pair['feature_a']\n",
    "            fb = pair['feature_b']\n",
    "            if fa not in survived_r1 or fb not in survived_r1:\n",
    "                continue\n",
    "            if fa in r2_dropped or fb in r2_dropped:\n",
    "                continue\n",
    "            sa = score_map.get(fa, 0)\n",
    "            sb = score_map.get(fb, 0)\n",
    "            if sa >= sb:\n",
    "                r2_dropped.add(fb)\n",
    "            else:\n",
    "                r2_dropped.add(fa)\n",
    "\n",
    "    print(f\"    冗余对中剔除: {len(r2_dropped)} 个\")\n",
    "    survived_r2 = [f for f in survived_r1 if f not in r2_dropped]\n",
    "    print(f\"    第二轮存活: {len(survived_r2)} 个\")\n",
    "\n",
    "    # =====================================================================\n",
    "    # 第三轮：综合排名 Top N\n",
    "    #   final_score = composite_score_norm * 0.5\n",
    "    #               + label_corr_norm * 0.3\n",
    "    #               + vif_bonus * 0.2\n",
    "    #   其中 vif_bonus = 1 - min(VIF / max_vif, 1)  (VIF 越低越好)\n",
    "    # =====================================================================\n",
    "    print(f\"\\n  [ROUND 3] 综合排名取 Top {top_n}\")\n",
    "    survived_report = report[report['feature'].isin(survived_r2)].copy()\n",
    "\n",
    "    # composite_score 归一化到 [0, 1]\n",
    "    cs_min = survived_report['composite_score'].min()\n",
    "    cs_max = survived_report['composite_score'].max()\n",
    "    if cs_max > cs_min:\n",
    "        survived_report['cs_norm'] = (\n",
    "            (survived_report['composite_score'] - cs_min) / (cs_max - cs_min))\n",
    "    else:\n",
    "        survived_report['cs_norm'] = 0.5\n",
    "\n",
    "    # label_corr 归一化到 [0, 1]\n",
    "    lc_min = survived_report['label_corr'].min()\n",
    "    lc_max = survived_report['label_corr'].max()\n",
    "    if lc_max > lc_min:\n",
    "        survived_report['lc_norm'] = (\n",
    "            (survived_report['label_corr'] - lc_min) / (lc_max - lc_min))\n",
    "    else:\n",
    "        survived_report['lc_norm'] = 0.5\n",
    "\n",
    "    # VIF bonus: VIF 越低越好 (用对数缩放避免极端值主导)\n",
    "    log_vif = np.log1p(survived_report['VIF_raw'].values)\n",
    "    max_log_vif = log_vif.max() if log_vif.max() > 0 else 1.0\n",
    "    survived_report['vif_bonus'] = 1.0 - np.minimum(log_vif / max_log_vif, 1.0)\n",
    "\n",
    "    # 综合得分\n",
    "    survived_report['final_score'] = (\n",
    "        survived_report['cs_norm'] * 0.5 +\n",
    "        survived_report['lc_norm'] * 0.3 +\n",
    "        survived_report['vif_bonus'] * 0.2\n",
    "    ).round(4)\n",
    "\n",
    "    survived_report = survived_report.sort_values(\n",
    "        'final_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    top50_df = survived_report.head(top_n)\n",
    "    top50 = top50_df['feature'].tolist()\n",
    "    print(f\"    最终选取: {len(top50)} 个特征\")\n",
    "\n",
    "    # 将 final_score 写回 report\n",
    "    final_score_map = dict(\n",
    "        zip(survived_report['feature'], survived_report['final_score']))\n",
    "\n",
    "    # =====================================================================\n",
    "    # 各维度统计\n",
    "    # =====================================================================\n",
    "    top50_set = set(top50)\n",
    "    top50_info = survived_report[survived_report['feature'].isin(top50_set)]\n",
    "    n_low_vif = int((top50_info['VIF_raw'] <= 10).sum())\n",
    "    n_high_corr = int((top50_info['label_corr'] >= 0.05).sum())\n",
    "    n_redundant_in = int(top50_info['is_redundant'].sum()) if 'is_redundant' in top50_info.columns else 0\n",
    "\n",
    "    print(f\"\\n  [INFO] Top {top_n} 特征质量概览:\")\n",
    "    print(f\"    VIF<=10 (低共线性): {n_low_vif} / {len(top50)}\")\n",
    "    print(f\"    标签|r|>=0.05: {n_high_corr} / {len(top50)}\")\n",
    "    print(f\"    含冗余标记: {n_redundant_in} / {len(top50)}\")\n",
    "\n",
    "    # =====================================================================\n",
    "    # 构建筛选日志\n",
    "    # =====================================================================\n",
    "    log_records = []\n",
    "    for feat in all_features:\n",
    "        row_mask = report['feature'] == feat\n",
    "        rec = {\n",
    "            'feature': feat,\n",
    "            'composite_score': float(report.loc[row_mask, 'composite_score'].values[0]),\n",
    "            'AUC': float(report.loc[row_mask, 'AUC'].values[0]),\n",
    "            'VIF': float(report.loc[row_mask, 'VIF_raw'].values[0]),\n",
    "            'label_corr': float(report.loc[row_mask, 'label_corr'].values[0]),\n",
    "            'missing_pct': float(report.loc[row_mask, 'missing_pct'].values[0]),\n",
    "            'final_score': float(final_score_map.get(feat, 0.0)),\n",
    "        }\n",
    "\n",
    "        is_red = bool(report.loc[row_mask, 'is_redundant'].values[0]) if 'is_redundant' in report.columns else False\n",
    "\n",
    "        if feat in r1_dropped:\n",
    "            rec['round1'] = 'DROP'\n",
    "            rec['round2'] = '-'\n",
    "            rec['round3'] = '-'\n",
    "            rec['final'] = 'DROP'\n",
    "            rec['reason'] = f\"第一轮剔除: 缺失率{rec['missing_pct']:.1f}%>{missing_threshold}%\"\n",
    "        elif feat in r2_dropped:\n",
    "            rec['round1'] = 'PASS'\n",
    "            rec['round2'] = 'DROP'\n",
    "            rec['round3'] = '-'\n",
    "            rec['final'] = 'DROP'\n",
    "            rec['reason'] = '第二轮剔除: 存在更高分的冗余特征'\n",
    "        elif feat in top50_set:\n",
    "            rec['round1'] = 'PASS'\n",
    "            rec['round2'] = 'PASS'\n",
    "            rec['round3'] = 'PASS'\n",
    "            rec['final'] = 'SELECTED'\n",
    "            rec['reason'] = (\n",
    "                f\"入选Top{top_n}: \"\n",
    "                f\"得分={rec['composite_score']:.4f}, \"\n",
    "                f\"AUC={rec['AUC']:.4f}, \"\n",
    "                f\"VIF={rec['VIF']:.1f}, \"\n",
    "                f\"|r|={rec['label_corr']:.4f}, \"\n",
    "                f\"冗余={'是' if is_red else '否'}, \"\n",
    "                f\"时序安全=是\"\n",
    "            )\n",
    "        else:\n",
    "            rec['round1'] = 'PASS'\n",
    "            rec['round2'] = 'PASS'\n",
    "            rec['round3'] = 'DROP'\n",
    "            rec['final'] = 'DROP'\n",
    "            rec['reason'] = (\n",
    "                f\"第三轮排名截断: final_score={rec['final_score']:.4f}\"\n",
    "                f\"未进Top{top_n}\")\n",
    "\n",
    "        log_records.append(rec)\n",
    "\n",
    "    selection_log = pd.DataFrame(log_records)\n",
    "\n",
    "    return top50, selection_log\n",
    "\n",
    "\n",
    "# ---- 执行 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 5.1: 三轮筛选 + Top50 特征列表\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# 1) 计算特征与标签的相关性\n",
    "print(\"  计算特征-标签相关系数...\")\n",
    "label_corr = compute_label_correlation(df_cleaned, active_features,\n",
    "                                         TARGET_LABEL)\n",
    "print(f\"  相关系数 > 0.05 的特征: {int((label_corr >= 0.05).sum())} / \"\n",
    "      f\"{len(active_features)}\")\n",
    "print(f\"  相关系数中位数: {label_corr.median():.4f}, \"\n",
    "      f\"最大值: {label_corr.max():.4f}\\n\")\n",
    "\n",
    "# 2) 三轮筛选 — 保证输出恰好 50 个特征\n",
    "top50_features, selection_log = select_top50_features(\n",
    "    evaluation_report=evaluation_report,\n",
    "    redundant_pairs=redundant_pairs,\n",
    "    label_corr=label_corr,\n",
    "    diag_df=diag_df,\n",
    "    vif_series=vif_series,\n",
    "    top_n=50,\n",
    "    missing_threshold=80.0\n",
    ")\n",
    "\n",
    "# 3) 输出 Top50 入选理由\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Top 50 特征列表及入选理由\")\n",
    "print(f\"{'='*60}\")\n",
    "top50_log = selection_log[selection_log['final'] == 'SELECTED'].copy()\n",
    "top50_log = top50_log.sort_values('final_score', ascending=False)\n",
    "for rank, (_, row) in enumerate(top50_log.iterrows(), 1):\n",
    "    print(f\"  {rank:3d}. {row['feature']:6s} | {row['reason']}\")\n",
    "\n",
    "# 4) 各轮统计\n",
    "print(f\"\\n[SUMMARY] 筛选统计:\")\n",
    "r1_count = int((selection_log['round1'] == 'DROP').sum())\n",
    "r2_count = int((selection_log['round2'] == 'DROP').sum())\n",
    "r3_count = int((selection_log['round3'] == 'DROP').sum())\n",
    "sel_count = int((selection_log['final'] == 'SELECTED').sum())\n",
    "print(f\"  第一轮剔除 (缺失>80%): {r1_count}\")\n",
    "print(f\"  第二轮剔除 (冗余去重): {r2_count}\")\n",
    "print(f\"  第三轮剔除 (排名截断): {r3_count}\")\n",
    "print(f\"  最终入选: {sel_count}\")\n",
    "\n",
    "# 保存\n",
    "selection_log.to_csv('feature_selection_log.csv', index=False,\n",
    "                       encoding='utf-8-sig')\n",
    "print(f\"\\n[OK] 筛选日志已保存: feature_selection_log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd34f72",
   "metadata": {},
   "source": [
    "### 5.2 模型验证\n",
    "使用千问Agent选择合适的分类模型，用Top50特征训练分类模型，严格按时间划分训练/测试集，计算AUC/Precision/Recall/F1，验证无数据泄漏。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7f8f613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 5.2: 模型验证\n",
      "============================================================\n",
      "\n",
      "  [AGENT] 正在请求千问Agent选择分类模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-02-28 17:54:58,284] INFO: HTTP Request: POST https://api.siliconflow.cn/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Agent 推荐模型: ['LogisticRegression']\n",
      "       理由: 推荐理由：在金融时序数据分类任务中，Logistic回归模型能够快速训练并且提供较高的可解释性，适合初步筛选特征和生成基准模型。XGBoost则是当前在排序问题和分类问题上表现优异的模型，且已经针对高效性和准确性进行了优化。\n",
      "\n",
      "  推荐模型: ['LogisticRegression']\n",
      "  超参数配置:\n",
      "    LogisticRegression: {'max_iter': 1000, 'C': 1.0}\n",
      "\n",
      "  使用 Top50 特征 (50 个) 训练分类模型...\n",
      "\n",
      "    训练 LogisticRegression (训练集: 61,923, 测试集: 19,123)...\n",
      "\n",
      "    [LogisticRegression] 结果:\n",
      "      AUC       = 0.5689\n",
      "      Accuracy  = 0.6999\n",
      "      Precision = 0.3984\n",
      "      Recall    = 0.3359\n",
      "      F1        = 0.2835\n",
      "      泄漏检查  = PASS (训练截止: 2019-10-22 00:00:00, 测试起始: 2019-10-23 00:00:00)\n",
      "[OK] 模型对比图已保存: images\\model_comparison.png\n",
      "\n",
      "[OK] 最优模型: LogisticRegression (AUC=0.5689)\n",
      "[OK] 模型验证结果已保存: model_validation_results.csv\n",
      "\n",
      "============================================================\n",
      "[FINAL] 自动特征工程系统完成\n",
      "============================================================\n",
      "  原始特征: 300\n",
      "  清理后有效: 300\n",
      "  最终筛选: 50 个 Top50 特征\n",
      "  目标标签: Y1\n",
      "  最优模型: LogisticRegression\n",
      "  最优 AUC: 0.5689\n",
      "  数据泄漏: 全部 PASS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 千问Agent选择分类模型 + Top50模型训练验证\n",
    "# =============================================================================\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (roc_auc_score, accuracy_score,\n",
    "                             precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix)\n",
    "\n",
    "\n",
    "def ask_agent_model_selection(top50_features: list,\n",
    "                                target_label: str,\n",
    "                                n_train: int,\n",
    "                                n_test: int) -> dict:\n",
    "    \"\"\"\n",
    "    调用千问Agent选择适合Top50特征的分类模型。\n",
    "\n",
    "    Description:\n",
    "        向 Agent 描述数据规模、特征数量、标签分布等信息，\n",
    "        由 Agent 从候选模型中推荐 1~2 个最适合的分类器及其超参数。\n",
    "        若 API 失败则使用默认配置 (LogisticRegression + GradientBoosting)。\n",
    "\n",
    "    Parameters:\n",
    "        top50_features : list of str\n",
    "            Top50 特征名列表。\n",
    "        target_label : str\n",
    "            目标标签名。\n",
    "        n_train : int\n",
    "            训练集样本数。\n",
    "        n_test : int\n",
    "            测试集样本数。\n",
    "\n",
    "    Returns:\n",
    "        model_config : dict\n",
    "            包含:\n",
    "            - 'models': list of str, 推荐的模型名称列表\n",
    "            - 'reason': str, 推荐理由\n",
    "            - 'hyperparams': dict, 各模型建议的主要超参数\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"我已经从300个金融时序特征中筛选出Top50特征，需要训练分类模型进行验证。\n",
    "情况如下：\n",
    "- 特征数: {len(top50_features)}\n",
    "- 目标标签: {target_label}\n",
    "- 训练集: {n_train:,} 条（按时间前80%划分）\n",
    "- 测试集: {n_test:,} 条（按时间后20%划分）\n",
    "- 任务类型: 金融时序数据分类（预测涨跌方向）\n",
    "\n",
    "候选模型:\n",
    "1. LogisticRegression — 线性模型，可解释性强，快速\n",
    "2. RandomForest — 集成方法，抗过拟合\n",
    "3. GradientBoosting — 梯度提升，精度较高但易过拟合\n",
    "4. XGBoost — 极端梯度提升（需额外安装）\n",
    "\n",
    "请从候选模型中推荐1~2个最适合的模型，并给出主要超参数建议。\n",
    "严格按如下JSON格式回复:\n",
    "{{\n",
    "  \"models\": [\"LogisticRegression\", \"GradientBoosting\"],\n",
    "  \"reason\": \"推荐理由\",\n",
    "  \"hyperparams\": {{\n",
    "    \"LogisticRegression\": {{\"max_iter\": 1000, \"C\": 1.0}},\n",
    "    \"GradientBoosting\": {{\"n_estimators\": 200, \"max_depth\": 4, \"learning_rate\": 0.05}}\n",
    "  }}\n",
    "}}\"\"\"\n",
    "\n",
    "    system_prompt = (\"你是金融机器学习专家Agent。\"\n",
    "                     \"请根据数据特点选择最优模型方案，严格按JSON格式回复。\")\n",
    "\n",
    "    print(\"  [AGENT] 正在请求千问Agent选择分类模型...\")\n",
    "    response = call_qwen_agent(prompt, system_prompt)\n",
    "\n",
    "    # 默认配置\n",
    "    config = {\n",
    "        \"models\": [\"LogisticRegression\", \"GradientBoosting\"],\n",
    "        \"reason\": \"默认选择: LR可解释性强+GBDT精度较高\",\n",
    "        \"hyperparams\": {\n",
    "            \"LogisticRegression\": {\"max_iter\": 1000, \"C\": 1.0},\n",
    "            \"GradientBoosting\": {\"n_estimators\": 200, \"max_depth\": 4,\n",
    "                                  \"learning_rate\": 0.05}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if response:\n",
    "        try:\n",
    "            match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if match:\n",
    "                parsed = json.loads(match.group())\n",
    "                valid_models = ['LogisticRegression', 'RandomForest',\n",
    "                                'GradientBoosting']\n",
    "                if 'models' in parsed:\n",
    "                    ms = [m for m in parsed['models'] if m in valid_models]\n",
    "                    if ms:\n",
    "                        config['models'] = ms\n",
    "                if 'reason' in parsed:\n",
    "                    config['reason'] = str(parsed['reason'])\n",
    "                if 'hyperparams' in parsed:\n",
    "                    config['hyperparams'] = parsed['hyperparams']\n",
    "            print(f\"  [OK] Agent 推荐模型: {config['models']}\")\n",
    "            print(f\"       理由: {config['reason']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARN] Agent 返回解析失败: {e}, 使用默认配置\")\n",
    "    else:\n",
    "        print(\"  [WARN] Agent 未返回有效响应, 使用默认配置\")\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def build_model(model_name: str, hyperparams: dict):\n",
    "    \"\"\"\n",
    "    根据模型名称和超参数实例化 sklearn 分类器。\n",
    "\n",
    "    Description:\n",
    "        支持 LogisticRegression, RandomForest, GradientBoosting 三种模型。\n",
    "        超参数通过字典传入，未指定的参数使用默认值。\n",
    "\n",
    "    Parameters:\n",
    "        model_name : str\n",
    "            模型名称，标量字符串。\n",
    "        hyperparams : dict\n",
    "            超参数字典。\n",
    "\n",
    "    Returns:\n",
    "        model : sklearn estimator\n",
    "            实例化后的分类器对象。\n",
    "    \"\"\"\n",
    "    if model_name == 'LogisticRegression':\n",
    "        params = {'max_iter': 1000, 'solver': 'lbfgs', 'random_state': 42}\n",
    "        params.update({k: v for k, v in hyperparams.items()\n",
    "                       if k in ['max_iter', 'C', 'solver', 'penalty']})\n",
    "        return LogisticRegression(**params)\n",
    "    elif model_name == 'RandomForest':\n",
    "        params = {'n_estimators': 200, 'max_depth': 6, 'random_state': 42,\n",
    "                  'n_jobs': -1}\n",
    "        params.update({k: v for k, v in hyperparams.items()\n",
    "                       if k in ['n_estimators', 'max_depth',\n",
    "                                'min_samples_split', 'min_samples_leaf']})\n",
    "        return RandomForestClassifier(**params)\n",
    "    elif model_name == 'GradientBoosting':\n",
    "        params = {'n_estimators': 200, 'max_depth': 4,\n",
    "                  'learning_rate': 0.05, 'random_state': 42,\n",
    "                  'subsample': 0.8}\n",
    "        params.update({k: v for k, v in hyperparams.items()\n",
    "                       if k in ['n_estimators', 'max_depth',\n",
    "                                'learning_rate', 'subsample']})\n",
    "        return GradientBoostingClassifier(**params)\n",
    "    else:\n",
    "        return LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "\n",
    "def train_and_evaluate(df: pd.DataFrame, features: list,\n",
    "                         target: str, train_idx, test_idx,\n",
    "                         model_name: str, model) -> dict:\n",
    "    \"\"\"\n",
    "    用指定模型和特征集在时间划分的训练/测试集上训练并评估。\n",
    "\n",
    "    Description:\n",
    "        1. 提取 Top50 特征和标签，去除缺失值行。\n",
    "        2. 标准化特征。\n",
    "        3. 训练模型，在测试集上预测。\n",
    "        4. 计算 AUC, accuracy, precision, recall, f1 五项指标。\n",
    "        5. 检查数据泄漏: 验证训练集最大日期 < 测试集最小日期。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns)。\n",
    "        features : list of str\n",
    "            特征列名列表，长度一般为 50。\n",
    "        target : str\n",
    "            目标标签列名。\n",
    "        train_idx : pd.Index\n",
    "            训练集行索引。\n",
    "        test_idx : pd.Index\n",
    "            测试集行索引。\n",
    "        model_name : str\n",
    "            模型名称，用于日志输出。\n",
    "        model : sklearn estimator\n",
    "            实例化后的分类器。\n",
    "\n",
    "    Returns:\n",
    "        result : dict\n",
    "            包含 model_name, AUC, accuracy, precision, recall, f1,\n",
    "            n_train, n_test, leakage_check 等键。\n",
    "    \"\"\"\n",
    "    # 准备数据\n",
    "    X_train = df.loc[train_idx, features].values\n",
    "    y_train = df.loc[train_idx, target].values\n",
    "    X_test = df.loc[test_idx, features].values\n",
    "    y_test = df.loc[test_idx, target].values\n",
    "\n",
    "    # 去除含 NaN 的行\n",
    "    valid_tr = ~(np.isnan(X_train).any(axis=1) | np.isnan(y_train))\n",
    "    valid_te = ~(np.isnan(X_test).any(axis=1) | np.isnan(y_test))\n",
    "    X_train, y_train = X_train[valid_tr], y_train[valid_tr]\n",
    "    X_test, y_test = X_test[valid_te], y_test[valid_te]\n",
    "\n",
    "    # 标准化\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    # 训练\n",
    "    print(f\"    训练 {model_name} (训练集: {len(X_train):,}, \"\n",
    "          f\"测试集: {len(X_test):,})...\")\n",
    "    model.fit(X_train_s, y_train)\n",
    "\n",
    "    # 预测\n",
    "    y_pred = model.predict(X_test_s)\n",
    "    y_proba = model.predict_proba(X_test_s)\n",
    "\n",
    "    # 指标计算\n",
    "    result = {'model_name': model_name}\n",
    "    n_classes = len(model.classes_)\n",
    "    if n_classes == 2:\n",
    "        result['AUC'] = float(roc_auc_score(y_test, y_proba[:, 1]))\n",
    "    else:\n",
    "        try:\n",
    "            result['AUC'] = float(roc_auc_score(\n",
    "                y_test, y_proba, multi_class='ovr', average='macro'))\n",
    "        except Exception:\n",
    "            result['AUC'] = 0.5\n",
    "\n",
    "    result['accuracy'] = float(accuracy_score(y_test, y_pred))\n",
    "    result['precision'] = float(precision_score(\n",
    "        y_test, y_pred, average='macro', zero_division=0))\n",
    "    result['recall'] = float(recall_score(\n",
    "        y_test, y_pred, average='macro', zero_division=0))\n",
    "    result['f1'] = float(f1_score(\n",
    "        y_test, y_pred, average='macro', zero_division=0))\n",
    "    result['n_train'] = len(X_train)\n",
    "    result['n_test'] = len(X_test)\n",
    "\n",
    "    # 数据泄漏检查\n",
    "    train_max_date = df.loc[train_idx, 'trade_date'].max()\n",
    "    test_min_date = df.loc[test_idx, 'trade_date'].min()\n",
    "    leakage_ok = (train_max_date < test_min_date)\n",
    "    result['leakage_check'] = 'PASS' if leakage_ok else 'FAIL'\n",
    "    result['train_max_date'] = str(train_max_date)\n",
    "    result['test_min_date'] = str(test_min_date)\n",
    "\n",
    "    return result, model\n",
    "\n",
    "\n",
    "def plot_model_comparison(results: list, img_dir: str = 'images') -> None:\n",
    "    \"\"\"\n",
    "    绘制多模型性能对比雷达图和柱状图。\n",
    "\n",
    "    Description:\n",
    "        图1: 各模型指标柱状对比图（AUC/Precision/Recall/F1）\n",
    "        图2: 混淆矩阵概要（仅文字展示区间）\n",
    "\n",
    "    Parameters:\n",
    "        results : list of dict\n",
    "            各模型评估结果列表。\n",
    "        img_dir : str\n",
    "            图片保存目录，默认 'images'。\n",
    "    \"\"\"\n",
    "    metrics = ['AUC', 'accuracy', 'precision', 'recall', 'f1']\n",
    "    model_names = [r['model_name'] for r in results]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.8 / len(results)\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "    for i, res in enumerate(results):\n",
    "        vals = [res.get(m, 0) for m in metrics]\n",
    "        bars = ax.bar(x + i * width, vals, width,\n",
    "                       label=res['model_name'], color=colors[i % len(colors)],\n",
    "                       edgecolor='white')\n",
    "        for bar, val in zip(bars, vals):\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2., bar.get_height(),\n",
    "                    f'{val:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    ax.set_xlabel('评价指标')\n",
    "    ax.set_ylabel('得分')\n",
    "    ax.set_title('Top50 特征 — 分类模型性能对比')\n",
    "    ax.set_xticks(x + width * (len(results) - 1) / 2)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(img_dir, 'model_comparison.png')\n",
    "    plt.savefig(fig_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"[OK] 模型对比图已保存: {fig_path}\")\n",
    "\n",
    "\n",
    "# ---- 执行 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 5.2: 模型验证\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# 1) Agent 选择模型\n",
    "model_config = ask_agent_model_selection(\n",
    "    top50_features, TARGET_LABEL,\n",
    "    n_train=len(train_idx), n_test=len(test_idx))\n",
    "\n",
    "print(f\"\\n  推荐模型: {model_config['models']}\")\n",
    "print(f\"  超参数配置:\")\n",
    "for mname, hp in model_config['hyperparams'].items():\n",
    "    if mname in model_config['models']:\n",
    "        print(f\"    {mname}: {hp}\")\n",
    "\n",
    "# 2) 逐模型训练与评估\n",
    "print(f\"\\n  使用 Top50 特征 ({len(top50_features)} 个) 训练分类模型...\\n\")\n",
    "all_results = []\n",
    "trained_models = {}\n",
    "\n",
    "for mname in model_config['models']:\n",
    "    hp = model_config['hyperparams'].get(mname, {})\n",
    "    model = build_model(mname, hp)\n",
    "    result, fitted_model = train_and_evaluate(\n",
    "        df_cleaned, top50_features, TARGET_LABEL,\n",
    "        train_idx, test_idx, mname, model)\n",
    "    all_results.append(result)\n",
    "    trained_models[mname] = fitted_model\n",
    "\n",
    "    print(f\"\\n    [{mname}] 结果:\")\n",
    "    print(f\"      AUC       = {result['AUC']:.4f}\")\n",
    "    print(f\"      Accuracy  = {result['accuracy']:.4f}\")\n",
    "    print(f\"      Precision = {result['precision']:.4f}\")\n",
    "    print(f\"      Recall    = {result['recall']:.4f}\")\n",
    "    print(f\"      F1        = {result['f1']:.4f}\")\n",
    "    print(f\"      泄漏检查  = {result['leakage_check']} \"\n",
    "          f\"(训练截止: {result['train_max_date']}, \"\n",
    "          f\"测试起始: {result['test_min_date']})\")\n",
    "\n",
    "# 3) 模型对比可视化\n",
    "plot_model_comparison(all_results, IMG_DIR)\n",
    "\n",
    "# 4) 最优模型\n",
    "best = max(all_results, key=lambda x: x['AUC'])\n",
    "print(f\"\\n[OK] 最优模型: {best['model_name']} (AUC={best['AUC']:.4f})\")\n",
    "\n",
    "# 5) 保存结果\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv('model_validation_results.csv', index=False,\n",
    "                    encoding='utf-8-sig')\n",
    "print(f\"[OK] 模型验证结果已保存: model_validation_results.csv\")\n",
    "\n",
    "# 6) 汇总输出\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"[FINAL] 自动特征工程系统完成\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  原始特征: {len(FEATURE_COLS)}\")\n",
    "print(f\"  清理后有效: {len(active_features)}\")\n",
    "print(f\"  最终筛选: {len(top50_features)} 个 Top50 特征\")\n",
    "print(f\"  目标标签: {TARGET_LABEL}\")\n",
    "print(f\"  最优模型: {best['model_name']}\")\n",
    "print(f\"  最优 AUC: {best['AUC']:.4f}\")\n",
    "print(f\"  数据泄漏: 全部 {'PASS' if all(r['leakage_check']=='PASS' for r in all_results) else 'FAIL'}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e1844",
   "metadata": {},
   "source": [
    "## 步骤6：输出可视化与完整报告\n",
    "- 6.1 综合可视化：数据概况（缺失热力图/时序折线图）、特征处理（清理前后对比/异常值箱型图）、特征评估（重要性条形图/冗余热力图）\n",
    "- 6.2 模型评估可视化：混淆矩阵、ROC曲线、Precision-Recall曲线\n",
    "- 6.3 数据泄漏检测报告：汇总步骤1~5中所有泄漏检查结果\n",
    "- 6.4 完整结构化报告汇总：步骤1~6全过程决策、结果、关键输出索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c4f52",
   "metadata": {},
   "source": [
    "### 6.1 综合可视化\n",
    "数据概况可视化（缺失值热力图、时序折线图）、特征清理前后对比（分布对比图、异常值箱型图）、特征评估可视化（Top50重要性条形图、冗余特征相关性热力图）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "578e5d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 6.1: 综合可视化\n",
      "============================================================\n",
      "\n",
      "[OK] 数据概况可视化已保存: images\\step6_data_overview.png\n",
      "[OK] 分布对比图已保存: images\\step6_distribution_compare.png\n",
      "[OK] 异常值箱型图已保存: images\\step6_outlier_boxplot.png\n",
      "[OK] 特征评估可视化已保存: images\\step6_feature_evaluation.png\n",
      "\n",
      "[OK] 步骤 6.1 综合可视化完成\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 6.1 综合可视化 — 数据概况 + 特征处理 + 特征评估\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def plot_data_overview(df: pd.DataFrame, df_cleaned: pd.DataFrame,\n",
    "                         top50: list, img_dir: str = 'images') -> None:\n",
    "    \"\"\"\n",
    "    绘制数据概况可视化：缺失值热力图 + 收盘价时序折线图。\n",
    "\n",
    "    Description:\n",
    "        图1: Top50 特征的缺失值热力图 (原始数据)\n",
    "        图2: 收盘价按日期的时序折线图，标注训练/测试分界线\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据。\n",
    "        df_cleaned : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的清理后数据。\n",
    "        top50 : list of str\n",
    "            Top50 特征名列表。\n",
    "        img_dir : str\n",
    "            图片保存目录，默认 'images'。\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 7))\n",
    "\n",
    "    # 图1: 缺失值热力图 (原始数据, Top50 特征)\n",
    "    ax1 = axes[0]\n",
    "    missing_matrix = df[top50].isnull().astype(int)\n",
    "    # 采样展示避免过大\n",
    "    if len(missing_matrix) > 2000:\n",
    "        sample_idx = np.linspace(0, len(missing_matrix)-1, 2000, dtype=int)\n",
    "        missing_matrix = missing_matrix.iloc[sample_idx]\n",
    "    sns.heatmap(missing_matrix.T, cbar=False, cmap='YlOrRd',\n",
    "                yticklabels=True, ax=ax1)\n",
    "    ax1.set_title('Top50 特征缺失值热力图 (原始数据)')\n",
    "    ax1.set_xlabel('样本索引 (采样)')\n",
    "    ax1.set_ylabel('特征')\n",
    "    ax1.tick_params(axis='y', labelsize=6)\n",
    "\n",
    "    # 图2: 收盘价时序折线图\n",
    "    ax2 = axes[1]\n",
    "    if 'trade_date' in df.columns and 'close' in df.columns:\n",
    "        daily = df.groupby('trade_date')['close'].mean().sort_index()\n",
    "        ax2.plot(range(len(daily)), daily.values, color='steelblue',\n",
    "                 linewidth=0.8, alpha=0.8)\n",
    "        # 标注训练/测试分界线\n",
    "        split_pos = int(len(daily) * 0.8)\n",
    "        ax2.axvline(x=split_pos, color='red', linestyle='--',\n",
    "                    linewidth=1.5, label='训练/测试分界')\n",
    "        ax2.set_xlabel('交易日序号')\n",
    "        ax2.set_ylabel('收盘价 (日均)')\n",
    "        ax2.set_title('收盘价时序趋势 (训练/测试划分)')\n",
    "        ax2.legend(fontsize=9)\n",
    "        ax2.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(img_dir, 'step6_data_overview.png')\n",
    "    plt.savefig(fig_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"[OK] 数据概况可视化已保存: {fig_path}\")\n",
    "\n",
    "\n",
    "def plot_feature_cleaning_comparison(df: pd.DataFrame,\n",
    "                                       df_cleaned: pd.DataFrame,\n",
    "                                       top50: list,\n",
    "                                       img_dir: str = 'images') -> None:\n",
    "    \"\"\"\n",
    "    绘制特征清理前后对比：分布对比直方图 + 异常值箱型图。\n",
    "\n",
    "    Description:\n",
    "        图1: 选取 Top50 中前 6 个特征, 清理前后分布直方图对比\n",
    "        图2: 选取 Top50 中前 10 个特征, 清理前后箱型图对比\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据。\n",
    "        df_cleaned : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的清理后数据。\n",
    "        top50 : list of str\n",
    "            Top50 特征名列表。\n",
    "        img_dir : str\n",
    "            图片保存目录，默认 'images'。\n",
    "    \"\"\"\n",
    "    # 图1: 分布对比直方图 (前 6 个特征)\n",
    "    show_feats = [f for f in top50[:6] if f in df.columns and f in df_cleaned.columns]\n",
    "    if show_feats:\n",
    "        fig, axes = plt.subplots(2, len(show_feats), figsize=(4*len(show_feats), 8))\n",
    "        if len(show_feats) == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "        for j, feat in enumerate(show_feats):\n",
    "            # 清理前\n",
    "            ax_before = axes[0, j]\n",
    "            vals_before = df[feat].dropna()\n",
    "            ax_before.hist(vals_before, bins=50, color='#e74c3c', alpha=0.7,\n",
    "                           edgecolor='white')\n",
    "            ax_before.set_title(f'{feat} (清理前)', fontsize=9)\n",
    "            ax_before.set_ylabel('频数' if j == 0 else '')\n",
    "            ax_before.tick_params(labelsize=7)\n",
    "\n",
    "            # 清理后\n",
    "            ax_after = axes[1, j]\n",
    "            vals_after = df_cleaned[feat].dropna()\n",
    "            ax_after.hist(vals_after, bins=50, color='#2ecc71', alpha=0.7,\n",
    "                          edgecolor='white')\n",
    "            ax_after.set_title(f'{feat} (清理后)', fontsize=9)\n",
    "            ax_after.set_ylabel('频数' if j == 0 else '')\n",
    "            ax_after.tick_params(labelsize=7)\n",
    "\n",
    "        plt.suptitle('Top50 特征清理前后分布对比', fontsize=13, y=1.01)\n",
    "        plt.tight_layout()\n",
    "        fig_path = os.path.join(img_dir, 'step6_distribution_compare.png')\n",
    "        plt.savefig(fig_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"[OK] 分布对比图已保存: {fig_path}\")\n",
    "\n",
    "    # 图2: 异常值箱型图 (前 10 个特征)\n",
    "    show_box = [f for f in top50[:10] if f in df.columns and f in df_cleaned.columns]\n",
    "    if show_box:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "        # 清理前\n",
    "        ax1 = axes[0]\n",
    "        box_data_before = [df[f].dropna().values for f in show_box]\n",
    "        bp1 = ax1.boxplot(box_data_before, labels=show_box, patch_artist=True,\n",
    "                           showfliers=True)\n",
    "        for patch in bp1['boxes']:\n",
    "            patch.set_facecolor('#e74c3c')\n",
    "            patch.set_alpha(0.5)\n",
    "        ax1.set_title('清理前 — 异常值箱型图 (Top10 特征)')\n",
    "        ax1.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        # 清理后\n",
    "        ax2 = axes[1]\n",
    "        box_data_after = [df_cleaned[f].dropna().values for f in show_box]\n",
    "        bp2 = ax2.boxplot(box_data_after, labels=show_box, patch_artist=True,\n",
    "                           showfliers=True)\n",
    "        for patch in bp2['boxes']:\n",
    "            patch.set_facecolor('#2ecc71')\n",
    "            patch.set_alpha(0.5)\n",
    "        ax2.set_title('清理后 — 异常值箱型图 (Top10 特征)')\n",
    "        ax2.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig_path = os.path.join(img_dir, 'step6_outlier_boxplot.png')\n",
    "        plt.savefig(fig_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"[OK] 异常值箱型图已保存: {fig_path}\")\n",
    "\n",
    "\n",
    "def plot_feature_evaluation_viz(evaluation_report: pd.DataFrame,\n",
    "                                  top50: list,\n",
    "                                  df_cleaned: pd.DataFrame,\n",
    "                                  img_dir: str = 'images') -> None:\n",
    "    \"\"\"\n",
    "    绘制特征评估可视化：Top50重要性条形图 + 冗余特征相关性热力图。\n",
    "\n",
    "    Description:\n",
    "        图1: Top50 特征综合得分水平条形图（按 final_score 降序）\n",
    "        图2: Top50 特征间皮尔逊相关系数热力图\n",
    "\n",
    "    Parameters:\n",
    "        evaluation_report : pd.DataFrame\n",
    "            综合评估报告表。\n",
    "        top50 : list of str\n",
    "            Top50 特征名列表。\n",
    "        df_cleaned : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的清理后数据。\n",
    "        img_dir : str\n",
    "            图片保存目录，默认 'images'。\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "    # 图1: Top50 特征重要性条形图\n",
    "    ax1 = axes[0]\n",
    "    top50_report = evaluation_report[\n",
    "        evaluation_report['feature'].isin(top50)\n",
    "    ].copy()\n",
    "    top50_report = top50_report.sort_values('composite_score', ascending=True)\n",
    "    colors_bar = []\n",
    "    for _, r in top50_report.iterrows():\n",
    "        if r.get('is_redundant', False) and r.get('is_multicollinear', False):\n",
    "            colors_bar.append('#e74c3c')  # high risk\n",
    "        elif r.get('is_redundant', False) or r.get('is_multicollinear', False):\n",
    "            colors_bar.append('#f39c12')  # medium risk\n",
    "        else:\n",
    "            colors_bar.append('#2ecc71')  # low risk\n",
    "\n",
    "    ax1.barh(range(len(top50_report)), top50_report['composite_score'].values,\n",
    "             color=colors_bar, edgecolor='white', height=0.7)\n",
    "    ax1.set_yticks(range(len(top50_report)))\n",
    "    ax1.set_yticklabels(top50_report['feature'].values, fontsize=7)\n",
    "    ax1.set_xlabel('综合得分 (composite_score)')\n",
    "    ax1.set_title(f'Top50 特征重要性排名')\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_items = [\n",
    "        Patch(facecolor='#2ecc71', label='low risk'),\n",
    "        Patch(facecolor='#f39c12', label='medium risk'),\n",
    "        Patch(facecolor='#e74c3c', label='high risk')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_items, loc='lower right', fontsize=8)\n",
    "\n",
    "    # 图2: Top50 特征间相关性热力图\n",
    "    ax2 = axes[1]\n",
    "    corr_top50 = df_cleaned[top50].corr(method='pearson')\n",
    "    mask = np.triu(np.ones_like(corr_top50, dtype=bool), k=1)\n",
    "    sns.heatmap(corr_top50, mask=mask, cmap='RdBu_r', center=0,\n",
    "                vmin=-1, vmax=1, square=True, linewidths=0.3,\n",
    "                xticklabels=True, yticklabels=True, ax=ax2,\n",
    "                cbar_kws={'shrink': 0.6})\n",
    "    ax2.set_title('Top50 特征间相关性热力图')\n",
    "    ax2.tick_params(axis='both', labelsize=5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(img_dir, 'step6_feature_evaluation.png')\n",
    "    plt.savefig(fig_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"[OK] 特征评估可视化已保存: {fig_path}\")\n",
    "\n",
    "\n",
    "# ---- 执行 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 6.1: 综合可视化\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# 数据概况\n",
    "plot_data_overview(df, df_cleaned, top50_features, IMG_DIR)\n",
    "\n",
    "# 特征清理前后对比\n",
    "plot_feature_cleaning_comparison(df, df_cleaned, top50_features, IMG_DIR)\n",
    "\n",
    "# 特征评估\n",
    "plot_feature_evaluation_viz(evaluation_report, top50_features,\n",
    "                              df_cleaned, IMG_DIR)\n",
    "\n",
    "print(\"\\n[OK] 步骤 6.1 综合可视化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e260d",
   "metadata": {},
   "source": [
    "### 6.2 模型评估可视化\n",
    "\n",
    "- 混淆矩阵热力图\n",
    "- ROC 曲线 (含 AUC 值标注)\n",
    "- Precision-Recall 曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c61d1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 6.2: 模型评估可视化\n",
      "============================================================\n",
      "\n",
      "[OK] 重新训练完成 | n_classes=3 | AUC = 0.5671 | 测试集样本 = 19123\n",
      "[OK] 模型评估可视化已保存: images\\step6_model_evaluation.png\n",
      "\n",
      "--- 分类报告 ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0     0.7553    0.5625    0.6448     13423\n",
      "         0.0     0.1633    0.3415    0.2209      2662\n",
      "         1.0     0.1927    0.2258    0.2079      3038\n",
      "\n",
      "    accuracy                         0.4782     19123\n",
      "   macro avg     0.3704    0.3766    0.3579     19123\n",
      "weighted avg     0.5835    0.4782    0.5164     19123\n",
      "\n",
      "\n",
      "[OK] 步骤 6.2 模型评估可视化完成\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 6.2 模型评估可视化 — 混淆矩阵 / ROC / PR 曲线\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import (roc_curve, precision_recall_curve,\n",
    "                             average_precision_score, confusion_matrix,\n",
    "                             roc_auc_score, classification_report)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "def retrain_and_predict(df_cleaned: pd.DataFrame,\n",
    "                         top50: list,\n",
    "                         target: str,\n",
    "                         train_idx: np.ndarray,\n",
    "                         test_idx: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    使用 Top50 特征重新训练 LogisticRegression 并返回预测结果。\n",
    "\n",
    "    Description:\n",
    "        对清理后数据进行标准化 -> 训练 LR -> 收集 y_true / y_pred / y_proba。\n",
    "        自动检测二分类/多分类并使用对应 AUC 计算方式。\n",
    "\n",
    "    Parameters:\n",
    "        df_cleaned : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的清理后数据。\n",
    "        top50 : list of str\n",
    "            长度为 50 的特征名列表。\n",
    "        target : str\n",
    "            目标标签列名。\n",
    "        train_idx : np.ndarray\n",
    "            训练集行索引。\n",
    "        test_idx : np.ndarray\n",
    "            测试集行索引。\n",
    "\n",
    "    Returns:\n",
    "        dict : 包含 y_train, y_test, y_pred, y_proba, model, scaler,\n",
    "               auc, n_classes, classes 的字典。\n",
    "    \"\"\"\n",
    "    X_train = df_cleaned.loc[train_idx, top50].values\n",
    "    X_test = df_cleaned.loc[test_idx, top50].values\n",
    "    y_train = df_cleaned.loc[train_idx, target].values\n",
    "    y_test = df_cleaned.loc[test_idx, target].values\n",
    "\n",
    "    # 去除含 NaN 的行\n",
    "    valid_tr = ~(np.isnan(X_train).any(axis=1) | np.isnan(y_train))\n",
    "    valid_te = ~(np.isnan(X_test).any(axis=1) | np.isnan(y_test))\n",
    "    X_train, y_train = X_train[valid_tr], y_train[valid_tr]\n",
    "    X_test, y_test = X_test[valid_te], y_test[valid_te]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42,\n",
    "                               class_weight='balanced', solver='lbfgs')\n",
    "    model.fit(X_train_s, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_s)\n",
    "    y_proba = model.predict_proba(X_test_s)\n",
    "\n",
    "    n_classes = len(model.classes_)\n",
    "    if n_classes == 2:\n",
    "        auc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "    else:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, y_proba,\n",
    "                                multi_class='ovr', average='macro')\n",
    "        except Exception:\n",
    "            auc = 0.5\n",
    "\n",
    "    print(f\"[OK] 重新训练完成 | n_classes={n_classes} | AUC = {auc:.4f} \"\n",
    "          f\"| 测试集样本 = {len(y_test)}\")\n",
    "    return {\n",
    "        'y_train': y_train, 'y_test': y_test,\n",
    "        'y_pred': y_pred, 'y_proba': y_proba,\n",
    "        'model': model, 'scaler': scaler, 'auc': auc,\n",
    "        'n_classes': n_classes, 'classes': model.classes_,\n",
    "        'X_train_s': X_train_s, 'X_test_s': X_test_s\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix_heatmap(y_test, y_pred, classes, ax=None):\n",
    "    \"\"\"\n",
    "    绘制混淆矩阵热力图 (支持多分类)。\n",
    "\n",
    "    Parameters:\n",
    "        y_test : np.ndarray, 形状 (n_test,), 真实标签。\n",
    "        y_pred : np.ndarray, 形状 (n_test,), 预测标签。\n",
    "        classes : np.ndarray, 类别标签数组。\n",
    "        ax : matplotlib.axes.Axes or None, 绘图轴。\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=[f'{c}' for c in classes],\n",
    "                yticklabels=[f'{c}' for c in classes])\n",
    "    ax.set_title('混淆矩阵')\n",
    "    ax.set_xlabel('预测标签')\n",
    "    ax.set_ylabel('真实标签')\n",
    "\n",
    "\n",
    "def plot_roc_multiclass(y_test, y_proba, classes, auc_val, ax=None):\n",
    "    \"\"\"\n",
    "    绘制 ROC 曲线 (多分类: 每类 OVR + macro 均值)。\n",
    "\n",
    "    Parameters:\n",
    "        y_test : np.ndarray, 形状 (n_test,), 真实标签。\n",
    "        y_proba : np.ndarray, 形状 (n_test, n_classes), 各类概率。\n",
    "        classes : np.ndarray, 类别标签数组。\n",
    "        auc_val : float, macro-average AUC。\n",
    "        ax : matplotlib.axes.Axes or None, 绘图轴。\n",
    "    \"\"\"\n",
    "    y_bin = label_binarize(y_test, classes=classes)\n",
    "    n_classes = len(classes)\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, max(n_classes, 3)))\n",
    "\n",
    "    if n_classes == 2:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])\n",
    "        ax.plot(fpr, tpr, color=colors[0], linewidth=2,\n",
    "                label=f'ROC (AUC = {auc_val:.4f})')\n",
    "        ax.fill_between(fpr, tpr, alpha=0.1, color=colors[0])\n",
    "    else:\n",
    "        for i, cls in enumerate(classes):\n",
    "            if y_bin.ndim == 1:\n",
    "                y_i = (y_test == cls).astype(int)\n",
    "            else:\n",
    "                y_i = y_bin[:, i]\n",
    "            fpr_i, tpr_i, _ = roc_curve(y_i, y_proba[:, i])\n",
    "            try:\n",
    "                cls_auc = roc_auc_score(y_i, y_proba[:, i])\n",
    "            except Exception:\n",
    "                cls_auc = 0.5\n",
    "            ax.plot(fpr_i, tpr_i, color=colors[i % len(colors)],\n",
    "                    linewidth=1.5,\n",
    "                    label=f'Class {cls} (AUC={cls_auc:.3f})')\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', color='gray',\n",
    "            linewidth=1, alpha=0.7)\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'ROC 曲线 (macro AUC={auc_val:.4f})')\n",
    "    ax.legend(loc='lower right', fontsize=7)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "\n",
    "def plot_pr_multiclass(y_test, y_proba, classes, ax=None):\n",
    "    \"\"\"\n",
    "    绘制 Precision-Recall 曲线 (多分类: 每类 OVR)。\n",
    "\n",
    "    Parameters:\n",
    "        y_test : np.ndarray, 形状 (n_test,), 真实标签。\n",
    "        y_proba : np.ndarray, 形状 (n_test, n_classes), 各类概率。\n",
    "        classes : np.ndarray, 类别标签数组。\n",
    "        ax : matplotlib.axes.Axes or None, 绘图轴。\n",
    "    \"\"\"\n",
    "    y_bin = label_binarize(y_test, classes=classes)\n",
    "    n_classes = len(classes)\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, max(n_classes, 3)))\n",
    "\n",
    "    if n_classes == 2:\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_proba[:, 1])\n",
    "        ap = average_precision_score(y_test, y_proba[:, 1])\n",
    "        ax.plot(recall, precision, color=colors[0], linewidth=2,\n",
    "                label=f'PR (AP={ap:.4f})')\n",
    "        ax.fill_between(recall, precision, alpha=0.1, color=colors[0])\n",
    "    else:\n",
    "        for i, cls in enumerate(classes):\n",
    "            if y_bin.ndim == 1:\n",
    "                y_i = (y_test == cls).astype(int)\n",
    "            else:\n",
    "                y_i = y_bin[:, i]\n",
    "            prec_i, rec_i, _ = precision_recall_curve(y_i, y_proba[:, i])\n",
    "            try:\n",
    "                ap_i = average_precision_score(y_i, y_proba[:, i])\n",
    "            except Exception:\n",
    "                ap_i = 0.0\n",
    "            ax.plot(rec_i, prec_i, color=colors[i % len(colors)],\n",
    "                    linewidth=1.5,\n",
    "                    label=f'Class {cls} (AP={ap_i:.3f})')\n",
    "\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision-Recall 曲线')\n",
    "    ax.legend(loc='upper right', fontsize=7)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "\n",
    "# ---- 执行 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 6.2: 模型评估可视化\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# 重新训练获取预测结果\n",
    "pred_results = retrain_and_predict(df_cleaned, top50_features,\n",
    "                                     TARGET_LABEL, train_idx, test_idx)\n",
    "\n",
    "# 绘制三合一图\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "plot_confusion_matrix_heatmap(pred_results['y_test'], pred_results['y_pred'],\n",
    "                                pred_results['classes'], ax=axes[0])\n",
    "plot_roc_multiclass(pred_results['y_test'], pred_results['y_proba'],\n",
    "                      pred_results['classes'], pred_results['auc'],\n",
    "                      ax=axes[1])\n",
    "plot_pr_multiclass(pred_results['y_test'], pred_results['y_proba'],\n",
    "                     pred_results['classes'], ax=axes[2])\n",
    "\n",
    "plt.suptitle(f'模型评估 -- LogisticRegression | 特征数={len(top50_features)} '\n",
    "             f'| 类别数={pred_results[\"n_classes\"]}',\n",
    "             fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(IMG_DIR, 'step6_model_evaluation.png')\n",
    "plt.savefig(fig_path, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"[OK] 模型评估可视化已保存: {fig_path}\")\n",
    "\n",
    "# 分类报告\n",
    "print(\"\\n--- 分类报告 ---\")\n",
    "print(classification_report(pred_results['y_test'], pred_results['y_pred'],\n",
    "                              digits=4))\n",
    "\n",
    "print(\"\\n[OK] 步骤 6.2 模型评估可视化完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b552f",
   "metadata": {},
   "source": [
    "### 6.3 数据泄漏检测报告 (独立模块)\n",
    "\n",
    "独立汇总全流程中的数据泄漏检测结果：\n",
    "1. 时间序列划分验证：训练集日期 < 测试集日期\n",
    "2. 特征泄漏检测：检查是否有未来信息泄漏\n",
    "3. 模型泄漏检测：训练/测试 AUC 差异是否异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66528560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 6.3: 数据泄漏检测报告 (独立模块)\n",
      "============================================================\n",
      "\n",
      "--- 数据泄漏检测汇总 ---\n",
      "\n",
      "  [PASS] 时间序列划分\n",
      "       详情: 训练集最大日期=2019-10-22 00:00:00, 测试集最小日期=2019-10-23 00:00:00\n",
      "       风险等级: 无风险\n",
      "\n",
      "  [PASS] 样本泄漏 (行重叠)\n",
      "       详情: 重叠样本数=0\n",
      "       风险等级: 无风险\n",
      "\n",
      "  [PASS] 特征泄漏 (|r|>0.95)\n",
      "       详情: 极高相关特征: 无\n",
      "       风险等级: 无风险\n",
      "\n",
      "  [PASS] 模型泄漏 (过拟合)\n",
      "       详情: train_AUC=0.6182, test_AUC=0.5671, gap=0.0511\n",
      "       风险等级: 无风险\n",
      "\n",
      "  [PASS] 标签分布一致性\n",
      "       详情: class=-1.0: train=0.724, test=0.702; class=0.0: train=0.135, test=0.139; class=1.0: train=0.141, test=0.159 | max_diff=0.0221\n",
      "       风险等级: 无风险\n",
      "\n",
      "[OK] 泄漏检测报告已保存: leakage_detection_report.csv\n",
      "\n",
      "[SUMMARY] 通过 5/5 项检测\n",
      "[OK] 全部检测通过, 无数据泄漏风险\n",
      "\n",
      "[OK] 步骤 6.3 数据泄漏检测报告完成\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 6.3 数据泄漏检测报告 — 独立模块\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def run_leakage_detection(df: pd.DataFrame,\n",
    "                            df_cleaned: pd.DataFrame,\n",
    "                            train_idx: np.ndarray,\n",
    "                            test_idx: np.ndarray,\n",
    "                            top50: list,\n",
    "                            target: str,\n",
    "                            pred_results: dict,\n",
    "                            img_dir: str = 'images') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    独立数据泄漏检测模块，汇总全流程泄漏风险。\n",
    "\n",
    "    Description:\n",
    "        检测维度:\n",
    "        1. 时间序列划分完整性: 训练集最大日期 < 测试集最小日期\n",
    "        2. 样本泄漏: 训练集与测试集是否有重叠行\n",
    "        3. 特征泄漏 (标签相关性): Top50 中是否有与标签相关性过高 (|r|>0.95) 的特征\n",
    "        4. 模型泄漏 (过拟合检测): train_AUC - test_AUC 差值是否 > 0.15\n",
    "        5. 标签分布一致性: 训练/测试正样本率差异是否 < 0.10\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据。\n",
    "        df_cleaned : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的清理后数据。\n",
    "        train_idx : np.ndarray\n",
    "            训练集行索引。\n",
    "        test_idx : np.ndarray\n",
    "            测试集行索引。\n",
    "        top50 : list of str\n",
    "            长度为 50 的特征名列表。\n",
    "        target : str\n",
    "            目标标签列名。\n",
    "        pred_results : dict\n",
    "            包含 model, scaler, auc, y_test, y_train, n_classes 等。\n",
    "        img_dir : str\n",
    "            图片保存目录。\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame : 形状为 (n_checks, 4) 的泄漏检测报告，\n",
    "                       列 = ['检测项', '结果', '详情', '风险等级']。\n",
    "    \"\"\"\n",
    "    checks = []\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1. 时间序列划分验证\n",
    "    # ------------------------------------------------------------------\n",
    "    if 'trade_date' in df.columns:\n",
    "        train_max = df.loc[train_idx, 'trade_date'].max()\n",
    "        test_min = df.loc[test_idx, 'trade_date'].min()\n",
    "        time_pass = train_max < test_min\n",
    "        checks.append({\n",
    "            '检测项': '时间序列划分',\n",
    "            '结果': 'PASS' if time_pass else 'FAIL',\n",
    "            '详情': f'训练集最大日期={train_max}, 测试集最小日期={test_min}',\n",
    "            '风险等级': '无风险' if time_pass else '高风险'\n",
    "        })\n",
    "    else:\n",
    "        checks.append({\n",
    "            '检测项': '时间序列划分',\n",
    "            '结果': 'SKIP',\n",
    "            '详情': '未找到 trade_date 列',\n",
    "            '风险等级': '未知'\n",
    "        })\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2. 样本泄漏 (训练/测试集重叠)\n",
    "    # ------------------------------------------------------------------\n",
    "    overlap = set(train_idx) & set(test_idx)\n",
    "    sample_pass = len(overlap) == 0\n",
    "    checks.append({\n",
    "        '检测项': '样本泄漏 (行重叠)',\n",
    "        '结果': 'PASS' if sample_pass else 'FAIL',\n",
    "        '详情': f'重叠样本数={len(overlap)}',\n",
    "        '风险等级': '无风险' if sample_pass else '高风险'\n",
    "    })\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3. 特征泄漏 (极高标签相关性)\n",
    "    # ------------------------------------------------------------------\n",
    "    high_corr_feats = []\n",
    "    for feat in top50:\n",
    "        if feat in df_cleaned.columns and target in df_cleaned.columns:\n",
    "            valid = df_cleaned[[feat, target]].dropna()\n",
    "            if len(valid) > 30:\n",
    "                corr_val = abs(valid[feat].corr(valid[target]))\n",
    "                if corr_val > 0.95:\n",
    "                    high_corr_feats.append((feat, corr_val))\n",
    "\n",
    "    feat_pass = len(high_corr_feats) == 0\n",
    "    detail_str = '无' if feat_pass else '; '.join(\n",
    "        [f'{f}(|r|={v:.3f})' for f, v in high_corr_feats])\n",
    "    checks.append({\n",
    "        '检测项': '特征泄漏 (|r|>0.95)',\n",
    "        '结果': 'PASS' if feat_pass else 'WARN',\n",
    "        '详情': f'极高相关特征: {detail_str}',\n",
    "        '风险等级': '无风险' if feat_pass else '高风险'\n",
    "    })\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4. 模型泄漏 (过拟合检测)\n",
    "    # ------------------------------------------------------------------\n",
    "    model = pred_results['model']\n",
    "    scaler = pred_results['scaler']\n",
    "    n_classes = pred_results.get('n_classes', 2)\n",
    "\n",
    "    # 使用已存储的训练数据重新预测\n",
    "    X_train_raw = df_cleaned.loc[train_idx, top50].values\n",
    "    y_train_raw = df_cleaned.loc[train_idx, target].values\n",
    "    valid_tr = ~(np.isnan(X_train_raw).any(axis=1) | np.isnan(y_train_raw))\n",
    "    X_train_valid = X_train_raw[valid_tr]\n",
    "    y_train_valid = y_train_raw[valid_tr]\n",
    "    X_train_s = scaler.transform(X_train_valid)\n",
    "    train_proba = model.predict_proba(X_train_s)\n",
    "\n",
    "    if n_classes == 2:\n",
    "        train_auc = roc_auc_score(y_train_valid, train_proba[:, 1])\n",
    "    else:\n",
    "        try:\n",
    "            train_auc = roc_auc_score(y_train_valid, train_proba,\n",
    "                                        multi_class='ovr', average='macro')\n",
    "        except Exception:\n",
    "            train_auc = 0.5\n",
    "\n",
    "    test_auc = pred_results['auc']\n",
    "    auc_gap = train_auc - test_auc\n",
    "    overfit_pass = auc_gap < 0.15\n",
    "    checks.append({\n",
    "        '检测项': '模型泄漏 (过拟合)',\n",
    "        '结果': 'PASS' if overfit_pass else 'WARN',\n",
    "        '详情': f'train_AUC={train_auc:.4f}, test_AUC={test_auc:.4f}, gap={auc_gap:.4f}',\n",
    "        '风险等级': '无风险' if overfit_pass else '中风险'\n",
    "    })\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5. 标签分布一致性\n",
    "    # ------------------------------------------------------------------\n",
    "    from collections import Counter\n",
    "    train_dist = Counter(y_train_valid)\n",
    "    test_dist = Counter(pred_results['y_test'])\n",
    "    train_total = sum(train_dist.values())\n",
    "    test_total = sum(test_dist.values())\n",
    "\n",
    "    all_classes = sorted(set(list(train_dist.keys()) + list(test_dist.keys())))\n",
    "    dist_details = []\n",
    "    max_diff = 0.0\n",
    "    for cls in all_classes:\n",
    "        tr_rate = train_dist.get(cls, 0) / train_total\n",
    "        te_rate = test_dist.get(cls, 0) / test_total\n",
    "        diff = abs(tr_rate - te_rate)\n",
    "        max_diff = max(max_diff, diff)\n",
    "        dist_details.append(f'class={cls}: train={tr_rate:.3f}, test={te_rate:.3f}')\n",
    "\n",
    "    dist_pass = max_diff < 0.10\n",
    "    checks.append({\n",
    "        '检测项': '标签分布一致性',\n",
    "        '结果': 'PASS' if dist_pass else 'WARN',\n",
    "        '详情': '; '.join(dist_details) + f' | max_diff={max_diff:.4f}',\n",
    "        '风险等级': '无风险' if dist_pass else '中风险'\n",
    "    })\n",
    "\n",
    "    report_df = pd.DataFrame(checks)\n",
    "    return report_df\n",
    "\n",
    "\n",
    "# ---- 执行 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 6.3: 数据泄漏检测报告 (独立模块)\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "leakage_detection_report = run_leakage_detection(\n",
    "    df, df_cleaned, train_idx, test_idx,\n",
    "    top50_features, TARGET_LABEL, pred_results, IMG_DIR\n",
    ")\n",
    "\n",
    "# 打印报告\n",
    "print(\"--- 数据泄漏检测汇总 ---\\n\")\n",
    "for _, row in leakage_detection_report.iterrows():\n",
    "    status_tag = '[PASS]' if row['结果'] == 'PASS' else (\n",
    "        '[WARN]' if row['结果'] == 'WARN' else '[FAIL]')\n",
    "    print(f\"  {status_tag} {row['检测项']}\")\n",
    "    print(f\"       详情: {row['详情']}\")\n",
    "    print(f\"       风险等级: {row['风险等级']}\\n\")\n",
    "\n",
    "# 保存\n",
    "leakage_csv = 'leakage_detection_report.csv'\n",
    "leakage_detection_report.to_csv(leakage_csv, index=False, encoding='utf-8-sig')\n",
    "print(f\"[OK] 泄漏检测报告已保存: {leakage_csv}\")\n",
    "\n",
    "# 总结\n",
    "n_pass = (leakage_detection_report['结果'] == 'PASS').sum()\n",
    "n_total = len(leakage_detection_report)\n",
    "print(f\"\\n[SUMMARY] 通过 {n_pass}/{n_total} 项检测\")\n",
    "if n_pass == n_total:\n",
    "    print(\"[OK] 全部检测通过, 无数据泄漏风险\")\n",
    "else:\n",
    "    print(\"[WARN] 存在部分风险项, 请关注上述详情\")\n",
    "\n",
    "print(\"\\n[OK] 步骤 6.3 数据泄漏检测报告完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f580fc2",
   "metadata": {},
   "source": [
    "### 6.4 完整报告汇总\n",
    "\n",
    "汇总 Steps 1~6 全流程结果，生成结构化报告：\n",
    "- 各步骤决策摘要、关键指标\n",
    "- 产出文件索引\n",
    "- 系统设计总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "051a1b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "步骤 6.4: 完整报告汇总\n",
      "============================================================\n",
      "\n",
      "======================================================================\n",
      "  自动化特征工程系统 -- 全流程报告\n",
      "======================================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Step 1: 数据初始化与验证\n",
      "--------------------------------------------------\n",
      "  数据文件: data.pq\n",
      "  原始维度: 81046 行 x 321 列\n",
      "  特征列: 300 个 (X1~X300)\n",
      "  标签列: 12 个\n",
      "  日期范围: 2015-01-05 00:00:00 ~ 2020-12-31 00:00:00\n",
      "  平均缺失率: 23.64%\n",
      "\n",
      "--------------------------------------------------\n",
      "Step 2: 特征诊断\n",
      "--------------------------------------------------\n",
      "  诊断特征数: 300\n",
      "  诊断维度: 缺失率, 零值率, 偏度, 峰度, ADF平稳性,\n",
      "             Shapiro-Wilk正态性, 异常值比例 (IQR)\n",
      "  产出: feature_diagnosis_report.csv\n",
      "\n",
      "--------------------------------------------------\n",
      "Step 3: Agent 特征清理\n",
      "--------------------------------------------------\n",
      "  清理策略: Qwen Agent 分批决策 + 规则引擎兜底\n",
      "  缺失率变化: 23.64% -> 0.00%\n",
      "  产出: cleaning_plans.csv, cleaning_comparison.csv\n",
      "\n",
      "--------------------------------------------------\n",
      "Step 4: 特征评估\n",
      "--------------------------------------------------\n",
      "  目标标签: Y1\n",
      "  评估指标: AUC (单特征LR) + |coef| 绝对值\n",
      "  划分方式: 按时间 80/20 分割\n",
      "  冗余特征对: 139 个标记冗余\n",
      "  多重共线性: 113 个 VIF>10\n",
      "  产出: feature_effectiveness.csv, redundant_pairs.csv, feature_evaluation_report.csv\n",
      "\n",
      "--------------------------------------------------\n",
      "Step 5: 特征选择 + 模型验证\n",
      "--------------------------------------------------\n",
      "  筛选策略: 3轮过滤 (缺失>80%剔除 -> 冗余对剔除 -> 综合评分Top50)\n",
      "  评分公式: final_score = composite_norm*0.5 + label_corr_norm*0.3 + vif_bonus*0.2\n",
      "  最终特征数: 50\n",
      "  模型: LogisticRegression (class_weight=balanced)\n",
      "  测试集 AUC: 0.5671\n",
      "  产出: feature_selection_log.csv, model_validation_results.csv\n",
      "\n",
      "--------------------------------------------------\n",
      "Step 6: 可视化与报告\n",
      "--------------------------------------------------\n",
      "  6.1 综合可视化:\n",
      "      - 缺失值热力图, 收盘价时序趋势\n",
      "      - 特征清理前后分布对比 (直方图)\n",
      "      - 异常值箱型图 (清理前后)\n",
      "      - Top50 特征重要性条形图\n",
      "      - Top50 特征间相关性热力图\n",
      "  6.2 模型评估可视化:\n",
      "      - 混淆矩阵\n",
      "      - ROC 曲线 (AUC 标注)\n",
      "      - Precision-Recall 曲线 (AP 标注)\n",
      "  6.3 数据泄漏检测:\n",
      "      通过 5/5 项检测\n",
      "  6.4 完整报告: 本文档\n",
      "\n",
      "======================================================================\n",
      "  产出文件索引\n",
      "======================================================================\n",
      "  [EXISTS] feature_diagnosis_report.csv             => Step 2 特征诊断报告\n",
      "  [EXISTS] cleaning_plans.csv                       => Step 3 Agent 清理方案\n",
      "  [EXISTS] cleaning_comparison.csv                  => Step 3 清理前后对比\n",
      "  [EXISTS] feature_effectiveness.csv                => Step 4 单特征有效性\n",
      "  [EXISTS] redundant_pairs.csv                      => Step 4 冗余特征对\n",
      "  [EXISTS] feature_evaluation_report.csv            => Step 4 综合评估报告\n",
      "  [EXISTS] feature_selection_log.csv                => Step 5 特征筛选日志\n",
      "  [EXISTS] model_validation_results.csv             => Step 5 模型验证结果\n",
      "  [EXISTS] leakage_detection_report.csv             => Step 6 泄漏检测报告\n",
      "  [MISSING] final_report.txt                         => Step 6 完整报告 (本文件)\n",
      "\n",
      "======================================================================\n",
      "  可视化图片索引\n",
      "======================================================================\n",
      "  [EXISTS] images\\step6_data_overview.png\n",
      "  [EXISTS] images\\step6_distribution_compare.png\n",
      "  [EXISTS] images\\step6_outlier_boxplot.png\n",
      "  [EXISTS] images\\step6_feature_evaluation.png\n",
      "  [EXISTS] images\\step6_model_evaluation.png\n",
      "  [EXISTS] images\\close_price_trend.png\n",
      "  [EXISTS] images\\missing_heatmap.png\n",
      "  [EXISTS] images\\missing_bar.png\n",
      "  [MISSING] images\\label_distributions_all.png\n",
      "  [EXISTS] images\\correlation_distribution.png\n",
      "  [EXISTS] images\\missing_top20.png\n",
      "  [EXISTS] images\\outlier_boxplot_top20.png\n",
      "  [EXISTS] images\\feature_label_corr_heatmap.png\n",
      "  [EXISTS] images\\missing_before_after.png\n",
      "  [EXISTS] images\\outlier_before_after.png\n",
      "  [EXISTS] images\\feature_evaluation_summary.png\n",
      "  [EXISTS] images\\model_comparison.png\n",
      "\n",
      "======================================================================\n",
      "  Top50 最终特征列表\n",
      "======================================================================\n",
      "    1. X286\n",
      "    2. X222\n",
      "    3. X197\n",
      "    4. X188\n",
      "    5. X265\n",
      "    6. X219\n",
      "    7. X40\n",
      "    8. X169\n",
      "    9. X187\n",
      "   10. X179\n",
      "   11. X291\n",
      "   12. X217\n",
      "   13. X159\n",
      "   14. X58\n",
      "   15. X196\n",
      "   16. X260\n",
      "   17. X23\n",
      "   18. X249\n",
      "   19. X254\n",
      "   20. X62\n",
      "   21. X206\n",
      "   22. X128\n",
      "   23. X131\n",
      "   24. X268\n",
      "   25. X164\n",
      "   26. X155\n",
      "   27. X130\n",
      "   28. X213\n",
      "   29. X205\n",
      "   30. X202\n",
      "   31. X157\n",
      "   32. X80\n",
      "   33. X191\n",
      "   34. X271\n",
      "   35. X209\n",
      "   36. X57\n",
      "   37. X216\n",
      "   38. X67\n",
      "   39. X113\n",
      "   40. X156\n",
      "   41. X211\n",
      "   42. X204\n",
      "   43. X63\n",
      "   44. X212\n",
      "   45. X93\n",
      "   46. X167\n",
      "   47. X129\n",
      "   48. X28\n",
      "   49. X287\n",
      "   50. X203\n",
      "\n",
      "======================================================================\n",
      "  Agent 系统设计总结\n",
      "======================================================================\n",
      "  1. 架构: Pipeline 式 Agent (数据初始化 -> 诊断 -> 清理 -> 评估 -> 选择 -> 报告)\n",
      "  2. LLM 调用: SiliconFlow API (Qwen2.5-7B-Instruct)\n",
      "     - 使用 openai SDK 兼容接口\n",
      "     - System Prompt 约束 JSON 输出格式\n",
      "     - 分批处理 (30特征/批) 避免 token 溢出\n",
      "  3. 错误处理:\n",
      "     - JSON 解析失败 -> 正则回退提取\n",
      "     - Agent 动作不合法 -> 规则引擎兜底 (基于缺失率/异常值率)\n",
      "     - API 超时/异常 -> 重试 + 降级策略\n",
      "  4. 评估体系:\n",
      "     - 单特征 LogisticRegression AUC + 系数绝对值\n",
      "     - 冗余检测: Pearson |r|>0.8 + VIF\n",
      "     - 综合评分 = composite_norm*0.5 + label_corr_norm*0.3 + vif_bonus*0.2\n",
      "  5. 泄漏防控:\n",
      "     - 严格按时间划分训练/测试集\n",
      "     - 独立泄漏检测模块 (5 个维度)\n",
      "     - 特征与标签极高相关性告警\n",
      "\n",
      "======================================================================\n",
      "  报告结束\n",
      "======================================================================\n",
      "\n",
      "[OK] 完整报告已保存: final_report.txt\n",
      "\n",
      "[OK] 步骤 6.4 完整报告汇总完成\n",
      "\n",
      "============================================================\n",
      "[OK] Step 6 全部完成\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: 6.4 完整报告汇总\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def generate_final_report(df: pd.DataFrame,\n",
    "                            df_cleaned: pd.DataFrame,\n",
    "                            top50: list,\n",
    "                            evaluation_report: pd.DataFrame,\n",
    "                            pred_results: dict,\n",
    "                            leakage_detection_report: pd.DataFrame,\n",
    "                            target: str) -> str:\n",
    "    \"\"\"\n",
    "    生成全流程结构化报告文本。\n",
    "\n",
    "    Description:\n",
    "        汇总 Steps 1~6 的关键决策、指标、产出文件索引。\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的原始数据。\n",
    "        df_cleaned : pd.DataFrame\n",
    "            形状为 (n_samples, n_columns) 的清理后数据。\n",
    "        top50 : list of str\n",
    "            长度为 50 的最终选定特征列表。\n",
    "        evaluation_report : pd.DataFrame\n",
    "            特征综合评估报告。\n",
    "        pred_results : dict\n",
    "            模型预测结果字典。\n",
    "        leakage_detection_report : pd.DataFrame\n",
    "            泄漏检测报告。\n",
    "        target : str\n",
    "            目标标签列名。\n",
    "\n",
    "    Returns:\n",
    "        str : 完整报告文本。\n",
    "    \"\"\"\n",
    "    sep = \"=\" * 70\n",
    "    lines = []\n",
    "    lines.append(sep)\n",
    "    lines.append(\"  自动化特征工程系统 -- 全流程报告\")\n",
    "    lines.append(sep)\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # ---- Step 1 ----\n",
    "    lines.append(\"-\" * 50)\n",
    "    lines.append(\"Step 1: 数据初始化与验证\")\n",
    "    lines.append(\"-\" * 50)\n",
    "    lines.append(f\"  数据文件: data.pq\")\n",
    "    lines.append(f\"  原始维度: {df.shape[0]} 行 x {df.shape[1]} 列\")\n",
    "    n_features = len([c for c in df.columns if c.startswith('X')])\n",
    "    n_labels = len([c for c in df.columns if c.startswith('Y')])\n",
    "    lines.append(f\"  特征列: {n_features} 个 (X1~X{n_features})\")\n",
    "    lines.append(f\"  标签列: {n_labels} 个\")\n",
    "    if 'trade_date' in df.columns:\n",
    "        lines.append(f\"  日期范围: {df['trade_date'].min()} ~ {df['trade_date'].max()}\")\n",
    "    miss_pct = df[[c for c in df.columns if c.startswith('X')]].isnull().mean().mean() * 100\n",
    "    lines.append(f\"  平均缺失率: {miss_pct:.2f}%\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # ---- Step 2 ----\n",
    "    lines.append(\"-\" * 50)\n",
    "    lines.append(\"Step 2: 特征诊断\")\n",
    "    lines.append(\"-\" * 50)\n",
    "    lines.append(f\"  诊断特征数: {n_features}\")\n",
    "    lines.append(f\"  诊断维度: 缺失率, 零值率, 偏度, 峰度, ADF平稳性,\")\n",
    "    lines.append(f\"             Shapiro-Wilk正态性, 异常值比例 (IQR)\")\n",
    "    lines.append(f\"  产出: feature_diagnosis_report.csv\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # ---- Step 3 ----\n",
    "    lines.append(\"-\" * 50)\n",
    "    lines.append(\"Step 3: Agent 特征清理\")\n",
    "    lines.append(\"-\" * 50)\n",
    "    miss_before = df[[c for c in df.columns if c.startswith('X')]].isnull().mean().mean() * 100\n",
    "    miss_after = df_cleaned[top50].isnull().mean().mean() * 100\n",
    "    lines.append(f\"  清理策略: Qwen Agent 分批决策 + 规则引擎兜底\")\n",
    "    lines.append(f\"  缺失率变化: {miss_before:.2f}% -> {miss_after:.2f}%\")\n",
    "    lines.append(f\"  产出: cleaning_plans.csv, cleaning_comparison.csv\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # ---- Step 4 ----\n",
    "    lines.append(\"-\" * 50)\n",
    "    lines.append(\"Step 4: 特征评估\")\n",
    "    lines.append(\"-\" * 50)\n",
    "    lines.append(f\"  目标标签: {target}\")\n",
    "    lines.append(f\"  评估指标: AUC (单特征LR) + |coef| 绝对值\")\n",
    "    lines.append(f\"  划分方式: 按时间 80/20 分割\")\n",
    "    n_redundant = evaluation_report['is_redundant'].sum() if 'is_redundant' in evaluation_report.columns else 0\n",
    "    n_multicol = evaluation_report['is_multicollinear'].sum() if 'is_multicollinear' in evaluation_report.columns else 0\n",
    "    lines.append(f\"  冗余特征对: {n_redundant} 个标记冗余\")\n",
    "    lines.append(f\"  多重共线性: {n_multicol} 个 VIF>10\")\n",
    "    lines.append(f\"  产出: feature_effectiveness.csv, redundant_pairs.csv, feature_evaluation_report.csv\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # ---- Step 5 ----\n",
    "    lines.append(\"-\" * 50)\n",
    "    lines.append(\"Step 5: 特征选择 + 模型验证\")\n",
    "    lines.append(\"-\" * 50)\n",
    "    lines.append(f\"  筛选策略: 3轮过滤 (缺失>80%剔除 -> 冗余对剔除 -> 综合评分Top50)\")\n",
    "    lines.append(f\"  评分公式: final_score = composite_norm*0.5 + label_corr_norm*0.3 + vif_bonus*0.2\")\n",
    "    lines.append(f\"  最终特征数: {len(top50)}\")\n",
    "    lines.append(f\"  模型: LogisticRegression (class_weight=balanced)\")\n",
    "    lines.append(f\"  测试集 AUC: {pred_results['auc']:.4f}\")\n",
    "    lines.append(f\"  产出: feature_selection_log.csv, model_validation_results.csv\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # ---- Step 6 ----\n",
    "    lines.append(\"-\" * 50)\n",
    "    lines.append(\"Step 6: 可视化与报告\")\n",
    "    lines.append(\"-\" * 50)\n",
    "    lines.append(\"  6.1 综合可视化:\")\n",
    "    lines.append(\"      - 缺失值热力图, 收盘价时序趋势\")\n",
    "    lines.append(\"      - 特征清理前后分布对比 (直方图)\")\n",
    "    lines.append(\"      - 异常值箱型图 (清理前后)\")\n",
    "    lines.append(\"      - Top50 特征重要性条形图\")\n",
    "    lines.append(\"      - Top50 特征间相关性热力图\")\n",
    "    lines.append(\"  6.2 模型评估可视化:\")\n",
    "    lines.append(\"      - 混淆矩阵\")\n",
    "    lines.append(\"      - ROC 曲线 (AUC 标注)\")\n",
    "    lines.append(\"      - Precision-Recall 曲线 (AP 标注)\")\n",
    "    lines.append(\"  6.3 数据泄漏检测:\")\n",
    "    n_leak_pass = (leakage_detection_report['结果'] == 'PASS').sum()\n",
    "    n_leak_total = len(leakage_detection_report)\n",
    "    lines.append(f\"      通过 {n_leak_pass}/{n_leak_total} 项检测\")\n",
    "    lines.append(\"  6.4 完整报告: 本文档\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # ---- 产出文件索引 ----\n",
    "    lines.append(sep)\n",
    "    lines.append(\"  产出文件索引\")\n",
    "    lines.append(sep)\n",
    "    files_index = [\n",
    "        ('feature_diagnosis_report.csv', 'Step 2 特征诊断报告'),\n",
    "        ('cleaning_plans.csv', 'Step 3 Agent 清理方案'),\n",
    "        ('cleaning_comparison.csv', 'Step 3 清理前后对比'),\n",
    "        ('feature_effectiveness.csv', 'Step 4 单特征有效性'),\n",
    "        ('redundant_pairs.csv', 'Step 4 冗余特征对'),\n",
    "        ('feature_evaluation_report.csv', 'Step 4 综合评估报告'),\n",
    "        ('feature_selection_log.csv', 'Step 5 特征筛选日志'),\n",
    "        ('model_validation_results.csv', 'Step 5 模型验证结果'),\n",
    "        ('leakage_detection_report.csv', 'Step 6 泄漏检测报告'),\n",
    "        ('final_report.txt', 'Step 6 完整报告 (本文件)'),\n",
    "    ]\n",
    "    for fname, desc in files_index:\n",
    "        marker = '[EXISTS]' if os.path.exists(fname) else '[MISSING]'\n",
    "        lines.append(f\"  {marker} {fname:40s} => {desc}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # ---- 图片索引 ----\n",
    "    lines.append(sep)\n",
    "    lines.append(\"  可视化图片索引\")\n",
    "    lines.append(sep)\n",
    "    img_files = [\n",
    "        'step6_data_overview.png',\n",
    "        'step6_distribution_compare.png',\n",
    "        'step6_outlier_boxplot.png',\n",
    "        'step6_feature_evaluation.png',\n",
    "        'step6_model_evaluation.png',\n",
    "        'close_price_trend.png',\n",
    "        'missing_heatmap.png',\n",
    "        'missing_bar.png',\n",
    "        'label_distributions_all.png',\n",
    "        'correlation_distribution.png',\n",
    "        'missing_top20.png',\n",
    "        'outlier_boxplot_top20.png',\n",
    "        'feature_label_corr_heatmap.png',\n",
    "        'missing_before_after.png',\n",
    "        'outlier_before_after.png',\n",
    "        'feature_evaluation_summary.png',\n",
    "        'model_comparison.png',\n",
    "    ]\n",
    "    for img in img_files:\n",
    "        path = os.path.join('images', img)\n",
    "        marker = '[EXISTS]' if os.path.exists(path) else '[MISSING]'\n",
    "        lines.append(f\"  {marker} {path}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # ---- Top50 特征列表 ----\n",
    "    lines.append(sep)\n",
    "    lines.append(\"  Top50 最终特征列表\")\n",
    "    lines.append(sep)\n",
    "    for i, feat in enumerate(top50, 1):\n",
    "        lines.append(f\"  {i:3d}. {feat}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # ---- 系统设计总结 ----\n",
    "    lines.append(sep)\n",
    "    lines.append(\"  Agent 系统设计总结\")\n",
    "    lines.append(sep)\n",
    "    lines.append(\"  1. 架构: Pipeline 式 Agent (数据初始化 -> 诊断 -> 清理 -> 评估 -> 选择 -> 报告)\")\n",
    "    lines.append(\"  2. LLM 调用: SiliconFlow API (Qwen2.5-7B-Instruct)\")\n",
    "    lines.append(\"     - 使用 openai SDK 兼容接口\")\n",
    "    lines.append(\"     - System Prompt 约束 JSON 输出格式\")\n",
    "    lines.append(\"     - 分批处理 (30特征/批) 避免 token 溢出\")\n",
    "    lines.append(\"  3. 错误处理:\")\n",
    "    lines.append(\"     - JSON 解析失败 -> 正则回退提取\")\n",
    "    lines.append(\"     - Agent 动作不合法 -> 规则引擎兜底 (基于缺失率/异常值率)\")\n",
    "    lines.append(\"     - API 超时/异常 -> 重试 + 降级策略\")\n",
    "    lines.append(\"  4. 评估体系:\")\n",
    "    lines.append(\"     - 单特征 LogisticRegression AUC + 系数绝对值\")\n",
    "    lines.append(\"     - 冗余检测: Pearson |r|>0.8 + VIF\")\n",
    "    lines.append(\"     - 综合评分 = composite_norm*0.5 + label_corr_norm*0.3 + vif_bonus*0.2\")\n",
    "    lines.append(\"  5. 泄漏防控:\")\n",
    "    lines.append(\"     - 严格按时间划分训练/测试集\")\n",
    "    lines.append(\"     - 独立泄漏检测模块 (5 个维度)\")\n",
    "    lines.append(\"     - 特征与标签极高相关性告警\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(sep)\n",
    "    lines.append(\"  报告结束\")\n",
    "    lines.append(sep)\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# ---- 执行 ----\n",
    "print(\"=\" * 60)\n",
    "print(\"步骤 6.4: 完整报告汇总\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "final_report_text = generate_final_report(\n",
    "    df, df_cleaned, top50_features, evaluation_report,\n",
    "    pred_results, leakage_detection_report, TARGET_LABEL\n",
    ")\n",
    "\n",
    "# 打印报告\n",
    "print(final_report_text)\n",
    "\n",
    "# 保存报告\n",
    "report_path = 'final_report.txt'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(final_report_text)\n",
    "print(f\"\\n[OK] 完整报告已保存: {report_path}\")\n",
    "\n",
    "print(\"\\n[OK] 步骤 6.4 完整报告汇总完成\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"[OK] Step 6 全部完成\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
